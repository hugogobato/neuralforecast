[
  {
    "objectID": "models.dilated_rnn.html",
    "href": "models.dilated_rnn.html",
    "title": "Dilated RNN",
    "section": "",
    "text": "The Dilated Recurrent Neural Network (DilatedRNN) addresses common challenges of modeling long sequences like vanishing gradients, computational efficiency, and improved model flexibility to model complex relationships while maintaining its parsimony. The DilatedRNN builds a deep stack of RNN layers using skip conditions on the temporal and the network’s depth dimensions. The temporal dilated recurrent skip connections offer the capability to focus on multi-resolution inputs.The predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{DilatedRNN}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nwhere \\(\\mathbf{h}_{t}\\), is the hidden state for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction.\nReferences-Shiyu Chang, et al. “Dilated Recurrent Neural Networks”.-Yao Qin, et al. “A Dual-Stage Attention-Based recurrent neural network for time series prediction”.-Kashif Rasul, et al. “Zalando Research: PyTorch Dilated Recurrent Neural Networks”.\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.dilated_rnn.html#usage-example",
    "href": "models.dilated_rnn.html#usage-example",
    "title": "Dilated RNN",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import DilatedRNN\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[DilatedRNN(h=12,\n                       input_size=-1,\n                       loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                       scaler_type='robust',\n                       encoder_hidden_size=100,\n                       max_steps=200,\n                       futr_exog_list=['y_[lag12]'],\n                       hist_exog_list=None,\n                       stat_exog_list=['airline1'],\n    )\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['DilatedRNN-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['DilatedRNN-lo-90'][-12:].values, \n                 y2=plot_df['DilatedRNN-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "models.vanillatransformer.html",
    "href": "models.vanillatransformer.html",
    "title": "Vanilla Transformer",
    "section": "",
    "text": "Vanilla Transformer, following implementation of the Informer paper, used as baseline.\nThe architecture has three distinctive features: - Full-attention mechanism with O(L^2) time and memory complexity. - Classic encoder-decoder proposed by Vaswani et al. (2017) with a multi-head attention mechanism. - An MLP multi-step decoder that predicts long time-series sequences in a single forward operation rather than step-by-step.\nThe Vanilla Transformer model utilizes a three-component approach to define its embedding: - It employs encoded autoregressive features obtained from a convolution network. - It uses window-relative positional embeddings derived from harmonic functions. - Absolute positional embeddings obtained from calendar features are utilized.\nReferences - Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang. “Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting”\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.vanillatransformer.html#auxiliary-functions",
    "href": "models.vanillatransformer.html#auxiliary-functions",
    "title": "Vanilla Transformer",
    "section": "1. Auxiliary Functions",
    "text": "1. Auxiliary Functions\n\nsource\n\nFullAttention\n\n FullAttention (mask_flag=True, scale=None, attention_dropout=0.1,\n                output_attention=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTriangularCausalMask\n\n TriangularCausalMask (B, L, device='cpu')\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "models.vanillatransformer.html#vanillatransformer",
    "href": "models.vanillatransformer.html#vanillatransformer",
    "title": "Vanilla Transformer",
    "section": "2. VanillaTransformer",
    "text": "2. VanillaTransformer\n\nsource\n\nVanillaTransformer\n\n VanillaTransformer (h:int, input_size:int, stat_exog_list=None,\n                     hist_exog_list=None, futr_exog_list=None,\n                     decoder_input_size_multiplier:float=0.5,\n                     hidden_size:int=128, dropout:float=0.05,\n                     n_head:int=4, conv_hidden_size:int=32,\n                     activation:str='gelu', encoder_layers:int=2,\n                     decoder_layers:int=1, loss=MAE(), valid_loss=None,\n                     max_steps:int=5000, learning_rate:float=0.0001,\n                     num_lr_decays:int=-1,\n                     early_stop_patience_steps:int=-1,\n                     val_check_steps:int=100, batch_size:int=32,\n                     valid_batch_size:Optional[int]=None,\n                     windows_batch_size=1024, step_size:int=1,\n                     scaler_type:str='identity', random_seed:int=1,\n                     num_workers_loader:int=0,\n                     drop_last_loader:bool=False, **trainer_kwargs)\n\nVanillaTransformer\nVanilla Transformer, following implementation of the Informer paper, used as baseline.\nThe architecture has three distinctive features: - Full-attention mechanism with O(L^2) time and memory complexity. - An MLP multi-step decoder that predicts long time-series sequences in a single forward operation rather than step-by-step.\nThe Vanilla Transformer model utilizes a three-component approach to define its embedding: - It employs encoded autoregressive features obtained from a convolution network. - It uses window-relative positional embeddings derived from harmonic functions. - Absolute positional embeddings obtained from calendar features are utilized.\nParameters: h: int, forecast horizon. input_size: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history. futr_exog_list: str list, future exogenous columns. hist_exog_list: str list, historic exogenous columns. stat_exog_list: str list, static exogenous columns. decoder_input_size_multiplier: float = 0.5, . hidden_size: int=128, units of embeddings and encoders. n_head: int=4, controls number of multi-head’s attention. dropout: float (0, 1), dropout throughout Informer architecture. conv_hidden_size: int=32, channels of the convolutional encoder. activation: str=GELU, activation from [‘ReLU’, ‘Softplus’, ‘Tanh’, ‘SELU’, ‘LeakyReLU’, ‘PReLU’, ‘Sigmoid’, ‘GELU’]. encoder_layers: int=2, number of layers for the TCN encoder. decoder_layers: int=1, number of layers for the MLP decoder. loss: PyTorch module, instantiated train loss class from losses collection. max_steps: int=1000, maximum number of training steps. learning_rate: float=1e-3, Learning rate between (0, 1). valid_batch_size: int=None, number of different series in each validation and test batch. num_lr_decays: int=-1, Number of learning rate decays, evenly distributed across max_steps. early_stop_patience_steps: int=-1, Number of validation iterations before early stopping. val_check_steps: int=100, Number of training steps between every validation loss check. batch_size: int=32, number of differentseries in each batch. scaler_type: str=‘robust’, type of scaler for temporal inputs normalization see temporal scalers. random_seed: int=1, random_seed for pytorch initializer and numpy generators. num_workers_loader: int=os.cpu_count(), workers to be used by TimeSeriesDataLoader. drop_last_loader: bool=False, if True TimeSeriesDataLoader drops last non-full batch. alias: str, optional, Custom name of the model. **trainer_kwargs: int, keyword trainer arguments inherited from PyTorch Lighning’s trainer.\n*References*&lt;br&gt;\n- [Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang. \"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\"](https://arxiv.org/abs/2012.07436)&lt;br&gt;\n\n\n\nVanillaTransformer.fit\n\n VanillaTransformer.fit (dataset, val_size=0, test_size=0,\n                         random_seed=None)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, windows_batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nVanillaTransformer.predict\n\n VanillaTransformer.predict (dataset, test_size=None, step_size=1,\n                             random_seed=None, **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. test_size: int=None, test size for temporal cross-validation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation."
  },
  {
    "objectID": "models.vanillatransformer.html#usage-example",
    "href": "models.vanillatransformer.html#usage-example",
    "title": "Vanilla Transformer",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import MLP\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = VanillaTransformer(h=12,\n                 input_size=24,\n                 hidden_size=16,\n                 conv_hidden_size=32,\n                 n_head=2,\n                 loss=MAE(),\n                 futr_exog_list=calendar_cols,\n                 scaler_type='robust',\n                 learning_rate=1e-3,\n                 max_steps=500,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['VanillaTransformer-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['VanillaTransformer-lo-90'][-12:].values, \n                    y2=plot_df['VanillaTransformer-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['VanillaTransformer'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()"
  },
  {
    "objectID": "models.nbeatsx.html",
    "href": "models.nbeatsx.html",
    "title": "NBEATSx",
    "section": "",
    "text": "The Neural Basis Expansion Analysis (NBEATS) is an MLP-based deep neural architecture with backward and forward residual links. The network has two variants: (1) in its interpretable configuration, NBEATS sequentially projects the signal into polynomials and harmonic basis to learn trend and seasonality components; (2) in its generic configuration, it substitutes the polynomial and harmonic basis for identity basis and larger network’s depth. The Neural Basis Expansion Analysis with Exogenous (NBEATSx), incorporates projections to exogenous temporal variables available at the time of the prediction. This method proved state-of-the-art performance on the M3, M4, and Tourism Competition datasets, improving accuracy by 3% over the ESRNN M4 competition winner. For Electricity Price Forecasting tasks NBEATSx model improved accuracy by 20% and 5% over ESRNN and NBEATS, and 5% on task-specialized architectures.References-Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2019). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”.-Kin G. Olivares, Cristian Challu, Grzegorz Marcjasz, Rafał Weron, Artur Dubrawski (2021). “Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx”.\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.nbeatsx.html#usage-example",
    "href": "models.nbeatsx.html#usage-example",
    "title": "NBEATSx",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATSx\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NBEATSx(h=12, input_size=24,\n                #loss=MQLoss(level=[80, 90]),\n                loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                scaler_type='robust',\n                stat_exog_list=['airline1'],\n                futr_exog_list=['trend'],\n                max_steps=200,\n                val_check_steps=10,\n                early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nY_hat_df = nf.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['NBEATSx'], c='purple', label='mean')\nplt.plot(plot_df['ds'], plot_df['NBEATSx-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['NBEATSx-lo-90'][-12:].values, \n                 y2=plot_df['NBEATSx-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "tsdataset.html",
    "href": "tsdataset.html",
    "title": "PyTorch Dataset/Loader",
    "section": "",
    "text": "source\n\nTimeSeriesLoader\n\n TimeSeriesLoader (dataset, **kwargs)\n\nTimeSeriesLoader DataLoader. Source code.\nSmall change to PyTorch’s Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\nThe class ~torch.utils.data.DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.\nParameters: batch_size: (int, optional): how many samples per batch to load (default: 1). shuffle: (bool, optional): set to True to have the data reshuffled at every epoch (default: False). sampler: (Sampler or Iterable, optional): defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, shuffle must not be specified.\n\nsource\n\n\nTimeSeriesDataset\n\n TimeSeriesDataset (temporal, temporal_cols, indptr, max_size:int,\n                    min_size:int, static=None, static_cols=None,\n                    sorted=False)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\nsource\n\n\nTimeSeriesDataModule\n\n TimeSeriesDataModule (dataset:__main__.TimeSeriesDataset, batch_size=32,\n                       valid_batch_size=1024, num_workers=0,\n                       drop_last=False)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n# To test correct future_df wrangling of the `update_df` method\n# We are checking that we are able to recover the AirPassengers dataset\n# using the dataframe or splitting it into parts and initializing.\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.autoformer.html",
    "href": "models.autoformer.html",
    "title": "Autoformer",
    "section": "",
    "text": "The Autoformer model tackles the challenge of finding reliable dependencies on intricate temporal patterns of long-horizon forecasting.\nThe architecture has the following distinctive features: - In-built progressive decomposition in trend and seasonal compontents based on a moving average filter. - Auto-Correlation mechanism that discovers the period-based dependencies by calculating the autocorrelation and aggregating similar sub-series based on the periodicity. - Classic encoder-decoder proposed by Vaswani et al. (2017) with a multi-head attention mechanism.\nThe Autoformer model utilizes a three-component approach to define its embedding: - It employs encoded autoregressive features obtained from a convolution network. - Absolute positional embeddings obtained from calendar features are utilized.\nReferences - Wu, Haixu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. “Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting”\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.autoformer.html#auxiliary-functions",
    "href": "models.autoformer.html#auxiliary-functions",
    "title": "Autoformer",
    "section": "1. Auxiliary Functions",
    "text": "1. Auxiliary Functions\n\nsource\n\nDecoder\n\n Decoder (layers, norm_layer=None, projection=None)\n\nAutoformer decoder\n\nsource\n\n\nDecoderLayer\n\n DecoderLayer (self_attention, cross_attention, hidden_size, c_out,\n               conv_hidden_size=None, MovingAvg=25, dropout=0.1,\n               activation='relu')\n\nAutoformer decoder layer with the progressive decomposition architecture\n\nsource\n\n\nEncoder\n\n Encoder (attn_layers, conv_layers=None, norm_layer=None)\n\nAutoformer encoder\n\nsource\n\n\nEncoderLayer\n\n EncoderLayer (attention, hidden_size, conv_hidden_size=None,\n               MovingAvg=25, dropout=0.1, activation='relu')\n\nAutoformer encoder layer with the progressive decomposition architecture\n\nsource\n\n\nSeriesDecomp\n\n SeriesDecomp (kernel_size)\n\nSeries decomposition block\n\nsource\n\n\nMovingAvg\n\n MovingAvg (kernel_size, stride)\n\nMoving average block to highlight the trend of time series\n\nsource\n\n\nLayerNorm\n\n LayerNorm (channels)\n\nSpecial designed layernorm for the seasonal part\n\nsource\n\n\nAutoCorrelationLayer\n\n AutoCorrelationLayer (correlation, hidden_size, n_head, d_keys=None,\n                       d_values=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nAutoCorrelation\n\n AutoCorrelation (mask_flag=True, factor=1, scale=None,\n                  attention_dropout=0.1, output_attention=False)\n\nAutoCorrelation Mechanism with the following two phases: (1) period-based dependencies discovery (2) time delay aggregation This block can replace the self-attention family mechanism seamlessly."
  },
  {
    "objectID": "models.autoformer.html#autoformer",
    "href": "models.autoformer.html#autoformer",
    "title": "Autoformer",
    "section": "2. Autoformer",
    "text": "2. Autoformer\n\nsource\n\nAutoformer\n\n Autoformer (h:int, input_size:int, stat_exog_list=None,\n             hist_exog_list=None, futr_exog_list=None,\n             decoder_input_size_multiplier:float=0.5, hidden_size:int=128,\n             dropout:float=0.05, factor:int=3, n_head:int=4,\n             conv_hidden_size:int=32, activation:str='gelu',\n             encoder_layers:int=2, decoder_layers:int=1,\n             MovingAvg_window:int=25, loss=MAE(), valid_loss=None,\n             max_steps:int=5000, learning_rate:float=0.0001,\n             num_lr_decays:int=-1, early_stop_patience_steps:int=-1,\n             val_check_steps:int=100, batch_size:int=32,\n             valid_batch_size:Optional[int]=None, windows_batch_size=1024,\n             step_size:int=1, scaler_type:str='identity',\n             random_seed:int=1, num_workers_loader:int=0,\n             drop_last_loader:bool=False, **trainer_kwargs)\n\nAutoformer\nThe Autoformer model tackles the challenge of finding reliable dependencies on intricate temporal patterns of long-horizon forecasting.\nThe architecture has the following distinctive features: - In-built progressive decomposition in trend and seasonal compontents based on a moving average filter. - Auto-Correlation mechanism that discovers the period-based dependencies by calculating the autocorrelation and aggregating similar sub-series based on the periodicity. - Classic encoder-decoder proposed by Vaswani et al. (2017) with a multi-head attention mechanism.\nThe Autoformer model utilizes a three-component approach to define its embedding: - It employs encoded autoregressive features obtained from a convolution network. - Absolute positional embeddings obtained from calendar features are utilized.\nParameters: h: int, forecast horizon. input_size: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history. futr_exog_list: str list, future exogenous columns. hist_exog_list: str list, historic exogenous columns. stat_exog_list: str list, static exogenous columns. decoder_input_size_multiplier: float = 0.5, . hidden_size: int=128, units of embeddings and encoders. n_head: int=4, controls number of multi-head’s attention. dropout: float (0, 1), dropout throughout Autoformer architecture. factor: int=3, Probsparse attention factor. conv_hidden_size: int=32, channels of the convolutional encoder. activation: str=GELU, activation from [‘ReLU’, ‘Softplus’, ‘Tanh’, ‘SELU’, ‘LeakyReLU’, ‘PReLU’, ‘Sigmoid’, ‘GELU’]. encoder_layers: int=2, number of layers for the TCN encoder. decoder_layers: int=1, number of layers for the MLP decoder. distil: bool = True, wether the Autoformer decoder uses bottlenecks. loss: PyTorch module, instantiated train loss class from losses collection. max_steps: int=1000, maximum number of training steps. learning_rate: float=1e-3, Learning rate between (0, 1). valid_batch_size: int=None, number of different series in each validation and test batch. num_lr_decays: int=-1, Number of learning rate decays, evenly distributed across max_steps. early_stop_patience_steps: int=-1, Number of validation iterations before early stopping. val_check_steps: int=100, Number of training steps between every validation loss check. batch_size: int=32, number of differentseries in each batch. scaler_type: str=‘robust’, type of scaler for temporal inputs normalization see temporal scalers. random_seed: int=1, random_seed for pytorch initializer and numpy generators. num_workers_loader: int=os.cpu_count(), workers to be used by TimeSeriesDataLoader. drop_last_loader: bool=False, if True TimeSeriesDataLoader drops last non-full batch. alias: str, optional, Custom name of the model. **trainer_kwargs: int, keyword trainer arguments inherited from PyTorch Lighning’s trainer.\n*References*&lt;br&gt;\n- [Wu, Haixu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. \"Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting\"](https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html)&lt;br&gt;\n\n\n\nAutoformer.fit\n\n Autoformer.fit (dataset, val_size=0, test_size=0, random_seed=None)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, windows_batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nAutoformer.predict\n\n Autoformer.predict (dataset, test_size=None, step_size=1,\n                     random_seed=None, **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. test_size: int=None, test size for temporal cross-validation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation."
  },
  {
    "objectID": "models.autoformer.html#usage-example",
    "href": "models.autoformer.html#usage-example",
    "title": "Autoformer",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import MLP\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = Autoformer(h=12,\n                 input_size=24,\n                 hidden_size = 16,\n                 conv_hidden_size = 32,\n                 n_head=2,\n                 loss=MAE(),\n                 futr_exog_list=calendar_cols,\n                 scaler_type='robust',\n                 learning_rate=1e-3,\n                 max_steps=300,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['Autoformer-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['Autoformer-lo-90'][-12:].values, \n                    y2=plot_df['Autoformer-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['Autoformer'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()"
  },
  {
    "objectID": "models.stemgnn.html",
    "href": "models.stemgnn.html",
    "title": "StemGNN",
    "section": "",
    "text": "The Spectral Temporal Graph Neural Network (StemGNN) is a Graph-based multivariate time-series forecasting model. StemGNN jointly learns temporal dependencies and inter-series correlations in the spectral domain, by combining Graph Fourier Transform (GFT) and Discrete Fourier Transform (DFT).\nThis method proved state-of-the-art performance on geo-temporal datasets such as Solar, METR-LA, and PEMS-BAY, and\nReferences -Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, Qi Zhang (2020). “Spectral Temporal Graph Neural Network for Multivariate Time-series Forecasting”.\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.stemgnn.html#usage-examples",
    "href": "models.stemgnn.html#usage-examples",
    "title": "StemGNN",
    "section": "Usage Examples",
    "text": "Usage Examples\nTrain model and forecast future values with predict method.\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.losses.pytorch import MAE\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = StemGNN(h=12,\n                input_size=24,\n                n_series=2,\n                stat_exog_list=['airline1'],\n                futr_exog_list=['trend'],\n                scaler_type='robust',\n                max_steps=200,\n                early_stop_patience_steps=-1,\n                val_check_steps=10,\n                learning_rate=1e-3,\n                loss=MAE(),\n                valid_loss=None,\n                batch_size=32\n                )\n\nfcst = NeuralForecast(models=[model], freq='M')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['StemGNN'], c='blue', label='Forecast')\nplt.legend()\nplt.grid()\n\nUsing cross_validation to forecast multiple historic values.\n\nfcst = NeuralForecast(models=[model], freq='M')\nforecasts = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=2, step_size=12)\n\n\n# Plot quantile predictions\nY_hat_df = forecasts[forecasts['unique_id']=='Airline1']\nY_df = AirPassengersPanel[AirPassengersPanel['unique_id']=='Airline1']\n\nplt.plot(Y_df['ds'], Y_df['y'], c='black', label='True')\nplt.plot(Y_hat_df['ds'], Y_hat_df['StemGNN'], c='blue', label='Forecast')\nplt.legend()\nplt.grid()"
  },
  {
    "objectID": "models.hint.html",
    "href": "models.hint.html",
    "title": "HINT",
    "section": "",
    "text": "Hierarchical Forecast Networks (HINT) is a novel approach that combines SoTA neural forecast methods with flexible and efficient probability distributions and advanced hierarchical reconciliation strategies. This powerful combination allows HINT to produce accurate and coherent probabilistic predictions.\nHINT’s architecture incorporates a unique TemporalNorm module that leverages scaled-decoupled optimization. Additionally, it integrated bootstrapped sample filtering that enforces hierarchical constraints on the samples of its forecast distribution.\nReferences - Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker (2022).”Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures”. International Journal Forecasting, accepted paper available at arxiv. - Kin G. Olivares, Federico Garza, David Luo, Cristian Challu, Max Mergenthaler, Souhaib Ben Taieb, Shanika Wickramasuriya, and Artur Dubrawski (2022). “HierarchicalForecast: A reference framework for hierarchical forecasting in python”. Journal of Machine Learning Research, submitted, abs/2207.03517, 2022b.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.hint.html#reconciliation-methods",
    "href": "models.hint.html#reconciliation-methods",
    "title": "HINT",
    "section": "1. Reconciliation Methods",
    "text": "1. Reconciliation Methods\n\nsource\n\nget_bottomup_P\n\n get_bottomup_P (S:numpy.ndarray)\n\nBottomUp Reconciliation Matrix.\nCreates BottomUp hierarchical “projection” matrix is defined as: \\[\\mathbf{P}_{\\text{BU}} = [\\mathbf{0}_{\\mathrm{[b],[a]}}\\;|\\;\\mathbf{I}_{\\mathrm{[b][b]}}]\\]\nParameters: S: Summing matrix of size (base, bottom).\nReturns: P: Reconciliation matrix of size (bottom, base).\nReferences: - Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). “Data aggregation and information loss”. The American Economic Review, 58 , 773{787).\n\nsource\n\n\nget_mintrace_ols_P\n\n get_mintrace_ols_P (S:numpy.ndarray)\n\nMinTraceOLS Reconciliation Matrix.\nCreates MinTraceOLS reconciliation matrix as proposed by Wickramasuriya et al.\n\\[\\mathbf{P}_{\\text{MinTraceOLS}}=\\left(\\mathbf{S}^{\\intercal}\\mathbf{S}\\right)^{-1}\\mathbf{S}^{\\intercal}\\]\nParameters: S: Summing matrix of size (base, bottom).\nReturns: P: Reconciliation matrix of size (bottom, base).\nReferences: - Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). “Optimal non-negative forecast reconciliation”. Stat Comput 30, 1167–1182, https://doi.org/10.1007/s11222-020-09930-0.\n\nsource\n\n\nget_mintrace_wls_P\n\n get_mintrace_wls_P (S:numpy.ndarray)\n\nMinTraceOLS Reconciliation Matrix.\nCreates MinTraceOLS reconciliation matrix as proposed by Wickramasuriya et al. Depending on a weighted GLS estimator and an estimator of the covariance matrix of the coherency errors \\(\\mathbf{W}_{h}\\).\n\\[ \\mathbf{W}_{h} = \\mathrm{Diag}(\\mathbf{S} \\mathbb{1}_{[b]})\\]\n\\[\\mathbf{P}_{\\text{MinTraceWLS}}=\\left(\\mathbf{S}^{\\intercal}\\mathbf{W}_{h}\\mathbf{S}\\right)^{-1}\n\\mathbf{S}^{\\intercal}\\mathbf{W}^{-1}_{h}\\]\nParameters: S: Summing matrix of size (base, bottom).\nReturns: P: Reconciliation matrix of size (bottom, base).\nReferences: - Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). “Optimal non-negative forecast reconciliation”. Stat Comput 30, 1167–1182, https://doi.org/10.1007/s11222-020-09930-0."
  },
  {
    "objectID": "models.hint.html#hint",
    "href": "models.hint.html#hint",
    "title": "HINT",
    "section": "2. HINT",
    "text": "2. HINT\n\nsource\n\nHINT\n\n HINT (h:int, S:numpy.ndarray, model, reconciliation:str,\n       alias:Optional[str]=None)\n\nHINT\nThe Hierarchical Forecast Network (HINT) combines SoTA neural forecast methods with flexible and efficient probability distributions and advanced hierarchical reconciliation strategies. This powerful combination allows HINT to produce accurate and coherent probabilistic predictions.\nParameters: h: int, Forecast horizon.  model: NeuralForecast model, instantiated train loss class from models collection. S: np.ndarray, dumming matrix of size (base, bottom) see aggregate method. reconciliation: str, HINT’s reconciliation method from [‘BottomUp’, ‘MinTraceOLS’, ‘MinTraceWLS’]. alias: str, optional, Custom name of the model.\n\nsource\n\n\nHINT.__init__\n\n HINT.__init__ (h:int, S:numpy.ndarray, model, reconciliation:str,\n                alias:Optional[str]=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nHINT.fit\n\n HINT.fit (dataset, val_size=0, test_size=0, random_seed=None)\n\nHINT.fit\nHINT trains on the entire hierarchical dataset, by minimizing a composite log likelihood or likelihoood objective. HINT’s architecture integrates TemporalNorm for a scale-decoupled optimization that robustifies cross-learning the hierachy’s series scales.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset see details here val_size: int, size of the validation set, (default 0). test_size: int, size of the test set, (default 0). random_seed: int, random seed for the prediction.\nReturns: self: A fitted base NeuralForecast model.\n\nsource\n\n\nHINT.predict\n\n HINT.predict (dataset, step_size=1, random_seed=None,\n               **data_module_kwargs)\n\nHINT.predict\nAfter fitting a base model on the entire hierarchical dataset. HINT ensures hierarchical constraints using bootstrapped sample reconciliation First sampling from its base forecast distribution.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset see details here step_size: int, steps between sequential predictions, (default 1). random_seed: int, random seed for the prediction. **data_kwarg: additional parameters for the dataset module.\nReturns: y_hat: numpy predictions of the NeuralForecast model.\n\nsource\n\n\nHINT.save\n\n HINT.save (path)\n\nHINT.save\nSave the HINT fitted model to disk.\nParameters: path: str, path to save the model.\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss, PMM, NBMM\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\nfrom datasetsforecast.hierarchical import HierarchicalData\n\n# Load TourismSmall dataset\nY_df, S_df, tags = HierarchicalData.load('./data', 'TourismSmall')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df = sort_df_hier(Y_df, S_df)\n\n# Instantiate HINT\n# Model + Distribution + Reconciliation\nnhits = NHITS(h=4,\n              input_size=12,\n              loss=GMM(n_components=2, level=[80,90]),\n              max_steps=1000,\n              early_stop_patience_steps=2,\n              val_check_steps=100,\n              scaler_type='robust',\n              learning_rate=1e-3)\nmodel = HINT(h=4, model=nhits, S=S_df.values, reconciliation='MinTraceOLS')\n\n# Fit and Predict\nnf = NeuralForecast(models=[model], freq='Q')\nY_hat_df = nf.cross_validation(df=Y_df, val_size=4, n_windows=1)\n\n\n# Plot coherent quantile predictions\nunique_id = 'total'\nY_plot_df = Y_df[Y_df.unique_id==unique_id]\nplot_df = Y_hat_df[Y_hat_df.unique_id==unique_id]\nplot_df = Y_plot_df.merge(plot_df, on=['ds', 'unique_id'], how='left')\n\nplt.plot(plot_df['ds'], plot_df['y_x'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['HINT'], c='purple', label='mean')\nplt.plot(plot_df['ds'], plot_df['HINT-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-4:],\n                 y1=plot_df['HINT-lo-90'][-4:].values,\n                 y2=plot_df['HINT-hi-90'][-4:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "models.gru.html",
    "href": "models.gru.html",
    "title": "GRU",
    "section": "",
    "text": "Cho et. al proposed the Gated Recurrent Unit (GRU) to improve on LSTM and Elman cells. The predictions at each time are given by a MLP decoder. This architecture follows closely the original Multi Layer Elman RNN with the main difference being its use of the GRU cells. The predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{GRU}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nwhere \\(\\mathbf{h}_{t}\\), is the hidden state for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction.\nReferences -Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio (2014). “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling”. -Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio (2014). “On the Properties of Neural Machine Translation: Encoder-Decoder Approaches”.\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.gru.html#usage-example",
    "href": "models.gru.html#usage-example",
    "title": "GRU",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import GRU\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[GRU(h=12,input_size=-1,\n                loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                scaler_type='robust',\n                encoder_n_layers=2,\n                encoder_hidden_size=128,\n                context_size=10,\n                decoder_hidden_size=128,\n                decoder_layers=2,\n                max_steps=200,\n                futr_exog_list=None,\n                hist_exog_list=['y_[lag12]'],\n                stat_exog_list=['airline1'],\n                )\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['GRU-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['GRU-lo-90'][-12:].values, \n                 y2=plot_df['GRU-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "examples/uncertaintyintervals.html",
    "href": "examples/uncertaintyintervals.html",
    "title": "Probabilistic Forecasts",
    "section": "",
    "text": "Probabilistic forecasting is a natural answer to quantify the uncertainty of target variable’s future. The task requires to model the following conditional predictive distribution:\n\\[\\mathbb{P}(\\mathbf{y}_{t+1:t+H} \\;|\\; \\mathbf{y}_{:t})\\]\nWe will show you how to tackle the task with NeuralForecast by combining a classic Long Short Term Memory Network (LSTM) and the Neural Hierarchical Interpolation (NHITS) with the multi quantile loss function (MQLoss).\n\\[ \\mathrm{MQLoss}(y_{\\tau}, [\\hat{y}^{(q1)}_{\\tau},\\hat{y}^{(q2)}_{\\tau},\\dots,\\hat{y}^{(Q)}_{\\tau}]) = \\frac{1}{H} \\sum_{q} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) \\]\nIn this notebook we will: 1. Install NeuralForecast Library 2. Explore the M4-Hourly data. 3. Train the LSTM and NHITS 4. Visualize the LSTM/NHITS prediction intervals.\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#installing-neuralforecast",
    "href": "examples/uncertaintyintervals.html#installing-neuralforecast",
    "title": "Probabilistic Forecasts",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n\n# %%capture\n# !pip install git+https://github.com/Nixtla/neuralforecast.git@main\n\n\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import product\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss\nfrom neuralforecast.models import LSTM, DilatedRNN, NHITS\n\n\nUseful functions\nThe plot_grid auxiliary function defined below will be useful to plot different time series, and different models’ forecasts.\n\ndef plot_grid(df_train, df_test=None, plot_random=True, model=None, level=None):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) &gt;= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train')\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            for col in ['y', f'{model}-median', 'y_test']:\n                if col in test_uid:\n                    axes[idx, idy].plot(test_uid['ds'], test_uid[col], label=col)\n            if level is not None:\n                for l, alpha in zip(sorted(level), [0.5, .4, .35, .2]):\n                    axes[idx, idy].fill_between(\n                        test_uid['ds'], \n                        test_uid[f'{model}-lo-{l}.0'], \n                        test_uid[f'{model}-hi-{l}.0'],\n                        alpha=alpha,\n                        color='orange',\n                        label=f'{model}_level_{l}',\n                    )\n        axes[idx, idy].set_title(f'M4 Hourly: {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#loading-m4-data",
    "href": "examples/uncertaintyintervals.html#loading-m4-data",
    "title": "Probabilistic Forecasts",
    "section": "2. Loading M4 Data",
    "text": "2. Loading M4 Data\nFor testing purposes, we will use the Hourly dataset from the M4 competition.\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\n\n\nY_train_df = pd.read_csv('M4-Hourly.csv')\nY_test_df = pd.read_csv('M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\nIn this example we will use a subset of the data to avoid waiting too long. You can modify the number of series if you want.\n\nn_series = 8\nuids = Y_train_df['unique_id'].unique()[:n_series]\nY_train_df = Y_train_df.query('unique_id in @uids')\nY_test_df = Y_test_df.query('unique_id in @uids')\n\n\nplot_grid(Y_train_df, Y_test_df)"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#model-training",
    "href": "examples/uncertaintyintervals.html#model-training",
    "title": "Probabilistic Forecasts",
    "section": "3. Model Training",
    "text": "3. Model Training\nThe core.NeuralForecast provides a high-level interface with our collection of PyTorch models. NeuralForecast is instantiated with a list of models=[LSTM(...), NHITS(...)], configured for the forecasting task.\n\nThe horizon parameter controls the number of steps ahead of the predictions, in this example 48 hours ahead (2 days).\nThe MQLoss with levels=[80,90] specializes the network’s output into the 80% and 90% prediction intervals.\nThe max_epochs=500, controls the duration of the network’s training.\n\nFor more network’s instantiation details check their documentation.\n\nhorizon = 48\nlevels = [80, 90]\nmodels = [LSTM(input_size=-1, h=horizon,\n               loss=MQLoss(level=levels), max_epochs=300),\n          NHITS(input_size=7*horizon, h=horizon,\n                n_freq_downsample=[24, 12, 1],\n                loss=MQLoss(level=levels), max_epochs=200),]\nnf = NeuralForecast(models=models, freq='H')\n\nThe models are trained using cross-learning, that is a set of correlated series in Y_train_df is used during a shared optimization.\n\nnf.fit(df=Y_train_df)\n\n\nY_hat_df = nf.predict()\nY_hat_df = Y_hat_df.reset_index()\nY_hat_df.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nLSTM-median\nLSTM-lo-90.0\nLSTM-lo-80.0\nLSTM-hi-80.0\nLSTM-hi-90.0\nNHITS-median\nNHITS-lo-90.0\nNHITS-lo-80.0\nNHITS-hi-80.0\nNHITS-hi-90.0\n\n\n\n\n0\nH1\n701\n661.390320\n514.213257\n554.171082\n772.187317\n812.635864\n580.240234\n457.294403\n519.597900\n653.173462\n709.418884\n\n\n1\nH1\n702\n608.767395\n469.444824\n492.870117\n723.411987\n763.704773\n523.138855\n417.413483\n460.784607\n590.277100\n656.792969\n\n\n2\nH1\n703\n553.830139\n408.536591\n440.972717\n675.927063\n709.604065\n481.189453\n377.372284\n398.614319\n566.097351\n602.510559\n\n\n3\nH1\n704\n509.802368\n359.942810\n386.292145\n633.949219\n680.044678\n451.847473\n353.026123\n376.023682\n536.535156\n573.252197\n\n\n4\nH1\n705\n479.271240\n321.798004\n357.680634\n608.942566\n648.161865\n429.752136\n316.402679\n368.250336\n510.772217\n576.168213\n\n\n\n\n\n\n\n\nY_test_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#plotting-predictions",
    "href": "examples/uncertaintyintervals.html#plotting-predictions",
    "title": "Probabilistic Forecasts",
    "section": "4. Plotting Predictions",
    "text": "4. Plotting Predictions\nHere we finalize our analysis by plotting the prediction intervals and verifying that both the LSTM and NHITS are giving reasonable results.\nConsider the output [NHITS-lo-90.0, NHITS-hi-90.0], that represents the 80% prediction interval of the NHITS network; its lower limit gives the 5th percentile (or 0.05 quantile) while its upper limit gives the 95th percentile (or 0.95 quantile). For well-trained models we expect that the target values lie within the interval 90% of the time.\n\nLSTM\n\nplot_grid(Y_train_df, Y_test_df, level=levels, model='LSTM')\n\n\n\n\n\n\nNHITS\n\nplot_grid(Y_train_df, Y_test_df, level=levels, model='NHITS')"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#references",
    "href": "examples/uncertaintyintervals.html#references",
    "title": "Probabilistic Forecasts",
    "section": "References",
    "text": "References\n\nRoger Koenker and Gilbert Basset (1978). Regression Quantiles, Econometrica.\nJeffrey L. Elman (1990). “Finding Structure in Time”.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2022). “NHITS: Neural Hierarchical Interpolation for Time Series Forecasting”."
  },
  {
    "objectID": "examples/data_format.html",
    "href": "examples/data_format.html",
    "title": "Data Inputs",
    "section": "",
    "text": "In this example we will go through the dataset input requirements of the core.NeuralForecast class.\nThe core.NeuralForecast methods operate as global models that receive a set of time series rather than single series. The class uses cross-learning technique to fit flexible-shared models such as neural networks improving its generalization capabilities as shown by the M4 international forecasting competition (Smyl 2019, Semenoglou 2021).\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/data_format.html#long-format",
    "href": "examples/data_format.html#long-format",
    "title": "Data Inputs",
    "section": "Long format",
    "text": "Long format\n\nMultiple time series\nStore your time series in a pandas dataframe in long format, that is, each row represents an observation for a specific series and timestamp. Let’s see an example using the datasetsforecast library.\nY_df = pd.concat( [series1, series2, ...])\n\n!pip install datasetsforecast\n\n\nimport pandas as pd\nfrom datasetsforecast.m3 import M3\n\n\nY_df, *_ = M3.load('./data', group='Yearly')\n\n100%|██████████| 1.76M/1.76M [00:00&lt;00:00, 5.55MiB/s]\nINFO:datasetsforecast.utils:Successfully downloaded M3C.xls, 1757696, bytes.\n\n\n\nY_df.groupby('unique_id').head(2)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nY1\n1975-12-31\n940.66\n\n\n1\nY1\n1976-12-31\n1084.86\n\n\n20\nY10\n1975-12-31\n2160.04\n\n\n21\nY10\n1976-12-31\n2553.48\n\n\n40\nY100\n1975-12-31\n1424.70\n\n\n...\n...\n...\n...\n\n\n18260\nY97\n1976-12-31\n1618.91\n\n\n18279\nY98\n1975-12-31\n1164.97\n\n\n18280\nY98\n1976-12-31\n1277.87\n\n\n18299\nY99\n1975-12-31\n1870.00\n\n\n18300\nY99\n1976-12-31\n1307.20\n\n\n\n\n1290 rows × 3 columns\n\n\n\n\nY_df.groupby('unique_id').tail(2)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n18\nY1\n1993-12-31\n8407.84\n\n\n19\nY1\n1994-12-31\n9156.01\n\n\n38\nY10\n1993-12-31\n3187.00\n\n\n39\nY10\n1994-12-31\n3058.00\n\n\n58\nY100\n1993-12-31\n3539.00\n\n\n...\n...\n...\n...\n\n\n18278\nY97\n1994-12-31\n4507.00\n\n\n18297\nY98\n1993-12-31\n1801.00\n\n\n18298\nY98\n1994-12-31\n1710.00\n\n\n18317\nY99\n1993-12-31\n2379.30\n\n\n18318\nY99\n1994-12-31\n2723.00\n\n\n\n\n1290 rows × 3 columns\n\n\n\nY_df is a dataframe with three columns: unique_id with a unique identifier for each time series, a column ds with the datestamp and a column y with the values of the series.\n\n\nSingle time series\nIf you have only one time series, you have to include the unique_id column. Consider, for example, the AirPassengers dataset.\n\nY_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/air_passengers.csv')\n\nIn this example Y_df only contains two columns: timestamp, and value. To use NeuralForecast we have to include the unique_id column and rename the previuos ones.\n\nY_df['unique_id'] = 1. # We can add an integer as identifier\nY_df = Y_df.rename(columns={'timestamp': 'ds', 'value': 'y'})\nY_df = Y_df[['unique_id', 'ds', 'y']]\n\n\nY_df\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n1.0\n1949-01-01\n112\n\n\n1\n1.0\n1949-02-01\n118\n\n\n2\n1.0\n1949-03-01\n132\n\n\n3\n1.0\n1949-04-01\n129\n\n\n4\n1.0\n1949-05-01\n121\n\n\n...\n...\n...\n...\n\n\n139\n1.0\n1960-08-01\n606\n\n\n140\n1.0\n1960-09-01\n508\n\n\n141\n1.0\n1960-10-01\n461\n\n\n142\n1.0\n1960-11-01\n390\n\n\n143\n1.0\n1960-12-01\n432\n\n\n\n\n144 rows × 3 columns"
  },
  {
    "objectID": "examples/data_format.html#references",
    "href": "examples/data_format.html#references",
    "title": "Data Inputs",
    "section": "References",
    "text": "References\n\nSlawek Smyl. (2019). “A hybrid method of exponential smoothing and recurrent networks for time series forecasting”. International Journal of Forecasting.\nArtemios-Anargyros Semenoglou, Evangelos Spiliotis, Spyros Makridakis, and Vassilios Assimakopoulos. (2021). Investigating the accuracy of cross-learning time series forecasting methods”. International Journal of Forecasting."
  },
  {
    "objectID": "examples/installation.html",
    "href": "examples/installation.html",
    "title": "Install",
    "section": "",
    "text": "You can install the released version of NeuralForecast from the Python package index with:\npip install neuralforecast\nor\nconda install -c conda-forge neuralforecast\n\n\n\n\n\n\nTip\n\n\n\nNeural Forecasting methods profit from using GPU computation. Be sure to have Cuda installed.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe are constantly updating neuralforecast, so we suggest fixing the version to avoid issues. pip install neuralforecast==\"1.0.0\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe recommend installing your libraries inside a python virtual or conda environment.\n\n\n\nUser our env (optional)\nIf you don’t have a Conda environment and need tools like Numba, Pandas, NumPy, Jupyter, Tune, and Nbdev you can use ours by following these steps:\n\nClone the NeuralForecast repo:\n\n$ git clone https://github.com/Nixtla/neuralforecast.git && cd neuralforecast\n\nCreate the environment using the environment.yml file:\n\n$ conda env create -f environment.yml\n\nActivate the environment:\n\n$ conda activate neuralforecast\n\nInstall NeuralForecast Dev\n\n$ pip install -e \".[dev]\"\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html",
    "href": "examples/longhorizon_with_nhits.html",
    "title": "Long-Horizon Forecast",
    "section": "",
    "text": "Long-horizon forecasting is challenging because of the volatility of the predictions and the computational complexity. To solve this problem we created the NHITS model and made the code available NeuralForecast library. NHITS specializes its partial outputs in the different frequencies of the time series through hierarchical interpolation and multi-rate input processing.\nIn this notebook we show how to use N-HiTS on the ETTm2 benchmark dataset. This data set includes data points for 2 Electricity Transformers at 2 stations, including load, oil temperature.\nWe will show you how to load data, train, and perform automatic hyperparameter tuning, to achieve SoTA performance, outperforming even the latest Transformer architectures for a fraction of their computational cost (50x faster).\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#installing-neuralforecast",
    "href": "examples/longhorizon_with_nhits.html#installing-neuralforecast",
    "title": "Long-Horizon Forecast",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n\n!pip install neuralforecast datasetsforecast\n\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom ray import tune\n\nfrom neuralforecast.auto import AutoNHITS\nfrom neuralforecast.core import NeuralForecast\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.losses.numpy import mae, mse\nfrom datasetsforecast.long_horizon import LongHorizon\n\n\nimport logging\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n\nThis example will automatically run on GPUs if available. Make sure cuda is available. (If you need help to put this into production send us an email or join or community, we also offer a fully hosted solution)\n\ntorch.cuda.is_available()\n\nTrue"
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#load-ettm2-data",
    "href": "examples/longhorizon_with_nhits.html#load-ettm2-data",
    "title": "Long-Horizon Forecast",
    "section": "2. Load ETTm2 Data",
    "text": "2. Load ETTm2 Data\nThe LongHorizon class will automatically download the complete ETTm2 dataset and process it.\nIt return three Dataframes: Y_df contains the values for the target variables, X_df contains exogenous calendar features and S_df contains static features for each time-series (none for ETTm2). For this example we will only use Y_df.\nIf you want to use your own data just replace Y_df. Be sure to use a long format and have a simmilar structure than our data set.\n\n# Change this to your own data to try the model\nY_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n# For this excercise we are going to take 20% of the DataSet\nn_time = len(Y_df.ds.unique())\nval_size = int(.2 * n_time)\ntest_size = int(.2 * n_time)\n\nY_df.groupby('unique_id').head(2)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nHUFL\n2016-07-01 00:00:00\n-0.041413\n\n\n1\nHUFL\n2016-07-01 00:15:00\n-0.185467\n\n\n57600\nHULL\n2016-07-01 00:00:00\n0.040104\n\n\n57601\nHULL\n2016-07-01 00:15:00\n-0.214450\n\n\n115200\nLUFL\n2016-07-01 00:00:00\n0.695804\n\n\n115201\nLUFL\n2016-07-01 00:15:00\n0.434685\n\n\n172800\nLULL\n2016-07-01 00:00:00\n0.434430\n\n\n172801\nLULL\n2016-07-01 00:15:00\n0.428168\n\n\n230400\nMUFL\n2016-07-01 00:00:00\n-0.599211\n\n\n230401\nMUFL\n2016-07-01 00:15:00\n-0.658068\n\n\n288000\nMULL\n2016-07-01 00:00:00\n-0.393536\n\n\n288001\nMULL\n2016-07-01 00:15:00\n-0.659338\n\n\n345600\nOT\n2016-07-01 00:00:00\n1.018032\n\n\n345601\nOT\n2016-07-01 00:15:00\n0.980124\n\n\n\n\n\n\n\n\n# We are going to plot the temperature of the transformer \n# and marking the validation and train splits\nu_id = 'HUFL'\nx_plot = pd.to_datetime(Y_df[Y_df.unique_id==u_id].ds)\ny_plot = Y_df[Y_df.unique_id==u_id].y.values\n\nx_val = x_plot[n_time - val_size - test_size]\nx_test = x_plot[n_time - test_size]\n\nfig = plt.figure(figsize=(10, 5))\nfig.tight_layout()\n\nplt.plot(x_plot, y_plot)\nplt.xlabel('Date', fontsize=17)\nplt.ylabel('HUFL [15 min temperature]', fontsize=17)\n\nplt.axvline(x_val, color='black', linestyle='-.')\nplt.axvline(x_test, color='black', linestyle='-.')\nplt.text(x_val, 5, '  Validation', fontsize=12)\nplt.text(x_test, 5, '  Test', fontsize=12)\n\nplt.grid()"
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#hyperparameter-selection-and-forecasting",
    "href": "examples/longhorizon_with_nhits.html#hyperparameter-selection-and-forecasting",
    "title": "Long-Horizon Forecast",
    "section": "3. Hyperparameter selection and forecasting",
    "text": "3. Hyperparameter selection and forecasting\nThe AutoNHITS class will automatically perform hyperparamter tunning using Tune library, exploring a user-defined or default search space. Models are selected based on the error on a validation set and the best model is then stored and used during inference.\nThe AutoNHITS.default_config attribute contains a suggested hyperparameter space. Here, we specify a different search space following the paper’s hyperparameters. Notice that 1000 Stochastic Gradient Steps are enough to achieve SoTA performance. Feel free to play around with this space.\n\nhorizon = 96 # 24hrs = 4 * 15 min.\n\n# Use your own config or AutoNHITS.default_config\nnhits_config = {\n       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n       \"max_steps\": tune.choice([1000]),                                         # Number of SGD steps\n       \"input_size\": tune.choice([5 * horizon]),                                 # input_size = multiplier * horizon\n       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n       \"val_check_steps\": tune.choice([100]),                                    # Compute validation every 100 epochs\n       \"random_seed\": tune.randint(1, 10),\n    }\n\n\n\n\n\n\n\nTip\n\n\n\nRefer to https://docs.ray.io/en/latest/tune/index.html for more information on the different space options, such as lists and continous intervals.m\n\n\nTo instantiate AutoNHITS you need to define:\n\nh: forecasting horizon\nloss: training loss. Use the DistributionLoss to produce probabilistic forecasts.\nconfig: hyperparameter search space. If None, the AutoNHITS class will use a pre-defined suggested hyperparameter space.\nnum_samples: number of configurations explored.\n\n\nmodels = [AutoNHITS(h=horizon,\n                    config=nhits_config, \n                    num_samples=5)]\n\nINFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp0ke5wzvj\nINFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp0ke5wzvj/_remote_module_non_scriptable.py\n\n\nFit the model by instantiating a NeuralForecast object with the following required parameters:\n\nmodels: a list of models.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\nThe cross_validation method allows you to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods.\nWith time series data, cross validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe cross_validation method will use the validation set for hyperparameter selection, and will then produce the forecasts for the test set.\n\nnf = NeuralForecast(\n    models=models,\n    freq='15min')\n\nY_hat_df = nf.cross_validation(df=Y_df, val_size=val_size,\n                               test_size=test_size, n_windows=None)\n\nINFO:ray.tune.tune:Initializing Ray automatically.For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run`."
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#evaluate-results",
    "href": "examples/longhorizon_with_nhits.html#evaluate-results",
    "title": "Long-Horizon Forecast",
    "section": "4. Evaluate Results",
    "text": "4. Evaluate Results\nThe AutoNHITS class contains a results tune attribute that stores information of each configuration explored. It contains the validation loss and best validation hyperparameter.\n\nnf.models[0].results.get_best_result().config\n\n{'learning_rate': 0.001,\n 'max_steps': 1000,\n 'input_size': 480,\n 'batch_size': 7,\n 'windows_batch_size': 256,\n 'n_pool_kernel_size': [2, 2, 2],\n 'n_freq_downsample': [24, 12, 1],\n 'activation': 'ReLU',\n 'n_blocks': [1, 1, 1],\n 'mlp_units': [[512, 512], [512, 512], [512, 512]],\n 'interpolation_mode': 'linear',\n 'check_val_every_n_epoch': 100,\n 'random_seed': 4,\n 'h': 96,\n 'loss': MAE()}\n\n\n\ny_true = Y_hat_df.y.values\ny_hat = Y_hat_df['AutoNHITS'].values\n\nn_series = len(Y_df.unique_id.unique())\n\ny_true = y_true.reshape(n_series, -1, horizon)\ny_hat = y_hat.reshape(n_series, -1, horizon)\n\nprint('Parsed results')\nprint('2. y_true.shape (n_series, n_windows, n_time_out):\\t', y_true.shape)\nprint('2. y_hat.shape  (n_series, n_windows, n_time_out):\\t', y_hat.shape)\n\nParsed results\n2. y_true.shape (n_series, n_windows, n_time_out):   (7, 11425, 96)\n2. y_hat.shape  (n_series, n_windows, n_time_out):   (7, 11425, 96)\n\n\n\nfig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10, 11))\nfig.tight_layout()\n\nseries = ['HUFL','HULL','LUFL','LULL','MUFL','MULL','OT']\nseries_idx = 3\n\nfor idx, w_idx in enumerate([200, 300, 400]):\n  axs[idx].plot(y_true[series_idx, w_idx,:],label='True')\n  axs[idx].plot(y_hat[series_idx, w_idx,:],label='Forecast')\n  axs[idx].grid()\n  axs[idx].set_ylabel(series[series_idx]+f' window {w_idx}', \n                      fontsize=17)\n  if idx==2:\n    axs[idx].set_xlabel('Forecast Horizon', fontsize=17)\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\nFinally, we compute the test errors for the two metrics of interest (which are also available in the trials object):\n\\(\\qquad MAE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}| \\qquad\\) and \\(\\qquad MSE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\qquad\\)\n\nprint('MAE: ', mae(y_hat, y_true))\nprint('MSE: ', mse(y_hat, y_true))\n\nMAE:  0.26096806135482414\nMSE:  0.18279484416711375\n\n\nFor reference we can check the performance when compared to previous ‘state-of-the-art’ long-horizon Transformer-based forecasting methods from the N-HiTS paper. To recover or improve the paper results try setting hyperopt_max_evals=30 in Hyperparameter Tuning.\nMean Absolute Error (MAE):\n\n\n\nHorizon\nN-HiTS\nAutoFormer\nInFormer\nARIMA\n\n\n\n\n96\n0.255\n0.339\n0.453\n0.301\n\n\n192\n0.305\n0.340\n0.563\n0.345\n\n\n336\n0.346\n0.372\n0.887\n0.386\n\n\n720\n0.426\n0.419\n1.388\n0.445\n\n\n\nMean Squared Error (MSE):\n\n\n\nHorizon\nN-HiTS\nAutoFormer\nInFormer\nARIMA\n\n\n\n\n96\n0.176\n0.255\n0.365\n0.225\n\n\n192\n0.245\n0.281\n0.533\n0.298\n\n\n336\n0.295\n0.339\n1.363\n0.370\n\n\n720\n0.401\n0.422\n3.379\n0.478"
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#references",
    "href": "examples/longhorizon_with_nhits.html#references",
    "title": "Long-Horizon Forecast",
    "section": "References",
    "text": "References\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023."
  },
  {
    "objectID": "examples/intermittentdata.html",
    "href": "examples/intermittentdata.html",
    "title": "Intermittent or Sparse Time Series (M5 Walmart)",
    "section": "",
    "text": "Intermittent or sparse data has very few non-zero observations. This type of data is hard to forecast because the zero values increase the uncertainty about the underlying patterns in the data. Furthermore, once a non-zero observation occurs, there can be considerable variation in its size. Intermittent time series are common in many industries, including finance, retail, transportation, and energy. Given the ubiquity of this type of series, special methods have been developed to forecast them. The first was from Croston (1972), followed by several variants and by different aggregation frameworks.\nThe models of NeuralForecast can be trained to model sparse or intermittent time series using a Poisson distribution loss. By the end of this tutorial, you’ll have a good understanding of these models and how to use them.\nOutline:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/intermittentdata.html#install-libraries",
    "href": "examples/intermittentdata.html#install-libraries",
    "title": "Intermittent or Sparse Time Series (M5 Walmart)",
    "section": "1. Install libraries",
    "text": "1. Install libraries\nWe assume that you have NeuralForecast already installed. If not, check this guide for instructions on how to install NeuralForecast\nInstall the necessary packages using pip install neuralforecast\n\n!pip install neuralforecast statsforecast s3fs fastparquet"
  },
  {
    "objectID": "examples/intermittentdata.html#load-and-explore-the-data",
    "href": "examples/intermittentdata.html#load-and-explore-the-data",
    "title": "Intermittent or Sparse Time Series (M5 Walmart)",
    "section": "2. Load and explore the data",
    "text": "2. Load and explore the data\nFor this example, we’ll use a subset of the M5 Competition dataset. Each time series represents the unit sales of a particular product in a given Walmart store. At this level (product-store), most of the data is intermittent. We first need to import the data.\n\nimport pandas as pd\nfrom statsforecast import StatsForecast as sf\n\n/usr/local/lib/python3.8/dist-packages/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\n\nY_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet')\nY_df = Y_df.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n})\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\nFor simplicity sake we will keep just one category\n\nY_df = Y_df.query('unique_id.str.startswith(\"FOODS_3\")')\nY_df['unique_id'] = Y_df['unique_id'].astype(str)\nY_df = Y_df.reset_index(drop=True)\n\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\nsf.plot(Y_df, engine='matplotlib')"
  },
  {
    "objectID": "examples/intermittentdata.html#train-models-for-intermittent-data",
    "href": "examples/intermittentdata.html#train-models-for-intermittent-data",
    "title": "Intermittent or Sparse Time Series (M5 Walmart)",
    "section": "3. Train models for intermittent data",
    "text": "3. Train models for intermittent data\n\nfrom ray import tune\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.auto import AutoNHITS, AutoTFT\nfrom neuralforecast.losses.pytorch import DistributionLoss\n\nEach Auto model contains a default search space that was extensively tested on multiple large-scale datasets. Additionally, users can define specific search spaces tailored for particular datasets and tasks.\nFirst, we create a custom search space for the AutoNHITS and AutoTFT models. Search spaces are specified with dictionaries, where keys corresponds to the model’s hyperparameter and the value is a Tune function to specify how the hyperparameter will be sampled. For example, use randint to sample integers uniformly, and choice to sample values of a list.\n\nconfig_nhits = {\n    \"input_size\": tune.choice([28, 28*2, 28*3, 28*5]),              # Length of input window\n    \"n_blocks\": 5*[1],                                              # Length of input window\n    \"mlp_units\": 5 * [[512, 512]],                                  # Length of input window\n    \"n_pool_kernel_size\": tune.choice([5*[1], 5*[2], 5*[4],         \n                                      [8, 4, 2, 1, 1]]),            # MaxPooling Kernel size\n    \"n_freq_downsample\": tune.choice([[8, 4, 2, 1, 1],\n                                      [1, 1, 1, 1, 1]]),            # Interpolation expressivity ratios\n    \"learning_rate\": tune.loguniform(1e-4, 1e-2),                   # Initial Learning rate\n    \"scaler_type\": tune.choice([None]),                             # Scaler type\n    \"max_steps\": tune.choice([1000]),                               # Max number of training iterations\n    \"batch_size\": tune.choice([32, 64, 128, 256]),                  # Number of series in batch\n    \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),       # Number of windows in batch\n    \"random_seed\": tune.randint(1, 20),                             # Random seed\n}\n\nconfig_tft = {\n        \"input_size\": tune.choice([28, 28*2, 28*3]),                # Length of input window\n        \"hidden_size\": tune.choice([64, 128, 256]),                 # Size of embeddings and encoders\n        \"learning_rate\": tune.loguniform(1e-4, 1e-2),               # Initial learning rate\n        \"scaler_type\": tune.choice([None]),                         # Scaler type\n        \"max_steps\": tune.choice([500, 1000]),                      # Max number of training iterations\n        \"batch_size\": tune.choice([32, 64, 128, 256]),              # Number of series in batch\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),   # Number of windows in batch\n        \"random_seed\": tune.randint(1, 20),                         # Random seed\n    }\n\nTo instantiate an Auto model you need to define:\n\nh: forecasting horizon.\nloss: training and validation loss from neuralforecast.losses.pytorch.\nconfig: hyperparameter search space. If None, the Auto class will use a pre-defined suggested hyperparameter space.\nsearch_alg: search algorithm (from tune.search), default is random search. Refer to https://docs.ray.io/en/latest/tune/api_docs/suggestion.html for more information on the different search algorithm options.\nnum_samples: number of configurations explored.\n\nIn this example we set horizon h as 28, use the Poisson distribution loss (ideal for count data) for training and validation, and use the default search algorithm.\n\nnf = NeuralForecast(\n    models=[\n        AutoNHITS(h=28, config=config_nhits, loss=DistributionLoss(distribution='Poisson', level=[80, 90], return_params=False), num_samples=5),\n        AutoTFT(h=28, config=config_tft, loss=DistributionLoss(distribution='Poisson', level=[80, 90], return_params=False), num_samples=2), \n    ],\n    freq='D'\n)\n\n\n\n\n\n\n\nTip\n\n\n\nThe number of samples, num_samples, is a crucial parameter! Larger values will usually produce better results as we explore more configurations in the search space, but it will increase training times. Larger search spaces will usually require more samples. As a general rule, we recommend setting num_samples higher than 20.\n\n\nNext, we use the Neuralforecast class to train the Auto model. In this step, Auto models will automatically perform hyperparamter tuning training multiple models with different hyperparameters, producing the forecasts on the validation set, and evaluating them. The best configuration is selected based on the error on a validation set. Only the best model is stored and used during inference.\n\nnf.fit(df=Y_df)\n\nNext, we use the predict method to forecast the next 28 days using the optimal hyperparameters.\n\nfcst_df = nf.predict()\n\n\n\n\n\n\n\n\nfcst_df.columns = fcst_df.columns.str.replace('-median', '')\n\n\nsf.plot(Y_df, fcst_df, engine='matplotlib', max_insample_length=28 * 3)"
  },
  {
    "objectID": "examples/intermittentdata.html#cross-validation",
    "href": "examples/intermittentdata.html#cross-validation",
    "title": "Intermittent or Sparse Time Series (M5 Walmart)",
    "section": "4. Cross Validation",
    "text": "4. Cross Validation\nTime series cross-validation is a method for evaluating how a model would have performed in the past. It works by defining a sliding window across the historical data and predicting the period following it.\n\nNeuralForecast has an implementation of time series cross-validation that is fast and easy to use.\nThe cross_validation method from the NeuralForecast class takes the following arguments.\n\ndf: training data frame\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows (int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\nnf = NeuralForecast(\n    models=[\n        AutoNHITS(h=28, config=config_nhits, loss=DistributionLoss(distribution='Poisson', level=[80, 90], return_params=False), num_samples=5),\n        AutoTFT(h=28, config=config_tft, loss=DistributionLoss(distribution='Poisson', level=[80, 90], return_params=False), num_samples=2), \n    ],\n    freq='D'\n)\n\n\ncv_df = nf.cross_validation(Y_df, n_windows=3, step_size=28)\n\nThe cv_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncv_df.columns = cv_df.columns.str.replace('-median', '')\n\n\ncv_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nunique_id\nds\ncutoff\nAutoNHITS\nAutoNHITS-lo-90.0\nAutoNHITS-lo-80.0\nAutoNHITS-hi-80.0\nAutoNHITS-hi-90.0\nAutoTFT\nAutoTFT-lo-90.0\nAutoTFT-lo-80.0\nAutoTFT-hi-80.0\nAutoTFT-hi-90.0\ny\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n0.0\n0.0\n2.0\n2.0\n1.0\n0.0\n0.0\n2.0\n2.0\n0.0\n\n\n1\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n0.0\n0.0\n0.0\n2.0\n2.0\n1.0\n0.0\n0.0\n2.0\n2.0\n1.0\n\n\n2\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n0.0\n0.0\n0.0\n2.0\n2.0\n1.0\n0.0\n0.0\n2.0\n2.0\n1.0\n\n\n3\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n0.0\n0.0\n2.0\n2.0\n1.0\n0.0\n0.0\n2.0\n2.0\n0.0\n\n\n4\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n0.0\n0.0\n2.0\n2.0\n0.0\n0.0\n0.0\n2.0\n2.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nfor cutoff in cv_df['cutoff'].unique():\n    sf.plot(Y_df, \n            cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']), \n            max_insample_length=28 * 5, \n            unique_ids=['FOODS_3_001_CA_1'],\n            engine='matplotlib')\n\n\n\n\n\n\n\n\n\n\n\nEvaluate\nIn this section we will evaluate the performance of each model each cross validation window using the MSE metric.\n\nfrom neuralforecast.losses.numpy import mse, mae\n\n\ndef evaluate(df):\n    eval_ = {}\n    models = df.loc[:, ~df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\n    for model in models:\n        eval_[model] = {}\n        for metric in [mse, mae]:\n            eval_[model][metric.__name__] = metric(df['y'].values, df[model].values)\n    eval_df = pd.DataFrame(eval_).rename_axis('metric')\n    return eval_df\n\n\ncv_df.groupby('cutoff').apply(lambda df: evaluate(df))\n\n\n  \n    \n      \n\n\n\n\n\n\n\nAutoNHITS\nAutoTFT\n\n\ncutoff\nmetric\n\n\n\n\n\n\n2016-02-28\nmse\n10.274085\n15.240116\n\n\nmae\n1.445398\n1.511810\n\n\n2016-03-27\nmse\n9.533789\n14.307356\n\n\nmae\n1.445806\n1.520717\n\n\n2016-04-24\nmse\n9.561473\n14.719155\n\n\nmae\n1.455149\n1.534106"
  },
  {
    "objectID": "examples/intermittentdata.html#references",
    "href": "examples/intermittentdata.html#references",
    "title": "Intermittent or Sparse Time Series (M5 Walmart)",
    "section": "References",
    "text": "References\n\nCroston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023."
  },
  {
    "objectID": "examples/getting_started_complete.html",
    "href": "examples/getting_started_complete.html",
    "title": "End to End Walkthrough",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with NeuralForecast. For a minimal example visit the Quick Start\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core NueralForecastclass and some relevant methods like NeuralForecast.fit, NeuralForecast.predict, and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series globally Therefore, you will train a set of models for the whole dataset, and then select the best model for each individual time series. NeuralForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/getting_started_complete.html#install-libraries",
    "href": "examples/getting_started_complete.html#install-libraries",
    "title": "End to End Walkthrough",
    "section": "1. Install libraries",
    "text": "1. Install libraries\nWe assume you have NeuralForecast already installed. Check this guide for instructions on how to install NeuralForecast.\nAdditionally, we will install s3fs to read from the S3 Filesystem of AWS, statsforecast for plotting, and datasetsforecast for common error metrics like MAE or MASE.\nInstall the necessary packages using pip install statsforecast s3fs datasetsforecast ``\n\n! pip install neuralforecast statsforecast s3fs datasetsforecast"
  },
  {
    "objectID": "examples/getting_started_complete.html#read-the-data",
    "href": "examples/getting_started_complete.html#read-the-data",
    "title": "End to End Walkthrough",
    "section": "2. Read the data",
    "text": "2. Read the data\nWe will use pandas to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to NeuralForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nThis data set already satisfies the requirement.\nDepending on your internet connection, this step should take around 10 seconds.\n\nimport pandas as pd\n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n\nY_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility’s sake, we will select only 10 unique IDs and keep only the last week. Depending on your processing infrastructure feel free to select more or less series.\n\n\n\n\n\n\nNote\n\n\n\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n\n\n\n#uids = Y_df['unique_id'].unique()[:10] # Select 10 ids to make the example faster\n\n#Y_df = Y_df.query('unique_id in @uids') \nmax_len = 10 * 24\nY_df = Y_df.groupby('unique_id').tail(10 * 24) #Select last 10 days of data to make example faster\n# ensure ds ends at the same point, avoid errors in cv\nY_df['ds'] = Y_df.groupby('unique_id')['ds'].transform(lambda x: range(len(x) - max_len + 1, max_len + 1))"
  },
  {
    "objectID": "examples/getting_started_complete.html#explore-data-with-the-plot-method-of-statsforecast",
    "href": "examples/getting_started_complete.html#explore-data-with-the-plot-method-of-statsforecast",
    "title": "End to End Walkthrough",
    "section": "3. Explore Data with the plot method of StatsForecast",
    "text": "3. Explore Data with the plot method of StatsForecast\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a defaul engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df, engine='matplotlib')\n\n/usr/local/lib/python3.9/dist-packages/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm"
  },
  {
    "objectID": "examples/getting_started_complete.html#train-multiple-models-for-many-series",
    "href": "examples/getting_started_complete.html#train-multiple-models-for-many-series",
    "title": "End to End Walkthrough",
    "section": "4. Train multiple models for many series",
    "text": "4. Train multiple models for many series\nNeuralForecast can train many models on many time series globally and efficiently.\n\nfrom ray import tune\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.auto import AutoNHITS, AutoLSTM\nfrom neuralforecast.losses.pytorch import MQLoss\n\nEach Auto model contains a default search space that was extensively tested on multiple large-scale datasets. Additionally, users can define specific search spaces tailored for particular datasets and tasks.\nFirst, we create a custom search space for the AutoNHITS and AutoTFT models. Search spaces are specified with dictionaries, where keys corresponds to the model’s hyperparameter and the value is a Tune function to specify how the hyperparameter will be sampled. For example, use randint to sample integers uniformly, and choice to sample values of a list.\n\nconfig_nhits = {\n    \"input_size\": tune.choice([48, 48*2, 48*3, 48*5]),              # Length of input window\n    \"n_blocks\": 5*[1],                                              # Length of input window\n    \"mlp_units\": 5 * [[64, 64]],                                  # Length of input window\n    \"n_pool_kernel_size\": tune.choice([5*[1], 5*[2], 5*[4],         \n                                      [8, 4, 2, 1, 1]]),            # MaxPooling Kernel size\n    \"n_freq_downsample\": tune.choice([[8, 4, 2, 1, 1],\n                                      [1, 1, 1, 1, 1]]),            # Interpolation expressivity ratios\n    \"learning_rate\": tune.loguniform(1e-4, 1e-2),                   # Initial Learning rate\n    \"scaler_type\": tune.choice([None]),                             # Scaler type\n    \"max_steps\": tune.choice([1000]),                               # Max number of training iterations\n    \"batch_size\": tune.choice([32, 64, 128, 256]),                  # Number of series in batch\n    \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),       # Number of windows in batch\n    \"random_seed\": tune.randint(1, 20),                             # Random seed\n}\n\nTo instantiate an Auto model you need to define:\n\nh: forecasting horizon.\nloss: training and validation loss from neuralforecast.losses.pytorch.\nconfig: hyperparameter search space. If None, the Auto class will use a pre-defined suggested hyperparameter space.\nsearch_alg: search algorithm (from tune.search), default is random search. Refer to https://docs.ray.io/en/latest/tune/api_docs/suggestion.html for more information on the different search algorithm options.\nnum_samples: number of configurations explored.\n\nIn this example we set horizon h as 48, use the MQLoss distribution loss for training and validation, and use the default search algorithm.\n\nnf = NeuralForecast(\n    models=[\n        AutoNHITS(h=48, config=config_nhits, loss=MQLoss(), num_samples=5),\n        AutoLSTM(h=48, loss=MQLoss(), num_samples=2), \n    ],\n    freq='H'\n)\n\n\n\n\n\n\n\nTip\n\n\n\nThe number of samples, num_samples, is a crucial parameter! Larger values will usually produce better results as we explore more configurations in the search space, but it will increase training times. Larger search spaces will usually require more samples. As a general rule, we recommend setting num_samples higher than 20.\n\n\nNext, we use the Neuralforecast class to train the Auto model. In this step, Auto models will automatically perform hyperparameter tuning training multiple models with different hyperparameters, producing the forecasts on the validation set, and evaluating them. The best configuration is selected based on the error on a validation set. Only the best model is stored and used during inference.\n\nnf.fit(df=Y_df)\n\nNext, we use the predict method to forecast the next 48 days using the optimal hyperparameters.\n\nfcst_df = nf.predict()\n\n\n\n\n\n\n\n\nfcst_df.columns = fcst_df.columns.str.replace('-median', '')\n\n\nfcst_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nds\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-lo-80\nAutoNHITS-hi-80\nAutoNHITS-hi-90\nAutoLSTM\nAutoLSTM-lo-90\nAutoLSTM-lo-80\nAutoLSTM-hi-80\nAutoLSTM-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n241\n601.250183\n563.528809\n569.778992\n616.412476\n619.268127\n612.727234\n575.606018\n581.778259\n638.676331\n653.332764\n\n\nH1\n242\n568.007385\n533.618042\n545.094604\n590.567688\n595.225342\n561.754028\n510.381958\n526.268066\n596.034912\n611.646851\n\n\nH1\n243\n506.227783\n476.450897\n488.572205\n539.061462\n550.607544\n534.809692\n486.517761\n499.971680\n566.240051\n581.725769\n\n\nH1\n244\n473.553070\n445.319397\n451.538269\n508.350586\n528.872742\n509.679291\n466.653137\n479.000336\n537.448975\n550.369507\n\n\nH1\n245\n459.816711\n431.483490\n433.343292\n494.174377\n522.226929\n485.630920\n446.374634\n457.723602\n515.539124\n529.474731\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nStatsForecast.plot(Y_df, fcst_df, engine='matplotlib', max_insample_length=48 * 3, level=[80, 90])\n\n\n\n\nThe StatsForecast.plot allows for further customization. For example, plot the results of the different models and unique ids.\n\n# Plot to unique_ids and some selected models\nStatsForecast.plot(Y_df, fcst_df, models=[\"AutoLSTM\"], unique_ids=[\"H185\", \"H221\"], level=[80, 90], engine='matplotlib')\n\n\n\n\n\n# Explore other models \nStatsForecast.plot(Y_df, fcst_df, models=[\"AutoNHITS\"], unique_ids=[\"H10\", \"H105\"], level=[80, 90], engine='matplotlib')"
  },
  {
    "objectID": "examples/getting_started_complete.html#evaluate-the-models-performance",
    "href": "examples/getting_started_complete.html#evaluate-the-models-performance",
    "title": "End to End Walkthrough",
    "section": "5. Evaluate the model’s performance",
    "text": "5. Evaluate the model’s performance\nIn previous steps, we’ve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\n\n\n\n\n\nTip\n\n\n\nSetting n_windows=1 mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set.\n\n\nThe cross_validation method from the NeuralForecast class takes the following arguments.\n\ndf: training data frame\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows (int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\nfrom neuralforecast.auto import AutoNHITS, AutoLSTM\nconfig_nhits = {\n    \"input_size\": tune.choice([48, 48*2, 48*3, 48*5]),              # Length of input window\n    \"n_blocks\": 5*[1],                                              # Length of input window\n    \"mlp_units\": 5 * [[64, 64]],                                  # Length of input window\n    \"n_pool_kernel_size\": tune.choice([5*[1], 5*[2], 5*[4],         \n                                      [8, 4, 2, 1, 1]]),            # MaxPooling Kernel size\n    \"n_freq_downsample\": tune.choice([[8, 4, 2, 1, 1],\n                                      [1, 1, 1, 1, 1]]),            # Interpolation expressivity ratios\n    \"learning_rate\": tune.loguniform(1e-4, 1e-2),                   # Initial Learning rate\n    \"scaler_type\": tune.choice([None]),                             # Scaler type\n    \"max_steps\": tune.choice([1000]),                               # Max number of training iterations\n    \"batch_size\": tune.choice([32, 64, 128, 256]),                  # Number of series in batch\n    \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),       # Number of windows in batch\n    \"random_seed\": tune.randint(1, 20),                             # Random seed\n}\nnf = NeuralForecast(\n    models=[\n        AutoNHITS(h=48, config=config_nhits, loss=MQLoss(), num_samples=5),\n        AutoLSTM(h=48, loss=MQLoss(), num_samples=2), \n    ],\n    freq='H'\n)\n\n\ncv_df = nf.cross_validation(Y_df, n_windows=2)\n\nThe cv_df object is a new data frame that includes the following columns:\n\nunique_id: identifies each time series\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncv_df.columns = cv_df.columns.str.replace('-median', '')\n\n\ncv_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nunique_id\nds\ncutoff\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-lo-80\nAutoNHITS-hi-80\nAutoNHITS-hi-90\nAutoLSTM\nAutoLSTM-lo-90\nAutoLSTM-lo-80\nAutoLSTM-hi-80\nAutoLSTM-hi-90\ny\n\n\n\n\n0\nH1\n192\n191\n681.490662\n646.704773\n666.150208\n713.381226\n726.007996\n662.800598\n561.542847\n587.252869\n739.114319\n765.682251\n684.0\n\n\n1\nH1\n193\n191\n625.682495\n576.598633\n596.370361\n649.335388\n690.485413\n605.095581\n509.803650\n535.485413\n676.695190\n698.457397\n619.0\n\n\n2\nH1\n194\n191\n573.010193\n537.415771\n535.986572\n598.875427\n619.973267\n557.995544\n471.069336\n500.247681\n624.394714\n642.217834\n565.0\n\n\n3\nH1\n195\n191\n521.889893\n475.909180\n492.678528\n571.340271\n577.343994\n516.248901\n420.724060\n453.731018\n586.593506\n599.947510\n532.0\n\n\n4\nH1\n196\n191\n487.002380\n427.999207\n441.623596\n521.709717\n538.648560\n492.773315\n396.841705\n431.890900\n572.862549\n584.546814\n495.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nfor cutoff in cv_df['cutoff'].unique():\n    StatsForecast.plot(\n        Y_df, \n        cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']), \n        max_insample_length=48 * 4, \n        unique_ids=['H185'],\n        engine='matplotlib'\n    )\n\n\n\n\n\n\n\nNow, let’s evaluate the models’ performance.\n\nfrom datasetsforecast.losses import mse, mae, rmse\nfrom datasetsforecast.evaluation import accuracy\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely hard to judge and not useful to assess forecasting quality.\n\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric.\n\nevaluation_df = accuracy(cv_df, [mse, mae, rmse], agg_by=['unique_id'])\nevaluation_df['best_model'] = evaluation_df.drop(columns=['metric', 'unique_id']).idxmin(axis=1)\nevaluation_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nmetric\nunique_id\nAutoNHITS\nAutoLSTM\nbest_model\n\n\n\n\n0\nmae\nH1\n26.642945\n25.342275\nAutoLSTM\n\n\n1\nmae\nH10\n13.429816\n19.879758\nAutoNHITS\n\n\n2\nmae\nH100\n172.525824\n171.068505\nAutoLSTM\n\n\n3\nmae\nH101\n301.557668\n104.159498\nAutoLSTM\n\n\n4\nmae\nH102\n131.746874\n505.692479\nAutoNHITS\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nCreate a summary table with a model column and the number of series where that model performs best.\n\nsummary_df = evaluation_df.groupby(['metric', 'best_model']).size().sort_values().to_frame()\n\nsummary_df = summary_df.reset_index()\nsummary_df.columns = ['metric', 'model', 'nr. of unique_ids']\nsummary_df\n\n\n  \n    \n      \n\n\n\n\n\n\nmetric\nmodel\nnr. of unique_ids\n\n\n\n\n0\nmse\nAutoNHITS\n186\n\n\n1\nrmse\nAutoNHITS\n187\n\n\n2\nmae\nAutoNHITS\n188\n\n\n3\nmae\nAutoLSTM\n226\n\n\n4\nrmse\nAutoLSTM\n227\n\n\n5\nmse\nAutoLSTM\n228\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nsummary_df.query('metric == \"mse\"')\n\n\n  \n    \n      \n\n\n\n\n\n\nmetric\nmodel\nnr. of unique_ids\n\n\n\n\n0\nmse\nAutoNHITS\n186\n\n\n5\nmse\nAutoLSTM\n228\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\nnhits_ids = evaluation_df.query('best_model == \"AutoNHITS\" and metric == \"mse\"')['unique_id'].unique()\n\nStatsForecast.plot(Y_df, fcst_df, unique_ids=nhits_ids, engine='matplotlib')"
  },
  {
    "objectID": "examples/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "href": "examples/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "title": "End to End Walkthrough",
    "section": "6. Select the best model for every unique series",
    "text": "6. Select the best model for every unique series\nDefine a utility function that takes your forecast’s data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\ndef get_best_model_forecast(forecasts_df, evaluation_df, metric):\n    df = forecasts_df.set_index('ds', append=True).stack().to_frame().reset_index(level=2) # Wide to long \n    df.columns = ['model', 'best_model_forecast'] \n    df = df.join(evaluation_df.query('metric == @metric').set_index('unique_id')[['best_model']])\n    df = df.query('model.str.replace(\"-lo-90|-hi-90\", \"\", regex=True) == best_model').copy()\n    df.loc[:, 'model'] = [model.replace(bm, 'best_model') for model, bm in zip(df['model'], df['best_model'])]\n    df = df.drop(columns='best_model').set_index('model', append=True).unstack()\n    df.columns = df.columns.droplevel()\n    df = df.reset_index(level=1)\n    return df\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nprod_forecasts_df = get_best_model_forecast(fcst_df, evaluation_df, metric='mse')\n\nprod_forecasts_df.head()\n\n\n  \n    \n      \n\n\n\n\n\nmodel\nds\nbest_model\nbest_model-hi-90\nbest_model-lo-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nH1\n241\n612.727234\n653.332764\n575.606018\n\n\nH1\n242\n561.754028\n611.646851\n510.381958\n\n\nH1\n243\n534.809692\n581.725769\n486.517761\n\n\nH1\n244\n509.679291\n550.369507\n466.653137\n\n\nH1\n245\n485.630920\n529.474731\n446.374634\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nPlot the results.\n\nStatsForecast.plot(Y_df, prod_forecasts_df, level=[90], engine='matplotlib')"
  },
  {
    "objectID": "examples/transfer_learning.html",
    "href": "examples/transfer_learning.html",
    "title": "Transfer Learning",
    "section": "",
    "text": "Transfer learning refers to the process of pre-training a flexible model on a large dataset and using it later on other data with little to no training. It is one of the most outstanding 🚀 achievements in Machine Learning 🧠 and has many practical applications.\nFor time series forecasting, the technique allows you to get lightning-fast predictions ⚡ bypassing the tradeoff between accuracy and speed (more than 30 times faster than our alreadsy fast autoARIMA for a similar accuracy).\nThis notebook shows how to generate a pre-trained model and store it in a checkpoint to make it available to forecast new time series never seen by the model.\nTable of Contents 1. Installing NeuralForecast/DatasetsForecast 2. Load M4 Data 3. Instantiate NeuralForecast core, Fit, and save 4. Load pre-trained model and predict on AirPassengers 5. Evaluate Results\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/transfer_learning.html#installing-libraries",
    "href": "examples/transfer_learning.html#installing-libraries",
    "title": "Transfer Learning",
    "section": "1. Installing Libraries",
    "text": "1. Installing Libraries\n\n# %%capture\n# !pip install git+https://github.com/Nixtla/datasetsforecast.git@main\n\n\n# %%capture\n# !pip install neuralforecast\n\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom IPython.display import display, Markdown\n\nimport matplotlib.pyplot as plt\n\nfrom datasetsforecast.m4 import M4\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import NHITS\nfrom neuralforecast.utils import AirPassengersDF\nfrom neuralforecast.losses.numpy import mae, mse\n\n\nimport logging\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n\nThis example will automatically run on GPUs if available. Make sure cuda is available. (If you need help to put this into production send us an email or join or community, we also offer a fully hosted solution)\n\ntorch.cuda.is_available()"
  },
  {
    "objectID": "examples/transfer_learning.html#load-m4-data",
    "href": "examples/transfer_learning.html#load-m4-data",
    "title": "Transfer Learning",
    "section": "2. Load M4 Data",
    "text": "2. Load M4 Data\nThe M4 class will automatically download the complete M4 dataset and process it.\nIt return three Dataframes: Y_df contains the values for the target variables, X_df contains exogenous calendar features and S_df contains static features for each time-series (none for M4). For this example we will only use Y_df.\nIf you want to use your own data just replace Y_df. Be sure to use a long format and have a simmilar structure than our data set.\n\nY_df, _, _ = M4.load(directory='./', group='Monthly', cache=True)\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df"
  },
  {
    "objectID": "examples/transfer_learning.html#model-train-and-save",
    "href": "examples/transfer_learning.html#model-train-and-save",
    "title": "Transfer Learning",
    "section": "3. Model Train and Save",
    "text": "3. Model Train and Save\nUsing the NeuralForecast.fit method you can train a set of models to your dataset. You just have to define the input_size and horizon of your model. The input_size is the number of historic observations (lags) that the model will use to learn to predict h steps in the future. Also, you can modify the hyperparameters of the model to get a better accuracy.\n\nhorizon = 12\nstacks = 3\nmodels = [NHITS(input_size=5 * horizon,\n                h=horizon,\n                max_steps=100,\n                stack_types = stacks*['identity'],\n                n_blocks = stacks*[1],\n                mlp_units = [[256,256] for _ in range(stacks)],\n                n_pool_kernel_size = stacks*[1],\n                batch_size = 32,\n                scaler_type='standard',\n                n_freq_downsample=[12,4,1])]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(df=Y_df)\n\nSave model with core.NeuralForecast.save method. This method uses PytorchLightning save_checkpoint function. We set save_dataset=False to only save the model.\n\nnf.save(path='./results/transfer/', model_index=None, overwrite=True, save_dataset=False)"
  },
  {
    "objectID": "examples/transfer_learning.html#transfer-m4-to-airpassengers",
    "href": "examples/transfer_learning.html#transfer-m4-to-airpassengers",
    "title": "Transfer Learning",
    "section": "4. Transfer M4 to AirPassengers",
    "text": "4. Transfer M4 to AirPassengers\nWe load the stored model with the core.NeuralForecast.load method, and forecast AirPassenger with the core.NeuralForecast.predict function.\n\nfcst2 = NeuralForecast.load(path='./results/transfer/')\n\n\n# We define the train df. \nY_df = AirPassengersDF.copy()\nmean = Y_df[Y_df.ds&lt;='1959-12-31']['y'].mean()\nstd = Y_df[Y_df.ds&lt;='1959-12-31']['y'].std()\n\nY_train_df = Y_df[Y_df.ds&lt;='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds&gt;'1959-12-31']   # 12 test\n\n\nY_hat_df = fcst2.predict(df=Y_train_df).reset_index()\nY_hat_df.head()\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\nplot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\n\nplot_df[['y', 'NHITS']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()"
  },
  {
    "objectID": "examples/transfer_learning.html#evaluate-results",
    "href": "examples/transfer_learning.html#evaluate-results",
    "title": "Transfer Learning",
    "section": "5. Evaluate Results",
    "text": "5. Evaluate Results\nWe evaluate the forecasts of the pre-trained model with the Mean Absolute Error (mae).\n\\[\n\\qquad MAE = \\frac{1}{Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}|\\qquad\n\\]\n\ny_true = Y_test_df.y.values\ny_hat = Y_hat_df['NHITS'].values\n\n\nprint('NHITS     MAE: %0.3f' % mae(y_hat, y_true))\nprint('ETS       MAE: 16.222')\nprint('AutoARIMA MAE: 18.551')"
  },
  {
    "objectID": "examples/save_load_models.html",
    "href": "examples/save_load_models.html",
    "title": "Save and Load Models",
    "section": "",
    "text": "Saving and loading trained Deep Learning models has multiple valuable uses. These models are often costly to train; storing a pre-trained model can help reduce costs as it can be loaded and reused to forecast multiple times. Moreover, it enables Transfer learning capabilities, consisting of pre-training a flexible model on a large dataset and using it later on other data with little to no training. It is one of the most outstanding 🚀 achievements in Machine Learning 🧠 and has many practical applications.\nIn this notebook we show an example on how to save and load NeuralForecast models.\nThe two methods to consider are: 1. NeuralForecast.save: Saves models into disk, allows save dataset and config. 2. NeuralForecast.load: Loads models from a given path.\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/save_load_models.html#installing-neuralforecast",
    "href": "examples/save_load_models.html#installing-neuralforecast",
    "title": "Save and Load Models",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n\n!pip install neuralforecast"
  },
  {
    "objectID": "examples/save_load_models.html#loading-airpassengers-data",
    "href": "examples/save_load_models.html#loading-airpassengers-data",
    "title": "Save and Load Models",
    "section": "2. Loading AirPassengers Data",
    "text": "2. Loading AirPassengers Data\nFor this example we will use the classical AirPassenger Data set. Import the pre-processed AirPassenger from utils.\n\nfrom neuralforecast.utils import AirPassengersDF\n\n/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nY_df = AirPassengersDF\nY_df = Y_df.reset_index(drop=True)\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n1.0\n1949-01-31\n112.0\n\n\n1\n1.0\n1949-02-28\n118.0\n\n\n2\n1.0\n1949-03-31\n132.0\n\n\n3\n1.0\n1949-04-30\n129.0\n\n\n4\n1.0\n1949-05-31\n121.0"
  },
  {
    "objectID": "examples/save_load_models.html#model-training",
    "href": "examples/save_load_models.html#model-training",
    "title": "Save and Load Models",
    "section": "3. Model Training",
    "text": "3. Model Training\nNext, we instantiate and train three models: NBEATS, NHITS, and AutoMLP. The models with their hyperparameters are defined in the models list.\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import NBEATS, NHITS\n\n\nhorizon = 12\nmodels = [NBEATS(input_size=2 * horizon, h=horizon, max_steps=50),\n          NHITS(input_size=2 * horizon, h=horizon, max_steps=50),\n          AutoMLP(# Ray tune explore config\n                  config=dict(max_steps=100, # Operates with steps not epochs\n                              input_size=tune.choice([3*horizon]),\n                              learning_rate=tune.choice([1e-3])),\n                  h=horizon,\n                  num_samples=1, cpus=1)]\n\n\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(df=Y_df)\n\nProduce the forecasts with the predict method.\n\nY_hat_df = nf.predict().reset_index()\nY_hat_df.head()\n\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 98.79it/s] \nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 123.41it/s]\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 161.79it/s]\n\n\n\n\n\n\n\n\n\nunique_id\nds\nNBEATS\nNHITS\nAutoMLP\n\n\n\n\n0\n1.0\n1961-01-31\n428.410553\n445.268158\n452.550446\n\n\n1\n1.0\n1961-02-28\n425.958557\n469.293945\n442.683807\n\n\n2\n1.0\n1961-03-31\n477.748016\n462.920807\n474.043457\n\n\n3\n1.0\n1961-04-30\n477.548798\n489.986633\n503.836334\n\n\n4\n1.0\n1961-05-31\n495.973541\n518.612610\n531.347900\n\n\n\n\n\n\n\nWe plot the forecasts for each model. Note how the two NBEATS models are differentiated with a numerical suffix.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplot_df = pd.concat([Y_df, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\n\nplt.figure(figsize = (12, 3))\nplot_df[['y', 'NBEATS', 'NHITS', 'AutoMLP']].plot(linewidth=2)\n\nplt.title('AirPassengers Forecast', fontsize=10)\nplt.ylabel('Monthly Passengers', fontsize=10)\nplt.xlabel('Timestamp [t]', fontsize=10)\nplt.axvline(x=plot_df.index[-horizon], color='k', linestyle='--', linewidth=2)\nplt.legend(prop={'size': 10})\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n&lt;Figure size 1200x300 with 0 Axes&gt;"
  },
  {
    "objectID": "examples/save_load_models.html#save-models",
    "href": "examples/save_load_models.html#save-models",
    "title": "Save and Load Models",
    "section": "4. Save models",
    "text": "4. Save models\nTo save all the trained models use the save method. This method will save both the hyperparameters and the learnable weights (parameters).\nThe save method has the following inputs:\n\npath: directory where models will be saved.\nmodel_index: optional list to specify which models to save. For example, to only save the NHITS model use model_index=[2].\noverwrite: boolean to overwrite existing files in path. When True, the method will only overwrite models with conflicting names.\nsave_dataset: boolean to save Dataset object with the dataset.\n\n\nnf.save(path='./checkpoints/test_run/',\n        model_index=None, \n        overwrite=True,\n        save_dataset=True)\n\nFor each model, two files are created and stored:\n\n[model_name]_[suffix].ckpt: Pytorch Lightning checkpoint file with the model parameters and hyperparameters.\n[model_name]_[suffix].pkl: Dictionary with configuration attributes.\n\nWhere model_name corresponds to the name of the model in lowercase (eg. nhits). We use a numerical suffix to distinguish multiple models of each class. In this example the names will be automlp_0, nbeats_0, and nhits_0.\n\n\n\n\n\n\nImportant\n\n\n\nThe Auto models will be stored as their base model. For example, the AutoMLP trained above is stored as an MLP model, with the best hyparparameters found during tuning."
  },
  {
    "objectID": "examples/save_load_models.html#load-models",
    "href": "examples/save_load_models.html#load-models",
    "title": "Save and Load Models",
    "section": "5. Load models",
    "text": "5. Load models\nLoad the saved models with the load method, specifying the path, and use the new nf2 object to produce forecasts.\n\nnf2 = NeuralForecast.load(path='./checkpoints/test_run/')\nY_hat_df = nf2.predict().reset_index()\nY_hat_df.head()\n\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 153.75it/s]\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 142.04it/s]\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 105.82it/s]\n\n\n\n\n\n\n\n\n\nunique_id\nds\nMLP\nNHITS\nNBEATS\n\n\n\n\n0\n1.0\n1961-01-31\n452.550446\n445.268158\n428.410553\n\n\n1\n1.0\n1961-02-28\n442.683807\n469.293945\n425.958557\n\n\n2\n1.0\n1961-03-31\n474.043457\n462.920807\n477.748016\n\n\n3\n1.0\n1961-04-30\n503.836334\n489.986633\n477.548798\n\n\n4\n1.0\n1961-05-31\n531.347900\n518.612610\n495.973541\n\n\n\n\n\n\n\nFinally, plot the forecasts to confirm they are identical to the original forecasts.\n\nplot_df = pd.concat([Y_df, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\n\nplt.figure(figsize = (12, 3))\nplot_df[['y', 'NBEATS', 'NHITS', 'MLP']].plot(linewidth=2)\n\nplt.title('AirPassengers Forecast', fontsize=10)\nplt.ylabel('Monthly Passengers', fontsize=10)\nplt.xlabel('Timestamp [t]', fontsize=10)\nplt.axvline(x=plot_df.index[-horizon], color='k', linestyle='--', linewidth=2)\nplt.legend(prop={'size': 10})\nplt.show()\n\n&lt;Figure size 1200x300 with 0 Axes&gt;"
  },
  {
    "objectID": "examples/save_load_models.html#references",
    "href": "examples/save_load_models.html#references",
    "title": "Save and Load Models",
    "section": "References",
    "text": "References\nhttps://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing_basic.html\nOreshkin, B. N., Carpov, D., Chapados, N., & Bengio, Y. (2019). N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. ICLR 2020\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023."
  },
  {
    "objectID": "examples/longhorizon_probabilistic.html",
    "href": "examples/longhorizon_probabilistic.html",
    "title": "Probabilistic Long-Horizon",
    "section": "",
    "text": "Long-horizon forecasting is challenging because of the volatility of the predictions and the computational complexity. To solve this problem we created the NHITS model and made the code available NeuralForecast library. NHITS specializes its partial outputs in the different frequencies of the time series through hierarchical interpolation and multi-rate input processing. We model the target time-series with Student’s t-distribution. The NHITS will output the distribution parameters for each timestamp.\nIn this notebook we show how to use N-HiTS on the ETTm2 benchmark dataset for probabilistic forecasting. This data set includes data points for 2 Electricity Transformers at 2 stations, including load, oil temperature.\nWe will show you how to load data, train, and perform automatic hyperparameter tuning, to achieve SoTA performance, outperforming even the latest Transformer architectures for a fraction of their computational cost (50x faster).\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/longhorizon_probabilistic.html#libraries",
    "href": "examples/longhorizon_probabilistic.html#libraries",
    "title": "Probabilistic Long-Horizon",
    "section": "1. Libraries",
    "text": "1. Libraries\n\n!pip install neuralforecast datasetsforecast\n\n\nimport torch\nimport pandas as pd\nfrom datasetsforecast.long_horizon import LongHorizon\n\n\ntorch.cuda.is_available()\n\nTrue"
  },
  {
    "objectID": "examples/longhorizon_probabilistic.html#load-ettm2-data",
    "href": "examples/longhorizon_probabilistic.html#load-ettm2-data",
    "title": "Probabilistic Long-Horizon",
    "section": "2. Load ETTm2 Data",
    "text": "2. Load ETTm2 Data\nThe LongHorizon class will automatically download the complete ETTm2 dataset and process it.\nIt return three Dataframes: Y_df contains the values for the target variables, X_df contains exogenous calendar features and S_df contains static features for each time-series (none for ETTm2). For this example we will only use Y_df.\nIf you want to use your own data just replace Y_df. Be sure to use a long format and have a simmilar structure than our data set.\n\n# Change this to your own data to try the model\nY_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n# For this excercise we are going to take 960 timestamps as validation and test\nn_time = len(Y_df.ds.unique())\nval_size = 96*10\ntest_size = 96*10\n\nY_df.groupby('unique_id').head(2)\n\n\n  \n    \n      \n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nHUFL\n2016-07-01 00:00:00\n-0.041413\n\n\n1\nHUFL\n2016-07-01 00:15:00\n-0.185467\n\n\n57600\nHULL\n2016-07-01 00:00:00\n0.040104\n\n\n57601\nHULL\n2016-07-01 00:15:00\n-0.214450\n\n\n115200\nLUFL\n2016-07-01 00:00:00\n0.695804\n\n\n115201\nLUFL\n2016-07-01 00:15:00\n0.434685\n\n\n172800\nLULL\n2016-07-01 00:00:00\n0.434430\n\n\n172801\nLULL\n2016-07-01 00:15:00\n0.428168\n\n\n230400\nMUFL\n2016-07-01 00:00:00\n-0.599211\n\n\n230401\nMUFL\n2016-07-01 00:15:00\n-0.658068\n\n\n288000\nMULL\n2016-07-01 00:00:00\n-0.393536\n\n\n288001\nMULL\n2016-07-01 00:15:00\n-0.659338\n\n\n345600\nOT\n2016-07-01 00:00:00\n1.018032\n\n\n345601\nOT\n2016-07-01 00:15:00\n0.980124\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n\n\n\nImportant\n\n\n\nDataFrames must include all ['unique_id', 'ds', 'y'] columns. Make sure y column does not have missing or non-numeric values.\n\n\nNext, plot the HUFL variable marking the validation and train splits.\n\nimport matplotlib.pyplot as plt\n\n\n# We are going to plot the temperature of the transformer \n# and marking the validation and train splits\nu_id = 'HUFL'\nx_plot = pd.to_datetime(Y_df[Y_df.unique_id==u_id].ds)\ny_plot = Y_df[Y_df.unique_id==u_id].y.values\n\nx_val = x_plot[n_time - val_size - test_size]\nx_test = x_plot[n_time - test_size]\n\nfig = plt.figure(figsize=(10, 5))\nfig.tight_layout()\n\nplt.plot(x_plot, y_plot)\nplt.xlabel('Date', fontsize=17)\nplt.ylabel('OT [15 min temperature]', fontsize=17)\n\nplt.axvline(x_val, color='black', linestyle='-.')\nplt.axvline(x_test, color='black', linestyle='-.')\nplt.text(x_val, 5, '  Validation', fontsize=12)\nplt.text(x_test, 3, '  Test', fontsize=12)\n\nplt.grid()\nplt.show()\nplt.close()"
  },
  {
    "objectID": "examples/longhorizon_probabilistic.html#hyperparameter-selection-and-forecasting",
    "href": "examples/longhorizon_probabilistic.html#hyperparameter-selection-and-forecasting",
    "title": "Probabilistic Long-Horizon",
    "section": "3. Hyperparameter selection and forecasting",
    "text": "3. Hyperparameter selection and forecasting\nThe AutoNHITS class will automatically perform hyperparamter tunning using Tune library, exploring a user-defined or default search space. Models are selected based on the error on a validation set and the best model is then stored and used during inference.\nThe AutoNHITS.default_config attribute contains a suggested hyperparameter space. Here, we specify a different search space following the paper’s hyperparameters. Notice that 1000 Stochastic Gradient Steps are enough to achieve SoTA performance. Feel free to play around with this space.\n\nfrom ray import tune\n\nfrom neuralforecast.auto import AutoNHITS\nfrom neuralforecast.core import NeuralForecast\n\nfrom neuralforecast.losses.pytorch import DistributionLoss\n\nimport logging\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n\n\nhorizon = 96 # 24hrs = 4 * 15 min.\n\n# Use your own config or AutoNHITS.default_config\nnhits_config = {\n       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n       \"max_steps\": tune.choice([1000]),                                         # Number of SGD steps\n       \"input_size\": tune.choice([5 * horizon]),                                 # input_size = multiplier * horizon\n       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n       \"random_seed\": tune.randint(1, 10),\n       \"scaler_type\": tune.choice(['robust']),\n       \"val_check_steps\": tune.choice([100])\n    }\n\n\n\n\n\n\n\nTip\n\n\n\nRefer to https://docs.ray.io/en/latest/tune/index.html for more information on the different space options, such as lists and continous intervals.m\n\n\nTo instantiate AutoNHITS you need to define:\n\nh: forecasting horizon\nloss: training loss. Use the DistributionLoss to produce probabilistic forecasts.\nconfig: hyperparameter search space. If None, the AutoNHITS class will use a pre-defined suggested hyperparameter space.\nnum_samples: number of configurations explored.\n\n\nmodels = [AutoNHITS(h=horizon,\n                    loss=DistributionLoss(distribution='StudentT', level=[80, 90]), \n                    config=nhits_config,\n                    num_samples=5)]\n\nFit the model by instantiating a NeuralForecast object with the following required parameters:\n\nmodels: a list of models.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\n\n# Fit and predict\nnf = NeuralForecast(\n    models=models,\n    freq='15min')\n\nThe cross_validation method allows you to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods.\nWith time series data, cross validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe cross_validation method will use the validation set for hyperparameter selection, and will then produce the forecasts for the test set.\n\nY_hat_df = nf.cross_validation(df=Y_df, val_size=val_size,\n                               test_size=test_size, n_windows=None)"
  },
  {
    "objectID": "examples/longhorizon_probabilistic.html#visualization",
    "href": "examples/longhorizon_probabilistic.html#visualization",
    "title": "Probabilistic Long-Horizon",
    "section": "4. Visualization",
    "text": "4. Visualization\nFinally, we merge the forecasts with the Y_df dataset and plot the forecasts.\n\nY_hat_df = Y_hat_df.reset_index(drop=True)\nY_hat_df = Y_hat_df[(Y_hat_df['unique_id']=='OT') & (Y_hat_df['cutoff']=='2018-02-11 12:00:00')]\nY_hat_df = Y_hat_df.drop(columns=['y','cutoff'])\n\n\nplot_df = Y_df.merge(Y_hat_df, on=['unique_id','ds'], how='outer').tail(96*10+50+96*4).head(96*2+96*4)\n\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['AutoNHITS-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'], \n                    y1=plot_df['AutoNHITS-lo-90.0'], y2=plot_df['AutoNHITS-hi-90.0'],\n                    alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n\n[]"
  },
  {
    "objectID": "examples/longhorizon_probabilistic.html#references",
    "href": "examples/longhorizon_probabilistic.html#references",
    "title": "Probabilistic Long-Horizon",
    "section": "References",
    "text": "References\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023."
  },
  {
    "objectID": "examples/getting_started.html",
    "href": "examples/getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "This notebook provides an example on how to start using the main functionalities of the NeuralForecast library. The NeuralForecast class allows users to easily interact with NeuralForecast.models PyTorch models. In this example we will forecast AirPassengers data with a classic LSTM and the recent NHITS models. The full list of available models is available here.\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/getting_started.html#installing-neuralforecast",
    "href": "examples/getting_started.html#installing-neuralforecast",
    "title": "Getting Started",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n\n!pip install neuralforecast\n\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\nimport matplotlib.pyplot as plt\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import LSTM, NHITS, RNN\nfrom neuralforecast.utils import AirPassengersDF"
  },
  {
    "objectID": "examples/getting_started.html#loading-airpassengers-data",
    "href": "examples/getting_started.html#loading-airpassengers-data",
    "title": "Getting Started",
    "section": "2. Loading AirPassengers Data",
    "text": "2. Loading AirPassengers Data\nThe core.NeuralForecast class contains shared, fit, predict and other methods that take as inputs pandas DataFrames with columns ['unique_id', 'ds', 'y'], where unique_id identifies individual time series from the dataset, ds is the date, and y is the target variable.\nIn this example dataset consists of a set of a single series, but you can easily fit your model to larger datasets in long format.\n\nY_df = AirPassengersDF # Defined in neuralforecast.utils\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n1.0\n1949-01-31\n112.0\n\n\n1\n1.0\n1949-02-28\n118.0\n\n\n2\n1.0\n1949-03-31\n132.0\n\n\n3\n1.0\n1949-04-30\n129.0\n\n\n4\n1.0\n1949-05-31\n121.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDataFrames must include all ['unique_id', 'ds', 'y'] columns. Make sure y column does not have missing or non-numeric values."
  },
  {
    "objectID": "examples/getting_started.html#model-training",
    "href": "examples/getting_started.html#model-training",
    "title": "Getting Started",
    "section": "3. Model Training",
    "text": "3. Model Training\n\nFit the models\nUsing the NeuralForecast.fit method you can train a set of models to your dataset. You can define the forecasting horizon (12 in this example), and modify the hyperparameters of the model. For example, for the LSTM we changed the default hidden size for both encoder and decoders.\n\nhorizon = 12\n\n# Try different hyperparmeters to improve accuracy.\nmodels = [LSTM(h=horizon,                    # Forecast horizon\n               max_steps=500,                # Number of steps to train\n               scaler_type='standard',       # Type of scaler to normalize data\n               encoder_hidden_size=64,       # Defines the size of the hidden state of the LSTM\n               decoder_hidden_size=64,),     # Defines the number of hidden units of each layer of the MLP decoder\n          NHITS(h=horizon,                   # Forecast horizon\n                input_size=2 * horizon,      # Length of input sequence\n                max_steps=100,               # Number of steps to train\n                n_freq_downsample=[2, 1, 1]) # Downsampling factors for each stack output\n          ]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(df=Y_df)\n\n\n\n\n\n\n\nTip\n\n\n\nThe performance of Deep Learning models can be very sensitive to the choice of hyperparameters. Tuning the correct hyperparameters is an important step to obtain the best forecasts. The Auto version of these models, AutoLSTM and AutoNHITS, already perform hyperparameter selection automatically.\n\n\n\n\nPredict using the fitted models\nUsing the NeuralForecast.predict method you can obtain the h forecasts after the training data Y_df.\n\nY_hat_df = nf.predict()\n\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 50.58it/s]\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 126.52it/s]\n\n\nThe NeuralForecast.predict method returns a DataFrame with the forecasts for each unique_id, ds, and model.\n\nY_hat_df = Y_hat_df.reset_index()\nY_hat_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nLSTM\nNHITS\n\n\n\n\n0\n1.0\n1961-01-31\n424.380310\n453.039185\n\n\n1\n1.0\n1961-02-28\n442.092010\n429.609192\n\n\n2\n1.0\n1961-03-31\n448.555664\n498.796204\n\n\n3\n1.0\n1961-04-30\n473.586609\n509.536224\n\n\n4\n1.0\n1961-05-31\n512.466370\n524.131592"
  },
  {
    "objectID": "examples/getting_started.html#plot-predictions",
    "href": "examples/getting_started.html#plot-predictions",
    "title": "Getting Started",
    "section": "4. Plot Predictions",
    "text": "4. Plot Predictions\nFinally, we plot the forecasts of both models againts the real values.\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = pd.concat([Y_df, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\nplot_df[['y', 'LSTM', 'NHITS']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor this guide we are using a simple LSTM model. More recent models, such as RNN, GRU, and DilatedRNN achieve better accuracy than LSTM in most settings. The full list of available models is available here."
  },
  {
    "objectID": "examples/getting_started.html#references",
    "href": "examples/getting_started.html#references",
    "title": "Getting Started",
    "section": "References",
    "text": "References\n\nBoris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2020). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”. International Conference on Learning Representations.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). “N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting”. Work in progress paper, submitted to AAAI."
  },
  {
    "objectID": "examples/hierarchicalnetworks.html",
    "href": "examples/hierarchicalnetworks.html",
    "title": "Hierarchical Forecast Networks",
    "section": "",
    "text": "This notebook offers a step by step guide to create a hierarchical forecasting pipeline.\nIn the pipeline we will use NeuralForecast and HINT class, to create fit, predict and reconcile forecasts.\nWe will use the TourismL dataset that summarizes large Australian national visitor survey.\nOutline 1. Installing packages 2. Load hierarchical dataset 3. Fit and Predict HINT 4. Forecast Evaluation\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/hierarchicalnetworks.html#installing-packages",
    "href": "examples/hierarchicalnetworks.html#installing-packages",
    "title": "Hierarchical Forecast Networks",
    "section": "1. Installing packages",
    "text": "1. Installing packages\n\n!pip install git+https://github.com/Nixtla/neuralforecast.git@fix/hint_predict\n\n\n!pip install datasetsforecast hierarchicalforecast"
  },
  {
    "objectID": "examples/hierarchicalnetworks.html#load-hierarchical-dataset",
    "href": "examples/hierarchicalnetworks.html#load-hierarchical-dataset",
    "title": "Hierarchical Forecast Networks",
    "section": "2. Load hierarchical dataset",
    "text": "2. Load hierarchical dataset\nThis detailed Australian Tourism Dataset comes from the National Visitor Survey, managed by the Tourism Research Australia, it is composed of 555 monthly series from 1998 to 2016, it is organized geographically, and purpose of travel. The natural geographical hierarchy comprises seven states, divided further in 27 zones and 76 regions. The purpose of travel categories are holiday, visiting friends and relatives (VFR), business and other. The MinT (Wickramasuriya et al., 2019), among other hierarchical forecasting studies has used the dataset it in the past. The dataset can be accessed in the MinT reconciliation webpage, although other sources are available.\n\n\n\n\n\n\n\n\n\nGeographical Division\nNumber of series per division\nNumber of series per purpose\nTotal\n\n\n\n\nAustralia\n1\n4\n5\n\n\nStates\n7\n28\n35\n\n\nZones\n27\n108\n135\n\n\nRegions\n76\n304\n380\n\n\nTotal\n111\n444\n555\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datasetsforecast.hierarchical import HierarchicalData\nfrom hierarchicalforecast.utils import aggregate, HierarchicalPlot\n\nfrom neuralforecast.utils import augment_calendar_df\n\ndef sort_df_hier(Y_df, S):\n    # NeuralForecast core, sorts unique_id lexicographically\n    # by default, this method matches S_df and Y_hat_df hierarchical order.\n    Y_df.unique_id = Y_df.unique_id.astype('category')\n    Y_df.unique_id = Y_df.unique_id.cat.set_categories(S.index)\n    Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n    return Y_df\n\n# Load hierarchical dataset\nY_df, S_df, tags = HierarchicalData.load('./data', 'TourismLarge')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df = sort_df_hier(Y_df, S_df)\n\nY_df, _ = augment_calendar_df(df=Y_df, freq='M')\n\nMathematically a hierarchical multivariate time series can be denoted by the vector \\(\\mathbf{y}_{[a,b],t}\\) defined by the following aggregation constraint: \\[\n\\mathbf{y}_{[a,b],t}  = \\mathbf{S}_{[a,b][b]} \\mathbf{y}_{[b],t} \\quad \\Leftrightarrow \\quad\n\\begin{bmatrix}\\mathbf{y}_{[a],t}\n\\\\ %\\hline\n\\mathbf{y}_{[b],t}\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf{A}_{[a][b]}\\\\ %\\hline\n\\mathbf{I}_{[b][b]}\n\\end{bmatrix}\n\\mathbf{y}_{[b],t}\n\\]\nwhere \\(\\mathbf{y}_{[a],t}\\) are the aggregate series, \\(\\mathbf{y}_{[b],t}\\) are the bottom level series and \\(\\mathbf{S}_{[a,b][b]}\\) are the hierarchical aggregation constraints.\n\n# Here we plot the hierarchical constraints matrix\nhplot = HierarchicalPlot(S=S_df, tags=tags)\nhplot.plot_summing_matrix()\n\n\n\n\n\n# Here we plot the top most series from the dataset\n# that corresponds to the total tourist monthly visits to Australia\nplt.figure(figsize=(10,5))\nplt.plot(Y_df[Y_df['unique_id']=='TotalAll']['ds'], \n         Y_df[Y_df['unique_id']=='TotalAll']['y'], label='target')\nplt.plot(Y_df[Y_df['unique_id']=='TotalAll']['ds'], \n         Y_df[Y_df['unique_id']=='TotalAll']['month']*80000, label='month dummy')\nplt.xlabel('Date')\nplt.ylabel('Tourist Visits')\nplt.legend()\nplt.grid()\nplt.show()\nplt.close()"
  },
  {
    "objectID": "examples/hierarchicalnetworks.html#fit-and-predict-hint",
    "href": "examples/hierarchicalnetworks.html#fit-and-predict-hint",
    "title": "Hierarchical Forecast Networks",
    "section": "3. Fit and Predict HINT",
    "text": "3. Fit and Predict HINT\nThe Hierarchical Forecast Network (HINT) combines into an easy to use model three components: 1. SoTA neural forecast model. 2. An efficient and flexible multivariate probability distribution. 3. Builtin reconciliation capabilities.\n\nimport numpy as np\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATSx, NHITS, HINT\nfrom neuralforecast.losses.pytorch import GMM, PMM, DistributionLoss, sCRPS\n\n\n# Train test splits\nhorizon = 12\nY_test_df  = Y_df.groupby('unique_id').tail(horizon)\nY_train_df = Y_df.drop(Y_test_df.index)\nY_test_df  = Y_test_df.set_index('unique_id')\nY_train_df = Y_train_df.set_index('unique_id')\n\n\n# Horizon and quantiles\nlevel = np.arange(0, 100, 2)\nqs = [[50-lv/2, 50+lv/2] for lv in level]\nquantiles = np.sort(np.concatenate(qs)/100)\n\n# HINT := BaseNetwork + Distribution + Reconciliation\nnhits = NHITS(h=horizon,\n              input_size=24,\n              loss=GMM(n_components=10, quantiles=quantiles),\n              hist_exog_list=['month'],\n              max_steps=2000,\n              early_stop_patience_steps=10,\n              val_check_steps=50,\n              scaler_type='robust',\n              learning_rate=1e-3,\n              valid_loss=sCRPS(quantiles=quantiles))\n\nmodel = HINT(h=horizon, S=S_df.values,\n             model=nhits,  reconciliation='BottomUp')\n\nINFO:lightning_fabric.utilities.seed:Global seed set to 1\n\n\n\n#%%capture\nY_df['y'] = Y_df['y'] * (Y_df['y'] &gt; 0)\nnf = NeuralForecast(models=[model], freq='MS')\n# Y_hat_df = nf.cross_validation(df=Y_df, val_size=12, n_windows=1)\nnf.fit(df=Y_train_df, val_size=12)\nY_hat_df = nf.predict()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id = 'TotalAll'\nY_plot_df = Y_df[Y_df.unique_id==unique_id].tail(12*5)\nY_hat_df = Y_hat_df.reset_index()\nplot_df = Y_hat_df[Y_hat_df.unique_id==unique_id]\nplot_df = Y_plot_df.merge(plot_df, on=['unique_id', 'ds'], how='left')\n\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['HINT-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:],\n                 y1=plot_df['HINT-lo-90.0'][-12:].values,\n                 y2=plot_df['HINT-hi-90.0'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n\n[]"
  },
  {
    "objectID": "examples/hierarchicalnetworks.html#forecast-evaluation",
    "href": "examples/hierarchicalnetworks.html#forecast-evaluation",
    "title": "Hierarchical Forecast Networks",
    "section": "4. Forecast Evaluation",
    "text": "4. Forecast Evaluation\nTo evaluate the coherent probabilistic predictions we use the scaled Continuous Ranked Probability Score (sCRPS), defined as follows:\n\\[\\mathrm{CRPS}(\\hat{F}_{[a,b],\\tau},\\mathbf{y}_{[a,b],\\tau}) = \\frac{2}{N_{a}+N_{b}} \\sum_{i} \\int^{1}_{0} \\mathrm{QL}(\\hat{F}_{i,\\tau}, y_{i,\\tau})_{q} dq\\]\n\\[\n\\mathrm{sCRPS}(\\hat{F}_{[a,b\\,],\\tau},\\mathbf{y}_{[a,b\\,],\\tau}) =\n    \\frac{\\mathrm{CRPS}(\\hat{F}_{[a,b\\,],\\tau},\\mathbf{y}_{[a,b\\,],\\tau})}{\\sum_{i} | y_{i,\\tau} |}\n\\]\nAs you can see the HINT model efficiently achieves state of the art accuracy under minimal tuning.\n\nfrom hierarchicalforecast.evaluation import scaled_crps    \n    \ndef _get_hierarchical_scrps(hier_idxs, Y, Yq_hat, quantiles):\n    # We use the indexes obtained from the aggregation tags\n    # to compute scaled CRPS across the hierarchy levels \n    scrps_list = []\n    for idxs in hier_idxs:\n        y      = Y[idxs, :]\n        yq_hat = Yq_hat[idxs, :, :]\n        scrps  = scaled_crps(y, yq_hat, quantiles)\n        scrps_list.append(scrps)\n    return scrps_list\n\nhier_idxs = [np.arange(len(S_df))] +\\\n    [S_df.index.get_indexer(tags[level]) for level in list(tags.keys())]\n\n\nn_series = len(S_df)\nn_quantiles = len(quantiles)\n\n# Bootstrap predictions\nn_samples = 5\nY_hat_df_list = [nf.predict() for _ in range(n_samples)]\n\n# Parse y_test and y_rec\n# Keep only quantile columns from Y_hat_df\n# Removing mean and median default outputs\nmodel_name = type(model).__name__\nquantile_columns = [model_name + n for n in nhits.loss.output_names]\nquantile_columns.remove(model_name)\nquantile_columns.remove(model_name + '-median')\nYq_hat = []\nfor sample_idx in range(n_samples):\n    Y_hat = Y_hat_df_list[sample_idx][quantile_columns].values\n    Yq_hat.append(Y_hat.reshape(1, n_series, horizon, n_quantiles))\n\nYq_hat = np.concatenate(Yq_hat, axis=0)\nY_test = Y_test_df['y'].values.reshape(n_series, horizon)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compute bootstraped sCRPS\nscrps_hint = [_get_hierarchical_scrps(hier_idxs, Y_test, Yq_hat[sample_idx], quantiles) \\\n              for sample_idx in range(n_samples)]\ncrps_mean = np.mean(np.array(scrps_hint), axis=0)\ncrps_std = np.std(np.array(scrps_hint), axis=0)\nscrps_hint = [f'{crps_mean[level_idx]:.4f}±{(1.96 * crps_std[level_idx]):.4f}' \\\n              for level_idx in range(len(crps_mean))]\n\n# Add reported baselines' performance\nlevels = ['Overall', 'Country', 'State', 'Zone', 'Region',\n          'Country/Purpose', 'State/Purpose', 'Zone/Purpose', 'Region/Purpose']\nscrps_dpmn = [\"0.1249±0.0020\",\"0.0431±0.0042\",\"0.0637±0.0032\",\"0.1084±0.0033\",\n              \"0.1554±0.0025\",\"0.0700±0.0038\",\"0.1070±0.0023\",\"0.1887±0.0032\",\"0.2629±0.0034\"]\nscrps_hiere2e = [\"0.1472±0.0029\",\"0.0842±0.0051\",\"0.1012±0.0029\",\"0.1317±0.0022\",\n              \"0.1705±0.0023\",\"0.0995±0.0061\",\"0.1336±0.0042\",\"0.1955±0.0025\",\"0.2615±0.0016\"]\nscrps_arima_mintrace = [\"0.1313±0.0009\",\"0.0471±0.0018\",\"0.0723±0.0011\",\"0.1143±0.0007\",\n              \"0.1591±0.0006\",\"0.0723±0.0014\",\"0.1243±0.0014\",\"0.1919±0.0008\",\"0.2694±0.0006\"]\nscrps_arima_bu = [\"0.1375±0.0013\",\"0.0622±0.0026\",\"0.0820±0.0019\",\"0.1207±0.0010\",\n              \"0.1646±0.0007\",\"0.0788±0.0018\",\"0.1268±0.0017\",\"0.1949±0.0010\",\"0.2698±0.0008\"]\nscrps_arima = [\"0.1416\",\"0.0263\",\"0.0904\",\"0.1389\",\"0.1878\",\"0.0770\",\"0.1270\",\"0.2022\",\"0.2834\"]\n\nscrps_results = dict(Levels=levels,\n                     HINT=scrps_hint, \n                     DPMN=scrps_dpmn,\n                     HierE2E=scrps_hiere2e, \n                     ARIMA_MinTrace_B=scrps_arima_mintrace,\n                     ARIMA_BottomUp_B=scrps_arima_bu,\n                     ARIMA=scrps_arima)\nscrps_results = pd.DataFrame(scrps_results)\nscrps_results\n\n\n  \n    \n      \n\n\n\n\n\n\nLevels\nHINT\nDPMN\nHierE2E\nARIMA_MinTrace_B\nARIMA_BottomUp_B\nARIMA\n\n\n\n\n0\nOverall\n0.1183±0.0002\n0.1249±0.0020\n0.1472±0.0029\n0.1313±0.0009\n0.1375±0.0013\n0.1416\n\n\n1\nCountry\n0.0289±0.0007\n0.0431±0.0042\n0.0842±0.0051\n0.0471±0.0018\n0.0622±0.0026\n0.0263\n\n\n2\nState\n0.0596±0.0004\n0.0637±0.0032\n0.1012±0.0029\n0.0723±0.0011\n0.0820±0.0019\n0.0904\n\n\n3\nZone\n0.1027±0.0003\n0.1084±0.0033\n0.1317±0.0022\n0.1143±0.0007\n0.1207±0.0010\n0.1389\n\n\n4\nRegion\n0.1456±0.0004\n0.1554±0.0025\n0.1705±0.0023\n0.1591±0.0006\n0.1646±0.0007\n0.1878\n\n\n5\nCountry/Purpose\n0.0754±0.0006\n0.0700±0.0038\n0.0995±0.0061\n0.0723±0.0014\n0.0788±0.0018\n0.0770\n\n\n6\nState/Purpose\n0.1113±0.0001\n0.1070±0.0023\n0.1336±0.0042\n0.1243±0.0014\n0.1268±0.0017\n0.1270\n\n\n7\nZone/Purpose\n0.1779±0.0003\n0.1887±0.0032\n0.1955±0.0025\n0.1919±0.0008\n0.1949±0.0010\n0.2022\n\n\n8\nRegion/Purpose\n0.2447±0.0002\n0.2629±0.0034\n0.2615±0.0016\n0.2694±0.0006\n0.2698±0.0008\n0.2834"
  },
  {
    "objectID": "examples/hierarchicalnetworks.html#references",
    "href": "examples/hierarchicalnetworks.html#references",
    "title": "Hierarchical Forecast Networks",
    "section": "References",
    "text": "References\n\nKin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker (2023).”Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures”. International Journal Forecasting, accepted paper. URL https://arxiv.org/pdf/2110.13179.pdf.\nKin G. Olivares, Federico Garza, David Luo, Cristian Challu, Max Mergenthaler, Souhaib Ben Taieb, Shanika Wickramasuriya, and Artur Dubrawski (2023). “HierarchicalForecast: A reference framework for hierarchical forecasting”. Journal of Machine Learning Research, submitted. URL https://arxiv.org/abs/2207.03517"
  },
  {
    "objectID": "examples/forecasting_tft.html",
    "href": "examples/forecasting_tft.html",
    "title": "Temporal Fusion Transformer",
    "section": "",
    "text": "Temporal Fusion Transformer (TFT) proposed by Lim et al. [1] is one of the most popular transformer-based model for time-series forecasting. In summary, TFT combines gating layers, an LSTM recurrent encoder, with multi-head attention layers for a multi-step forecasting strategy decoder. For more details on the Nixtla’s TFT implementation visit this link.\nIn this notebook we show how to train the TFT model on the Texas electricity market load data (ERCOT). Accurately forecasting electricity markets is of great interest, as it is useful for planning distribution and consumption.\nWe will show you how to load the data, train the TFT performing automatic hyperparameter tuning, and produce forecasts. Then, we will show you how to perform multiple historical forecasts for cross validation.\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/forecasting_tft.html#libraries",
    "href": "examples/forecasting_tft.html#libraries",
    "title": "Temporal Fusion Transformer",
    "section": "1. Libraries",
    "text": "1. Libraries\n\n!pip install neuralforecast\n\n\nimport torch\nimport numpy as np\nimport pandas as pd\n\n\ntorch.cuda.is_available()\n\nTrue"
  },
  {
    "objectID": "examples/forecasting_tft.html#load-ercot-data",
    "href": "examples/forecasting_tft.html#load-ercot-data",
    "title": "Temporal Fusion Transformer",
    "section": "2. Load ERCOT Data",
    "text": "2. Load ERCOT Data\nThe input to NeuralForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nFirst, read the 2022 historic total demand of the ERCOT market. We processed the original data (available here), by adding the missing hour due to daylight saving time, parsing the date to datetime format, and filtering columns of interest.\n\nY_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nERCOT\n2021-01-01 00:00:00\n43719.849616\n\n\n1\nERCOT\n2021-01-01 01:00:00\n43321.050347\n\n\n2\nERCOT\n2021-01-01 02:00:00\n43063.067063\n\n\n3\nERCOT\n2021-01-01 03:00:00\n43090.059203\n\n\n4\nERCOT\n2021-01-01 04:00:00\n43486.590073"
  },
  {
    "objectID": "examples/forecasting_tft.html#model-training-and-forecast",
    "href": "examples/forecasting_tft.html#model-training-and-forecast",
    "title": "Temporal Fusion Transformer",
    "section": "3. Model training and forecast",
    "text": "3. Model training and forecast\nFirst, instantiate the AutoTFT model. The AutoTFT class will automatically perform hyperparamter tunning using Tune library, exploring a user-defined or default search space. Models are selected based on the error on a validation set and the best model is then stored and used during inference.\nTo instantiate AutoTFT you need to define:\n\nh: forecasting horizon\nloss: training loss\nconfig: hyperparameter search space. If None, the AutoTFT class will use a pre-defined suggested hyperparameter space.\nnum_samples: number of configurations explored.\n\n\nfrom ray import tune\n\nfrom neuralforecast.auto import AutoTFT\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.losses.pytorch import MAE\n\nimport logging\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n\n\n\n\n\n\n\nTip\n\n\n\nIncrease the num_samples parameter to explore a wider set of configurations for the selected models. As a rule of thumb choose it to be bigger than 15.\nWith num_samples=3 this example should run in around 20 minutes.\n\n\n\nhorizon = 24\nmodels = [AutoTFT(h=horizon,\n                  loss=MAE(),\n                  config=None,\n                  num_samples=3)]\n\n\n\n\n\n\n\nTip\n\n\n\nAll our models can be used for both point and probabilistic forecasting. For producing probabilistic outputs, simply modify the loss to one of our DistributionLoss. The complete list of losses is available in this link\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n\nTFT is a very large model and can require a lot of memory! If you are running out of GPU memory, try declaring your config search space and decrease the hidden_size, n_heads, and windows_batch_size parameters.\nThis are all the parameters of the config:\nconfig = {\n      \"input_size\": tune.choice([horizon]),\n      \"hidden_size\": tune.choice([32]),\n      \"n_head\": tune.choice([2]),\n      \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n      \"scaler_type\": tune.choice(['robust', 'standard']),\n      \"max_steps\": tune.choice([500, 1000]),\n      \"windows_batch_size\": tune.choice([32]),\n      \"check_val_every_n_epoch\": tune.choice([100]),\n      \"random_seed\": tune.randint(1, 20),\n}\n\n\n\nThe NeuralForecast class has built-in methods to simplify the forecasting pipelines, such as fit, predit, and cross_validation. Instantiate a NeuralForecast object with the following required parameters:\n\nmodels: a list of models.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\nThen, use the fit method to train the AutoTFT model on the ERCOT data. The total training time will depend on the hardware and the explored configurations, it should take between 10 and 30 minutes.\n\nnf = NeuralForecast(\n    models=models,\n    freq='H')\n\nnf.fit(df=Y_df)\n\nFinally, use the predict method to forecast the next 24 hours after the training data and plot the forecasts.\n\nY_hat_df = nf.predict()\nY_hat_df.head()\n\n\n\n\n\n\n\n\n\n\n\nds\nAutoTFT\n\n\nunique_id\n\n\n\n\n\n\nERCOT\n2022-10-01 00:00:00\n38644.019531\n\n\nERCOT\n2022-10-01 01:00:00\n36833.121094\n\n\nERCOT\n2022-10-01 02:00:00\n35698.265625\n\n\nERCOT\n2022-10-01 03:00:00\n35065.148438\n\n\nERCOT\n2022-10-01 04:00:00\n34788.566406\n\n\n\n\n\n\n\nPlot the results with matplot lib\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize = (10, 3))\nplot_df = pd.concat([Y_df.tail(24*5).reset_index(drop=True), Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\nplot_df[['y', 'AutoTFT']].plot(ax=ax, linewidth=2)\n\nax.set_title('Load [MW]', fontsize=12)\nax.set_ylabel('Monthly Passengers', fontsize=12)\nax.set_xlabel('Date', fontsize=12)\nax.legend(prop={'size': 10})\nax.grid()"
  },
  {
    "objectID": "examples/forecasting_tft.html#cross-validation-for-multiple-historic-forecasts",
    "href": "examples/forecasting_tft.html#cross-validation-for-multiple-historic-forecasts",
    "title": "Temporal Fusion Transformer",
    "section": "4. Cross validation for multiple historic forecasts",
    "text": "4. Cross validation for multiple historic forecasts\nThe cross_validation method allows you to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods. See this tutorial for an animation of how the windows are defined.\nWith time series data, cross validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models. The cross_validation method will use the validation set for hyperparameter selection, and will then produce the forecasts for the test set.\nUse the cross_validation method to produce all the daily forecasts for September. Set the validation and test sizes. To produce daily forecasts set the forecasting set the step size between windows as 24, to only produce one forecast per day.\n\nval_size  = 90*24 # 90 days x 24 hours\ntest_size = 30*24 # 30 days x 24 hours\nfcst_df = nf.cross_validation(df=Y_df, val_size=val_size, test_size=test_size,\n                                n_windows=None, step_size=horizon)\n\nFinally, we merge the forecasts with the Y_df dataset and plot the forecasts.\n\nY_hat_df = fcst_df.reset_index(drop=True)\nY_hat_df = Y_hat_df.drop(columns=['y','cutoff'])\n\n\nplot_df = Y_df.merge(Y_hat_df, on=['unique_id','ds'], how='outer').tail(test_size+24*7)\n\nplt.figure(figsize=(20,5))\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['AutoTFT'], c='blue', label='Forecast')\nplt.axvline(pd.to_datetime('2022-09-01'), color='red', linestyle='-.')\nplt.legend()\nplt.grid()\nplt.plot()\n\n[]"
  },
  {
    "objectID": "examples/forecasting_tft.html#next-steps",
    "href": "examples/forecasting_tft.html#next-steps",
    "title": "Temporal Fusion Transformer",
    "section": "Next Steps",
    "text": "Next Steps\nIn Challu et al [2] we demonstrate that the N-HiTS model outperforms the latest transformers by more than 20% with 50 times less computation.\nLearn how to use the N-HiTS and the NeuralForecast library in this tutorial."
  },
  {
    "objectID": "examples/forecasting_tft.html#references",
    "href": "examples/forecasting_tft.html#references",
    "title": "Temporal Fusion Transformer",
    "section": "References",
    "text": "References\n[1] Lim, B., Arık, S. Ö., Loeff, N., & Pfister, T. (2021). Temporal fusion transformers for interpretable multi-horizon time series forecasting. International Journal of Forecasting, 37(4), 1748-1764..\n[2] Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023."
  },
  {
    "objectID": "examples/automatic_hyperparameter_tuning.html",
    "href": "examples/automatic_hyperparameter_tuning.html",
    "title": "Automatic Hyperparameter Tuning",
    "section": "",
    "text": "Deep-learning models are the state-of-the-art in time series forecasting. They have outperformed statistical and tree-based approaches in recent large-scale competitions, such as the M series, and are being increasingly adopted in industry. However, their performance is greatly affected by the choice of hyperparameters. Selecting the optimal configuration, a process called hyperparameter tuning, is essential to achieve the best performance.\nThe main steps of hyperparameter tuning are:\nWith Neuralforecast, we automatize and simplify the hyperparameter tuning process with the Auto models. Every model in the library has an Auto version (for example, AutoNHITS, AutoTFT) which can perform automatic hyperparameter selection on default or user-defined search space. The Auto models wrap Ray’s Tune library with a user-friendly and simplified API, with most of its capabilities.\nIn this tutorial, we show in detail how to instantiate and train an AutoNHITS model with a custom search space, install and use HYPEROPT search algorithm, and use the model with optimal hyperparameters to forecast.\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/automatic_hyperparameter_tuning.html#install-neuralforecast",
    "href": "examples/automatic_hyperparameter_tuning.html#install-neuralforecast",
    "title": "Automatic Hyperparameter Tuning",
    "section": "1. Install Neuralforecast",
    "text": "1. Install Neuralforecast\n\n#%%capture\n#!pip install neuralforecast\n#!pip install hyperopt"
  },
  {
    "objectID": "examples/automatic_hyperparameter_tuning.html#load-data",
    "href": "examples/automatic_hyperparameter_tuning.html#load-data",
    "title": "Automatic Hyperparameter Tuning",
    "section": "2. Load Data",
    "text": "2. Load Data\nIn this example we will use the AirPasengers, a popular dataset with monthly airline passengers in the US from 1949 to 1960. Load the data, available at our utils methods in the required format. See https://nixtla.github.io/neuralforecast/examples/data_format.html for more details on the data input format.\n\nfrom neuralforecast.utils import AirPassengersDF\n\nY_df = AirPassengersDF\nY_df.head()\n\n/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n1.0\n1949-01-31\n112.0\n\n\n1\n1.0\n1949-02-28\n118.0\n\n\n2\n1.0\n1949-03-31\n132.0\n\n\n3\n1.0\n1949-04-30\n129.0\n\n\n4\n1.0\n1949-05-31\n121.0"
  },
  {
    "objectID": "examples/automatic_hyperparameter_tuning.html#define-hyperparameter-grid",
    "href": "examples/automatic_hyperparameter_tuning.html#define-hyperparameter-grid",
    "title": "Automatic Hyperparameter Tuning",
    "section": "3. Define hyperparameter grid",
    "text": "3. Define hyperparameter grid\nEach Auto model contains a default search space that was extensively tested on multiple large-scale datasets. Additionally, users can define specific search spaces tailored for particular datasets and tasks.\nFirst, we create a custom search space for the AutoNHITS model. Search spaces are specified with dictionaries, where keys corresponds to the model’s hyperparameter and the value is a Tune function to specify how the hyperparameter will be sampled. For example, use randint to sample integers uniformly, and choice to sample values of a list.\nIn the following example we are optimizing the learning_rate and two NHITS specific hyperparameters: n_pool_kernel_size and n_freq_downsample. Additionaly, we use the search space to modify default hyperparameters, such as max_steps and val_check_steps.\n\nfrom ray import tune\n\n\nnhits_config = {\n       \"max_steps\": 100,                                                         # Number of SGD steps\n       \"input_size\": 24,                                                         # Size of input window\n       \"learning_rate\": tune.loguniform(1e-5, 1e-1),                             # Initial Learning rate\n       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n       \"val_check_steps\": 50,                                                    # Compute validation every 50 steps\n       \"random_seed\": tune.randint(1, 10),                                       # Random seed\n    }\n\n\n\n\n\n\n\nImportant\n\n\n\nConfiguration dictionaries are not interchangeable between models since they have different hyperparameters. Refer to https://nixtla.github.io/neuralforecast/models.html for a complete list of each model’s hyperparameters."
  },
  {
    "objectID": "examples/automatic_hyperparameter_tuning.html#instantiate-auto-model",
    "href": "examples/automatic_hyperparameter_tuning.html#instantiate-auto-model",
    "title": "Automatic Hyperparameter Tuning",
    "section": "4. Instantiate Auto model",
    "text": "4. Instantiate Auto model\nTo instantiate an Auto model you need to define:\n\nh: forecasting horizon.\nloss: training and validation loss from neuralforecast.losses.pytorch.\nconfig: hyperparameter search space. If None, the Auto class will use a pre-defined suggested hyperparameter space.\nsearch_alg: search algorithm (from tune.search), default is random search. Refer to https://docs.ray.io/en/latest/tune/api_docs/suggestion.html for more information on the different search algorithm options.\nnum_samples: number of configurations explored.\n\nIn this example we set horizon h as 12, use the MAE loss for training and validation, and use the HYPEROPT search algorithm.\n\nfrom ray.tune.search.hyperopt import HyperOptSearch\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.auto import AutoNHITS\n\n\nmodel = AutoNHITS(h=12,\n                  loss=MAE(),\n                  config=nhits_config,\n                  search_alg=HyperOptSearch(),\n                  num_samples=20)\n\n\n\n\n\n\n\nTip\n\n\n\nThe number of samples, num_samples, is a crucial parameter! Larger values will usually produce better results as we explore more configurations in the search space, but it will increase training times. Larger search spaces will usually require more samples. As a general rule, we recommend setting num_samples higher than 20."
  },
  {
    "objectID": "examples/automatic_hyperparameter_tuning.html#train-model-and-predict-with-core-class",
    "href": "examples/automatic_hyperparameter_tuning.html#train-model-and-predict-with-core-class",
    "title": "Automatic Hyperparameter Tuning",
    "section": "5. Train model and predict with Core class",
    "text": "5. Train model and predict with Core class\nNext, we use the Neuralforecast class to train the Auto model. In this step, Auto models will automatically perform hyperparamter tuning training multiple models with different hyperparameters, producing the forecasts on the validation set, and evaluating them. The best configuration is selected based on the error on a validation set. Only the best model is stored and used during inference.\n\nfrom neuralforecast import NeuralForecast\n\nUse the val_size parameter of the fit method to control the length of the validation set. In this case we set the validation set as twice the forecasting horizon.\n\nnf = NeuralForecast(models=[model], freq='M')\nnf.fit(df=Y_df, val_size=24)\n\nThe results of the hyperparameter tuning are available in the results attribute of the Auto model. Use the get_dataframe method to get the results in a pandas dataframe.\n\nresults = model.results.get_dataframe()\nresults.head()\n\n\n\n\n\n\n\n\nloss\ntime_this_iter_s\ndone\ntimesteps_total\nepisodes_total\ntraining_iteration\ntrial_id\nexperiment_id\ndate\ntimestamp\n...\nconfig/h\nconfig/input_size\nconfig/learning_rate\nconfig/loss\nconfig/max_steps\nconfig/n_freq_downsample\nconfig/n_pool_kernel_size\nconfig/random_seed\nconfig/val_check_steps\nlogdir\n\n\n\n\n0\n34791.984375\n11.577301\nFalse\nNaN\nNaN\n2\n12439254\n80bb7a4e18544f81be9787d173752bca\n2023-01-20_16-26-43\n1674250003\n...\n12\n24\n0.035251\nMAE()\n100\n[168, 24, 1]\n[2, 2, 2]\n8\n50\n/Users/cchallu/ray_results/train_tune_2023-01-...\n\n\n1\n42.189243\n10.443844\nFalse\nNaN\nNaN\n2\n125ce09c\n80bb7a4e18544f81be9787d173752bca\n2023-01-20_16-27-05\n1674250025\n...\n12\n24\n0.009097\nMAE()\n100\n[24, 12, 1]\n[2, 2, 2]\n4\n50\n/Users/cchallu/ray_results/train_tune_2023-01-...\n\n\n2\n19.739182\n10.449568\nFalse\nNaN\nNaN\n2\n22d60660\n80bb7a4e18544f81be9787d173752bca\n2023-01-20_16-27-26\n1674250046\n...\n12\n24\n0.000144\nMAE()\n100\n[1, 1, 1]\n[2, 2, 2]\n7\n50\n/Users/cchallu/ray_results/train_tune_2023-01-...\n\n\n3\n15.072639\n9.953516\nFalse\nNaN\nNaN\n2\n2fd95ef2\n80bb7a4e18544f81be9787d173752bca\n2023-01-20_16-27-46\n1674250066\n...\n12\n24\n0.003416\nMAE()\n100\n[24, 12, 1]\n[16, 8, 1]\n1\n50\n/Users/cchallu/ray_results/train_tune_2023-01-...\n\n\n4\n58.782948\n11.148285\nFalse\nNaN\nNaN\n2\n3c93c36c\n80bb7a4e18544f81be9787d173752bca\n2023-01-20_16-28-08\n1674250088\n...\n12\n24\n0.000027\nMAE()\n100\n[24, 12, 1]\n[16, 8, 1]\n7\n50\n/Users/cchallu/ray_results/train_tune_2023-01-...\n\n\n\n\n5 rows × 28 columns\n\n\n\nNext, we use the predict method to forecast the next 12 months using the optimal hyperparameters.\n\nY_hat_df = nf.predict()\nY_hat_df = Y_hat_df.reset_index()\nY_hat_df.head()\n\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 93.27it/s] \n\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoNHITS\n\n\n\n\n0\n1.0\n1961-01-31\n441.228943\n\n\n1\n1.0\n1961-02-28\n429.527832\n\n\n2\n1.0\n1961-03-31\n498.914246\n\n\n3\n1.0\n1961-04-30\n495.418640\n\n\n4\n1.0\n1961-05-31\n507.392731\n\n\n\n\n\n\n\nFinally, we plot the original time series and the forecasts.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = pd.concat([Y_df, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\nplot_df[['y', 'AutoNHITS']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\n\nReferences\n\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023.\nJames Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl (2011). “Algorithms for Hyper-Parameter Optimization”. In: Advances in Neural Information Processing Systems. url: https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf\nKirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabas Poczos, Eric P. Xing (2019). “Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly”. Journal of Machine Learning Research. url: https://arxiv.org/abs/1903.06694\nLisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar (2016). “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization”. Journal of Machine Learning Research. url: https://arxiv.org/abs/1603.06560"
  },
  {
    "objectID": "examples/exogenous_variables.html",
    "href": "examples/exogenous_variables.html",
    "title": "Exogenous Variables",
    "section": "",
    "text": "Exogenous variables can provide additional information to greatly improve forecasting accuracy. Some examples include price or future promotions variables for demand forecasting, and weather data for electricity load forecast. In this notebook we show an example on how to add different types of exogenous variables to NeuralForecast models for making day-ahead hourly electricity price forecasts (EPF) for France and Belgium markets.\nAll NeuralForecast models are capable of incorporating exogenous variables: \\[\\mathbf{\\hat{y}}_{[t+1:t+H]} = F_\\theta(\\;\\mathbf{y}_{[:t]},\\; \\mathbf{x}^{(h)}_{[:t]},\\; \\mathbf{x}^{(f)}_{[:t+H]},\\; \\mathbf{x}^{(s)})\\]\nwhere the regressors are static exogenous \\(\\mathbf{x}^{(s)}\\), historic exogenous \\(\\mathbf{x}^{(h)}_{[:t]}\\), exogenous available at the time of the prediction \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) and autorregresive features \\(\\mathbf{y}_{[:t]}\\).\nWe will show you how to include exogenous variables in the data, specify variables to a model, and produce forecasts using future exogenous variables.\nYou can run these experiments using GPU with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/exogenous_variables.html#libraries",
    "href": "examples/exogenous_variables.html#libraries",
    "title": "Exogenous Variables",
    "section": "1. Libraries",
    "text": "1. Libraries\n\n!pip install neuralforecast\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nFalse"
  },
  {
    "objectID": "examples/exogenous_variables.html#load-data",
    "href": "examples/exogenous_variables.html#load-data",
    "title": "Exogenous Variables",
    "section": "2. Load data",
    "text": "2. Load data\nThe df dataframe contains the target and exogenous variables past information to train the model. The unique_id column identifies the markets, ds contains the datestamps, and y the electricity price.\nInclude both historic and future temporal variables as columns. In this example, we are adding the system load (system_load) as historic data. For future variables, we include a forecast of how much electricity will be produced (gen_forecast) and day of week (week_day). Both the electricity system demand and offer impact the price significantly, including these variables to the model greatly improve performance, as we demonstrate in Olivares et al. (2022).\nThe distinction between historic and future variables will be made later as parameters of the model.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE.csv')\ndf['ds'] = pd.to_datetime(df['ds'])\ndf.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\ngen_forecast\nsystem_load\nweek_day\n\n\n\n\n0\nFR\n2015-01-01 00:00:00\n53.48\n76905.0\n74812.0\n3\n\n\n1\nFR\n2015-01-01 01:00:00\n51.93\n75492.0\n71469.0\n3\n\n\n2\nFR\n2015-01-01 02:00:00\n48.76\n74394.0\n69642.0\n3\n\n\n3\nFR\n2015-01-01 03:00:00\n42.27\n72639.0\n66704.0\n3\n\n\n4\nFR\n2015-01-01 04:00:00\n38.41\n69347.0\n65051.0\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCalendar variables such as day of week, month, and year are very useful to capture long seasonalities.\n\n\n\nplt.figure(figsize=(15,5))\nplt.plot(df[df['unique_id']=='FR']['ds'], df[df['unique_id']=='FR']['y'])\nplt.xlabel('Date')\nplt.ylabel('Price [EUR/MWh]')\nplt.grid()\n\n\n\n\nAdd the static variables in a separate static_df dataframe. In this example, we are using one-hot encoding of the electricity market. The static_df must include one observation (row) for each unique_id of the df dataframe, with the different statics variables as columns.\n\nstatic_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_static.csv')\nstatic_df.head()\n\n\n\n\n\n\n\n\nunique_id\nmarket_0\nmarket_1\n\n\n\n\n0\nFR\n1\n0\n\n\n1\nBR\n0\n1"
  },
  {
    "objectID": "examples/exogenous_variables.html#training-with-exogenous-variables",
    "href": "examples/exogenous_variables.html#training-with-exogenous-variables",
    "title": "Exogenous Variables",
    "section": "3. Training with exogenous variables",
    "text": "3. Training with exogenous variables\nWe distinguish the exogenous variables by whether they reflect static or time-dependent aspects of the modeled data.\n\nStatic exogenous variables: The static exogenous variables carry time-invariant information for each time series. When the model is built with global parameters to forecast multiple time series, these variables allow sharing information within groups of time series with similar static variable levels. Examples of static variables include designators such as identifiers of regions, groups of products, etc.\nHistoric exogenous variables: This time-dependent exogenous variable is restricted to past observed values. Its predictive power depends on Granger-causality, as its past values can provide significant information about future values of the target variable \\(\\mathbf{y}\\).\nFuture exogenous variables: In contrast with historic exogenous variables, future values are available at the time of the prediction. Examples include calendar variables, weather forecasts, and known events that can cause large spikes and dips such as scheduled promotions.\n\nTo add exogenous variables to the model, first specify the name of each variable from the previous dataframes to the corresponding model hyperparameter during initialization: futr_exog_list, hist_exog_list, and stat_exog_list. We also set horizon as 24 to produce the next day hourly forecasts, and set input_size to use the last 5 days of data as input.\n\nfrom neuralforecast.auto import NHITS\nfrom neuralforecast.core import NeuralForecast\n\nimport logging\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n\n\nhorizon = 24 # day-ahead daily forecast\nmodels = [NHITS(h = horizon,\n                input_size = 5*horizon,\n                futr_exog_list = ['gen_forecast', 'week_day'], # &lt;- Future exogenous variables\n                hist_exog_list = ['system_load'], # &lt;- Historical exogenous variables\n                stat_exog_list = ['market_0', 'market_1'], # &lt;- Static exogenous variables\n                scaler_type = 'robust')]\n\n\n\n\n\n\n\nTip\n\n\n\nWhen including exogenous variables always use a scaler by setting the scaler_type hyperparameter. The scaler will scale all the temporal features: the target variable y, historic and future variables.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure future and historic variables are correctly placed. Defining historic variables as future variables will lead to data leakage.\n\n\nNext, pass the datasets to the df and static_df inputs of the fit method.\n\nnf = NeuralForecast(models=models, freq='H')\nnf.fit(df=df,\n       static_df=static_df)"
  },
  {
    "objectID": "examples/exogenous_variables.html#forecasting-with-exogenous-variables",
    "href": "examples/exogenous_variables.html#forecasting-with-exogenous-variables",
    "title": "Exogenous Variables",
    "section": "4. Forecasting with exogenous variables",
    "text": "4. Forecasting with exogenous variables\nBefore predicting the prices, we need to gather the future exogenous variables for the day we want to forecast. Define a new dataframe (futr_df) with the unique_id, ds, and future exogenous variables. There is no need to add the target variable y and historic variables as they won’t be used by the model.\n\nfutr_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_futr.csv')\nfutr_df['ds'] = pd.to_datetime(futr_df['ds'])\nfutr_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ngen_forecast\nweek_day\n\n\n\n\n0\nFR\n2016-11-01 00:00:00\n49118.0\n1\n\n\n1\nFR\n2016-11-01 01:00:00\n47890.0\n1\n\n\n2\nFR\n2016-11-01 02:00:00\n47158.0\n1\n\n\n3\nFR\n2016-11-01 03:00:00\n45991.0\n1\n\n\n4\nFR\n2016-11-01 04:00:00\n45378.0\n1\n\n\n\n\n\n\n\nFinally, use the predict method to forecast the day-ahead prices.\n\nY_hat_df = nf.predict(futr_df=futr_df)\nY_hat_df.head()\n\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 95.56it/s] \n\n\n\n\n\n\n\n\n\nds\nNHITS\n\n\nunique_id\n\n\n\n\n\n\nBE\n2016-11-01 00:00:00\n36.936493\n\n\nBE\n2016-11-01 01:00:00\n33.701057\n\n\nBE\n2016-11-01 02:00:00\n30.956253\n\n\nBE\n2016-11-01 03:00:00\n28.285088\n\n\nBE\n2016-11-01 04:00:00\n27.118006\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplot_df = df[df['unique_id']=='FR'].tail(24*5).reset_index(drop=True)\nY_hat_df = Y_hat_df.reset_index(drop=False)\nY_hat_df = Y_hat_df[Y_hat_df['unique_id']=='FR']\n\nplot_df = pd.concat([plot_df, Y_hat_df ]).set_index('ds') # Concatenate the train and forecast dataframes\n\nplot_df[['y', 'NHITS']].plot(linewidth=2)\nplt.axvline('2016-11-01', color='red')\nplt.ylabel('Price [EUR/MWh]', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.grid()\n\n\n\n\nIn summary, to add exogenous variables to a model make sure to follow the next steps:\n\nAdd temporal exogenous variables as columns to the main dataframe (df).\nAdd static exogenous variables with the static_df dataframe.\nSpecify the name for each variable in the corresponding model hyperparameter.\nIf the model uses future exogenous variables, pass the future dataframe (futr_df) to the predict method."
  },
  {
    "objectID": "examples/exogenous_variables.html#references",
    "href": "examples/exogenous_variables.html#references",
    "title": "Exogenous Variables",
    "section": "References",
    "text": "References\n\nKin G. Olivares, Cristian Challu, Grzegorz Marcjasz, Rafał Weron, Artur Dubrawski, Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx, International Journal of Forecasting\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html",
    "href": "examples/electricitypeakforecasting.html",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers’ combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an NHITS model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the Neuralforecast.cross_validation method to fit the model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit NHITS model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#introduction",
    "href": "examples/electricitypeakforecasting.html#introduction",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers’ combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an NHITS model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the Neuralforecast.cross_validation method to fit the model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit NHITS model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#libraries",
    "href": "examples/electricitypeakforecasting.html#libraries",
    "title": "Detect Demand Peaks",
    "section": "Libraries",
    "text": "Libraries\nWe assume you have NeuralForecast already installed. Check this guide for instructions on how to install NeuralForecast.\nInstall the necessary packages using pip install neuralforecast"
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#load-data",
    "href": "examples/electricitypeakforecasting.html#load-data",
    "title": "Detect Demand Peaks",
    "section": "Load Data",
    "text": "Load Data\nThe input to NeuralForecast models is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nFirst, download and read the 2022 historic total demand of the ERCOT market, available here. The data processing includes adding the missing hour due to daylight saving time, parsing the date to datetime format, and filtering columns of interest.\n\nimport numpy as np\nimport pandas as pd\n\n\n# Load data\nY_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv', parse_dates=['ds'])\nY_df = Y_df.query(\"ds &gt;= '2022-01-01' & ds &lt;= '2022-10-01'\")\n\n\nY_df.plot(x='ds', y='y', figsize=(20, 7))\n\n&lt;AxesSubplot:xlabel='ds'&gt;"
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#fit-and-forecast-with-nhits",
    "href": "examples/electricitypeakforecasting.html#fit-and-forecast-with-nhits",
    "title": "Detect Demand Peaks",
    "section": "Fit and Forecast with NHITS",
    "text": "Fit and Forecast with NHITS\nImport the NeuralForecast class and the models you need.\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoNHITS\n\n/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nFirst, instantiate the model and define the parameters. To instantiate AutoNHITS you need to define:\n\nh: forecasting horizon\nloss: training loss. Use the DistributionLoss to produce probabilistic forecasts. Default: MAE.\nconfig: hyperparameter search space. If None, the AutoNHITS class will use a pre-defined suggested hyperparameter space.\nnum_samples: number of configurations explored.\n\n\nmodels = [AutoNHITS(h=24,\n                    config=None, # Uses default config\n                    num_samples=10\n                   )\n         ]\n\nWe fit the model by instantiating a NeuralForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\n\n# Instantiate StatsForecast class as sf\nnf = NeuralForecast(\n    models=models,\n    freq='H', \n)\n\nThe cross_validation method allows the user to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods. This method re-trains the model and forecast each window. See this tutorial for an animation of how the windows are defined.\nUse the cross_validation method to produce all the daily forecasts for September. To produce daily forecasts set the forecasting horizon h as 24. In this example we are simulating deploying the pipeline during September, so set the number of windows as 30 (one for each day). Finally, set the step size between windows as 24, to only produce one forecast per day.\n\ncrossvalidation_df = nf.cross_validation(\n    df=Y_df,\n    step_size=24,\n    n_windows=30\n  )\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\nAutoNHITS\ny\n\n\n\n\n0\nERCOT\n2022-09-01 00:00:00\n2022-08-31 23:00:00\n46184.621094\n45482.471757\n\n\n1\nERCOT\n2022-09-01 01:00:00\n2022-08-31 23:00:00\n44108.359375\n43602.658043\n\n\n2\nERCOT\n2022-09-01 02:00:00\n2022-08-31 23:00:00\n42990.058594\n42284.817342\n\n\n3\nERCOT\n2022-09-01 03:00:00\n2022-08-31 23:00:00\n42040.902344\n41663.156771\n\n\n4\nERCOT\n2022-09-01 04:00:00\n2022-08-31 23:00:00\n42007.972656\n41710.621904\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using cross_validation make sure the forecasts are produced at the desired timestamps. Check the cutoff column which specifices the last timestamp before the forecasting window."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#peak-detection",
    "href": "examples/electricitypeakforecasting.html#peak-detection",
    "title": "Detect Demand Peaks",
    "section": "Peak Detection",
    "text": "Peak Detection\nFinally, we use the forecasts in crossvaldation_df to detect the daily hourly demand peaks. For each day, we set the detected peaks as the highest forecasts. In this case, we want to predict one peak (npeaks); depending on your setting and goals, this parameter might change. For example, the number of peaks can correspond to how many hours a battery can be discharged to reduce demand.\n\nnpeaks = 1 # Number of peaks\n\nFor the ERCOT 4CP detection task we are interested in correctly predicting the highest monthly load. Next, we filter the day in September with the highest hourly demand and predict the peak.\n\ncrossvalidation_df = crossvalidation_df.reset_index()[['ds','y','AutoNHITS']]\nmax_day = crossvalidation_df.iloc[crossvalidation_df['y'].argmax()].ds.day # Day with maximum load\ncv_df_day = crossvalidation_df.query('ds.dt.day == @max_day')\nmax_hour = cv_df_day['y'].argmax()\npeaks = cv_df_day['AutoNHITS'].argsort().iloc[-npeaks:].values # Predicted peaks\n\nIn the following plot we see how the model is able to correctly detect the coincident peak for September 2022.\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(10, 5))\nplt.axvline(cv_df_day.iloc[max_hour]['ds'], color='black', label='True Peak')\nplt.scatter(cv_df_day.iloc[peaks]['ds'], cv_df_day.iloc[peaks]['AutoNHITS'], color='green', label=f'Predicted Top-{npeaks}')\nplt.plot(cv_df_day['ds'], cv_df_day['y'], label='y', color='blue')\nplt.plot(cv_df_day['ds'], cv_df_day['AutoNHITS'], label='Forecast', color='red')\nplt.xlabel('Time')\nplt.ylabel('Load (MW)')\nplt.grid()\nplt.legend()\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this example we only include September. However, NHITS can correctly predict the peaks for the 4 months of 2022. You can try this by increasing the nwindows parameter of cross_validation or filtering the Y_df dataset. The complete run for all months take only 10 minutes."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#references",
    "href": "examples/electricitypeakforecasting.html#references",
    "title": "Detect Demand Peaks",
    "section": "References",
    "text": "References\n\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). “N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting”. Accepted at AAAI 2023."
  },
  {
    "objectID": "common.base_auto.html",
    "href": "common.base_auto.html",
    "title": "Hyperparameter Optimization",
    "section": "",
    "text": "Figure 1. Example of dataset split (left), validation (yellow) and test (orange). The hyperparameter optimization guiding signal is obtained from the validation set.\n\n\n\n\nBaseAuto\n\n BaseAuto (cls_model, h, loss, valid_loss, config,\n           search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n           object at 0x7f272048c040&gt;, num_samples=10, cpus=2, gpus=0,\n           refit_with_val=False, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n\n\nBaseAuto.fit\n\n BaseAuto.fit (dataset, val_size=0, test_size=0)\n\nBaseAuto.fit\nPerform the hyperparameter optimization as specified by the BaseAuto configuration dictionary config.\nThe optimization is performed on the TimeSeriesDataset using temporal cross validation with the validation set that sequentially precedes the test set.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset see details here val_size: int, size of temporal validation set (needs to be bigger than 0). test_size: int, size of temporal test set (default 0).\nReturns: self: fitted instance of BaseAuto with best hyperparameters and results.\n\n\n\nBaseAuto.predict\n\n BaseAuto.predict (dataset, step_size=1, **data_kwargs)\n\nBaseAuto.predict\nPredictions of the best performing model on validation.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset see details here step_size: int, steps between sequential predictions, (default 1). **data_kwarg: additional parameters for the dataset module.\nReturns: y_hat: numpy predictions of the NeuralForecast model.\n\n\nReferences\n\nJames Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl (2011). “Algorithms for Hyper-Parameter Optimization”. In: Advances in Neural Information Processing Systems. url: https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf\nKirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabas Poczos, Eric P. Xing (2019). “Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly”. Journal of Machine Learning Research. url: https://arxiv.org/abs/1903.06694\nLisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar (2016). “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization”. Journal of Machine Learning Research. url: https://arxiv.org/abs/1603.06560\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "common.base_windows.html",
    "href": "common.base_windows.html",
    "title": "BaseWindows",
    "section": "",
    "text": "BaseWindows\n\n BaseWindows (h, input_size, loss, valid_loss, learning_rate, max_steps,\n              val_check_steps, batch_size, windows_batch_size,\n              valid_batch_size, step_size=1, num_lr_decays=0,\n              early_stop_patience_steps=-1, scaler_type='identity',\n              futr_exog_list=None, hist_exog_list=None,\n              stat_exog_list=None, num_workers_loader=0,\n              drop_last_loader=False, random_seed=1, alias=None,\n              **trainer_kwargs)\n\nBase Windows\nBase class for all windows-based models. The forecasts are produced separately for each window, which are randomly sampled during training.\nThis class implements the basic functionality for all windows-based models, including: - PyTorch Lightning’s methods training_step, validation_step, predict_step. - fit and predict methods used by NeuralForecast.core class. - sampling and wrangling methods to generate windows.\n\n\n\nBaseWindows.fit\n\n BaseWindows.fit (dataset, val_size=0, test_size=0, random_seed=None)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, windows_batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nBaseWindows.predict\n\n BaseWindows.predict (dataset, test_size=None, step_size=1,\n                      random_seed=None, **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. test_size: int=None, test size for temporal cross-validation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation.\n\n\n\nBaseWindows.decompose\n\n BaseWindows.decompose (dataset, step_size=1, random_seed=None,\n                        **data_module_kwargs)\n\nDecompose Predictions.\nDecompose the predictions through the network’s layers. Available methods are ESRNN, NHITS, NBEATS, and NBEATSx.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation here. step_size: int=1, step size between each window of temporal data. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.tcn.html",
    "href": "models.tcn.html",
    "title": "TCN",
    "section": "",
    "text": "For long time in deep learning, sequence modelling was synonymous with recurrent networks, yet several papers have shown that simple convolutional architectures can outperform canonical recurrent networks like LSTMs by demonstrating longer effective memory. By skipping temporal connections the causal convolution filters can be applied to larger time spans while remaining computationally efficient.\nThe predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{TCN}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nwhere \\(\\mathbf{h}_{t}\\), is the hidden state for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction.\nReferences -van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. W., & Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. Computing Research Repository, abs/1609.03499. URL: http://arxiv.org/abs/1609.03499. arXiv:1609.03499. -Shaojie Bai, Zico Kolter, Vladlen Koltun. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. Computing Research Repository, abs/1803.01271. URL: https://arxiv.org/abs/1803.01271.\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.tcn.html#usage-example",
    "href": "models.tcn.html#usage-example",
    "title": "TCN",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import TCN\nfrom neuralforecast.losses.pytorch import GMM, MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[TCN(h=12,\n                input_size=-1,\n                #loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                loss=GMM(n_components=7, return_params=True, level=[80,90]),\n                learning_rate=5e-4,\n                kernel_size=2,\n                dilations=[1,2,4,8,16],\n                encoder_hidden_size=128,\n                context_size=10,\n                decoder_hidden_size=128,\n                decoder_layers=2,\n                max_steps=500,\n                scaler_type='robust',\n                futr_exog_list=['y_[lag12]'],\n                hist_exog_list=None,\n                stat_exog_list=['airline1'],\n                )\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['TCN-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['TCN-lo-90'][-12:].values,\n                 y2=plot_df['TCN-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "🧠 NeuralForecast",
    "section": "",
    "text": "Exogenous Variables: Static, historic and future exogenous support.\nForecast Interpretability: Plot trend, seasonality and exogenous NBEATS, NHITS, TFT, ESRNN prediction components.\nProbabilistic Forecasting: Simple model adapters for quantile losses and parametric distributions.\nTrain and Evaluation Losses Scale-dependent, percentage and scale independent errors, and parametric likelihoods.\nAutomatic Model Selection Parallelized automatic hyperparameter tuning, that efficiently searches best validation configuration.\nSimple Interface Unified SKLearn Interface for StatsForecast and MLForecast compatibility.\nModel Collection: Out of the box implementation of MLP, LSTM, RNN, TCN, DilatedRNN, NBEATS, NHITS, ESRNN, Informer, TFT, PatchTST, VanillaTransformer, StemGNN and HINT. See the entire collection here.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "🧠 NeuralForecast",
    "section": "",
    "text": "Exogenous Variables: Static, historic and future exogenous support.\nForecast Interpretability: Plot trend, seasonality and exogenous NBEATS, NHITS, TFT, ESRNN prediction components.\nProbabilistic Forecasting: Simple model adapters for quantile losses and parametric distributions.\nTrain and Evaluation Losses Scale-dependent, percentage and scale independent errors, and parametric likelihoods.\nAutomatic Model Selection Parallelized automatic hyperparameter tuning, that efficiently searches best validation configuration.\nSimple Interface Unified SKLearn Interface for StatsForecast and MLForecast compatibility.\nModel Collection: Out of the box implementation of MLP, LSTM, RNN, TCN, DilatedRNN, NBEATS, NHITS, ESRNN, Informer, TFT, PatchTST, VanillaTransformer, StemGNN and HINT. See the entire collection here."
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "🧠 NeuralForecast",
    "section": "Why?",
    "text": "Why?\nThere is a shared belief in Neural forecasting methods’ capacity to improve our pipeline’s accuracy and efficiency.\nUnfortunately, available implementations and published research are yet to realize neural networks’ potential. They are hard to use and continuously fail to improve over statistical methods while being computationally prohibitive. For this reason, we created NeuralForecast, a library favoring proven accurate and efficient models focusing on their usability."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "🧠 NeuralForecast",
    "section": "💻 Installation",
    "text": "💻 Installation\n\nPyPI\nYou can install NeuralForecast’s released version from the Python package index pip with:\npip install neuralforecast\n(Installing inside a python virtualenvironment or a conda environment is recommended.)\n\n\nConda\nAlso you can install NeuralForecast’s released version from conda with:\nconda install -c conda-forge neuralforecast\n(Installing inside a python virtualenvironment or a conda environment is recommended.)\n\n\nDev Mode\nIf you want to make some modifications to the code and see the effects in real time (without reinstalling), follow the steps below:\ngit clone https://github.com/Nixtla/neuralforecast.git\ncd neuralforecast\npip install -e ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "🧠 NeuralForecast",
    "section": "How to Use",
    "text": "How to Use\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\nimport matplotlib.pyplot as plt\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATS, NHITS\nfrom neuralforecast.utils import AirPassengersDF\n\n# Split data and declare panel dataset\nY_df = AirPassengersDF\nY_train_df = Y_df[Y_df.ds&lt;='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds&gt;'1959-12-31'] # 12 test\n\n# Fit and predict with NBEATS and NHITS models\nhorizon = len(Y_test_df)\nmodels = [NBEATS(input_size=2 * horizon, h=horizon, max_epochs=50),\n          NHITS(input_size=2 * horizon, h=horizon, max_epochs=50)]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(df=Y_train_df)\nY_hat_df = nf.predict().reset_index()\n\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\nplot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\n\nplot_df[['y', 'NBEATS', 'NHITS']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()"
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "🧠 NeuralForecast",
    "section": "🙏 How to Cite",
    "text": "🙏 How to Cite\nIf you enjoy or benefit from using these Python implementations, a citation to the repository will be greatly appreciated.\n@misc{olivares2022library_neuralforecast,\n    author={Kin G. Olivares and\n            Cristian Challú and\n            Federico Garza and\n            Max Mergenthaler Canseco and\n            Artur Dubrawski},\n    title = {{NeuralForecast}: User friendly state-of-the-art neural forecasting models.},\n    year={2022},\n    howpublished={{PyCon} Salt Lake City, Utah, US 2022},\n    url={https://github.com/Nixtla/neuralforecast}\n}"
  },
  {
    "objectID": "models.mlp.html",
    "href": "models.mlp.html",
    "title": "MLP",
    "section": "",
    "text": "Figure 1. Three layer MLP with autorregresive inputs.\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.mlp.html#usage-example",
    "href": "models.mlp.html#usage-example",
    "title": "MLP",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import MLP\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = MLP(h=12, input_size=24,\n            loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n            scaler_type='robust',\n            learning_rate=1e-3,\n            max_steps=200,\n            val_check_steps=10,\n            early_stop_patience_steps=2)\n\nfcst = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['MLP-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['MLP-lo-90'][-12:].values, \n                 y2=plot_df['MLP-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.grid()\nplt.legend()\nplt.plot()"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": " Models ",
    "section": "",
    "text": "source\n\n\n\n AutoRNN (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f1659f843a0&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengersDF as Y_df\n\n# Split train/test and declare time series dataset\nY_train_df = Y_df[Y_df.ds&lt;='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds&gt;'1959-12-31']   # 12 test\ndataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n\n\n# Use your own config or AutoRNN.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1, encoder_hidden_size=256)\nmodel = AutoRNN(h=12, config=config, num_samples=1, cpus=1)\n\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\n# Plotting predictions\nY_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\nY_plot_df['AutoRNN'] = y_hat\n\npd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()\n\n\nsource\n\n\n\n\n AutoLSTM (h, loss=MAE(), valid_loss=None, config=None,\n           search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n           object at 0x7f1660985bb0&gt;, num_samples=10,\n           refit_with_val=False, cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoLSTM.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1)\nmodel = AutoLSTM(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoGRU (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f1660977070&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoGRU.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1)\nmodel = AutoGRU(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoTCN (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f166096d250&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoTCN.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1)\nmodel = AutoTCN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoDilatedRNN (h, loss=MAE(), valid_loss=None, config=None,\n                 search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGen\n                 erator object at 0x7f1677cfeee0&gt;, num_samples=10,\n                 refit_with_val=False, cpus=2, gpus=0, verbose=False,\n                 alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoDilatedRNN.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1)\nmodel = AutoDilatedRNN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\n\n\n\n\nsource\n\n\n\n AutoMLP (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f1660961c10&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoMLP.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=12)\nmodel = AutoMLP(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoNBEATS (h, loss=MAE(), valid_loss=None, config=None,\n             search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerat\n             or object at 0x7f1660964580&gt;, num_samples=10,\n             refit_with_val=False, cpus=2, gpus=0, verbose=False,\n             alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNBEATS.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=12)\nmodel = AutoNBEATS(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoNBEATSx (h, loss=MAE(), valid_loss=None, config=None,\n              search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenera\n              tor object at 0x7f16609a43d0&gt;, num_samples=10,\n              refit_with_val=False, cpus=2, gpus=0, verbose=False,\n              alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNBEATS.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=12)\nmodel = AutoNBEATSx(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoNHITS (h, loss=MAE(), valid_loss=None, config=None,\n            search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerato\n            r object at 0x7f16609bffd0&gt;, num_samples=10,\n            refit_with_val=False, cpus=2, gpus=0, verbose=False,\n            alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoNHITS(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\n\n\n\n\nsource\n\n\n\n AutoTFT (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f16609c4340&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoTFT(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoVanillaTransformer (h, loss=MAE(), valid_loss=None, config=None,\n                         search_alg=&lt;ray.tune.search.basic_variant.BasicVa\n                         riantGenerator object at 0x7f1660a059a0&gt;,\n                         num_samples=10, refit_with_val=False, cpus=2,\n                         gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoVanillaTransformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoInformer (h, loss=MAE(), valid_loss=None, config=None,\n               search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGener\n               ator object at 0x7f16609b3d00&gt;, num_samples=10,\n               refit_with_val=False, cpus=2, gpus=0, verbose=False,\n               alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoInformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoAutoformer (h, loss=MAE(), valid_loss=None, config=None,\n                 search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGen\n                 erator object at 0x7f1660991ac0&gt;, num_samples=10,\n                 refit_with_val=False, cpus=2, gpus=0, verbose=False,\n                 alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoAutoformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoPatchTST (h, loss=MAE(), valid_loss=None, config=None,\n               search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGener\n               ator object at 0x7f16609fe100&gt;, num_samples=10,\n               refit_with_val=False, cpus=2, gpus=0, verbose=False,\n               alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoPatchTST(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\n\n\n\n\nsource\n\n\n\n AutoStemGNN (h, n_series, loss=MAE(), valid_loss=None, config=None,\n              search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenera\n              tor object at 0x7f16609fcfd0&gt;, num_samples=10,\n              refit_with_val=False, cpus=2, gpus=0, verbose=False,\n              alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n#%%capture\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoStemGNN(h=12, n_series=1, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.html#a.-rnn-based",
    "href": "models.html#a.-rnn-based",
    "title": " Models ",
    "section": "",
    "text": "source\n\n\n\n AutoRNN (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f1659f843a0&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengersDF as Y_df\n\n# Split train/test and declare time series dataset\nY_train_df = Y_df[Y_df.ds&lt;='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds&gt;'1959-12-31']   # 12 test\ndataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n\n\n# Use your own config or AutoRNN.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1, encoder_hidden_size=256)\nmodel = AutoRNN(h=12, config=config, num_samples=1, cpus=1)\n\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\n# Plotting predictions\nY_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\nY_plot_df['AutoRNN'] = y_hat\n\npd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()\n\n\nsource\n\n\n\n\n AutoLSTM (h, loss=MAE(), valid_loss=None, config=None,\n           search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n           object at 0x7f1660985bb0&gt;, num_samples=10,\n           refit_with_val=False, cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoLSTM.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1)\nmodel = AutoLSTM(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoGRU (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f1660977070&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoGRU.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1)\nmodel = AutoGRU(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoTCN (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f166096d250&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoTCN.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1)\nmodel = AutoTCN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoDilatedRNN (h, loss=MAE(), valid_loss=None, config=None,\n                 search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGen\n                 erator object at 0x7f1677cfeee0&gt;, num_samples=10,\n                 refit_with_val=False, cpus=2, gpus=0, verbose=False,\n                 alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoDilatedRNN.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=-1)\nmodel = AutoDilatedRNN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)"
  },
  {
    "objectID": "models.html#b.-mlp-based",
    "href": "models.html#b.-mlp-based",
    "title": " Models ",
    "section": "",
    "text": "source\n\n\n\n AutoMLP (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f1660961c10&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoMLP.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=12)\nmodel = AutoMLP(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoNBEATS (h, loss=MAE(), valid_loss=None, config=None,\n             search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerat\n             or object at 0x7f1660964580&gt;, num_samples=10,\n             refit_with_val=False, cpus=2, gpus=0, verbose=False,\n             alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNBEATS.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=12)\nmodel = AutoNBEATS(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoNBEATSx (h, loss=MAE(), valid_loss=None, config=None,\n              search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenera\n              tor object at 0x7f16609a43d0&gt;, num_samples=10,\n              refit_with_val=False, cpus=2, gpus=0, verbose=False,\n              alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNBEATS.default_config\nconfig = dict(max_steps=10, val_check_steps=1, input_size=12)\nmodel = AutoNBEATSx(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoNHITS (h, loss=MAE(), valid_loss=None, config=None,\n            search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerato\n            r object at 0x7f16609bffd0&gt;, num_samples=10,\n            refit_with_val=False, cpus=2, gpus=0, verbose=False,\n            alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoNHITS(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)"
  },
  {
    "objectID": "models.html#c.-transformer-based",
    "href": "models.html#c.-transformer-based",
    "title": " Models ",
    "section": "",
    "text": "source\n\n\n\n AutoTFT (h, loss=MAE(), valid_loss=None, config=None,\n          search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7f16609c4340&gt;, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoTFT(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoVanillaTransformer (h, loss=MAE(), valid_loss=None, config=None,\n                         search_alg=&lt;ray.tune.search.basic_variant.BasicVa\n                         riantGenerator object at 0x7f1660a059a0&gt;,\n                         num_samples=10, refit_with_val=False, cpus=2,\n                         gpus=0, verbose=False, alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoVanillaTransformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoInformer (h, loss=MAE(), valid_loss=None, config=None,\n               search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGener\n               ator object at 0x7f16609b3d00&gt;, num_samples=10,\n               refit_with_val=False, cpus=2, gpus=0, verbose=False,\n               alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoInformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoAutoformer (h, loss=MAE(), valid_loss=None, config=None,\n                 search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGen\n                 erator object at 0x7f1660991ac0&gt;, num_samples=10,\n                 refit_with_val=False, cpus=2, gpus=0, verbose=False,\n                 alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoAutoformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoPatchTST (h, loss=MAE(), valid_loss=None, config=None,\n               search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGener\n               ator object at 0x7f16609fe100&gt;, num_samples=10,\n               refit_with_val=False, cpus=2, gpus=0, verbose=False,\n               alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoPatchTST(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)"
  },
  {
    "objectID": "models.html#c.-graph-based",
    "href": "models.html#c.-graph-based",
    "title": " Models ",
    "section": "",
    "text": "source\n\n\n\n AutoStemGNN (h, n_series, loss=MAE(), valid_loss=None, config=None,\n              search_alg=&lt;ray.tune.search.basic_variant.BasicVariantGenera\n              tor object at 0x7f16609fcfd0&gt;, num_samples=10,\n              refit_with_val=False, cpus=2, gpus=0, verbose=False,\n              alias=None)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs. alias: str, optional, Custom name of the model.\n\n#%%capture\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoStemGNN(h=12, n_series=1, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)"
  },
  {
    "objectID": "common.base_recurrent.html",
    "href": "common.base_recurrent.html",
    "title": "BaseRecurrent",
    "section": "",
    "text": "The BaseRecurrent class contains standard methods shared across recurrent neural networks; these models possess the ability to process variable-length sequences of inputs through their internal memory states. The class is represented by LSTM, GRU, and RNN, along with other more sophisticated architectures like MQCNN.\nThe standard methods include TemporalNorm preprocessing, optimization utilities like parameter initialization, training_step, validation_step, and shared fit and predict methods.These shared methods enable all the neuralforecast.models compatibility with the core.NeuralForecast wrapper class.\n\n\nBaseRecurrent\n\n BaseRecurrent (h, input_size, inference_input_size, loss, valid_loss,\n                learning_rate, max_steps, val_check_steps, batch_size,\n                valid_batch_size, scaler_type='robust', num_lr_decays=0,\n                early_stop_patience_steps=-1, futr_exog_list=None,\n                hist_exog_list=None, stat_exog_list=None,\n                num_workers_loader=0, drop_last_loader=False,\n                random_seed=1, alias=None, **trainer_kwargs)\n\nBase Recurrent\nBase class for all recurrent-based models. The forecasts are produced sequentially between windows.\nThis class implements the basic functionality for all windows-based models, including: - PyTorch Lightning’s methods training_step, validation_step, predict_step.  - fit and predict methods used by NeuralForecast.core class.  - sampling and wrangling methods to sequential windows. \n\n\n\nBaseRecurrent.fit\n\n BaseRecurrent.fit (dataset, val_size=0, test_size=0, random_seed=None)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nBaseRecurrent.predict\n\n BaseRecurrent.predict (dataset, step_size=1, random_seed=None,\n                        **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.informer.html",
    "href": "models.informer.html",
    "title": "Informer",
    "section": "",
    "text": "The Informer model tackles the vanilla Transformer computational complexity challenges for long-horizon forecasting.\nThe architecture has three distinctive features: - A ProbSparse self-attention mechanism with an O time and memory complexity Llog(L). - A self-attention distilling process that prioritizes attention and efficiently handles long input sequences. - An MLP multi-step decoder that predicts long time-series sequences in a single forward operation rather than step-by-step.\nThe Informer model utilizes a three-component approach to define its embedding: - It employs encoded autoregressive features obtained from a convolution network. - It uses window-relative positional embeddings derived from harmonic functions. - Absolute positional embeddings obtained from calendar features are utilized.\nReferences - Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang. “Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting”\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.informer.html#auxiliary-functions",
    "href": "models.informer.html#auxiliary-functions",
    "title": "Informer",
    "section": "1. Auxiliary Functions",
    "text": "1. Auxiliary Functions\n\nsource\n\nConvLayer\n\n ConvLayer (c_in)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nProbAttention\n\n ProbAttention (mask_flag=True, factor=5, scale=None,\n                attention_dropout=0.1, output_attention=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nProbMask\n\n ProbMask (B, H, L, index, scores, device='cpu')\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "models.informer.html#informer",
    "href": "models.informer.html#informer",
    "title": "Informer",
    "section": "2. Informer",
    "text": "2. Informer\n\nsource\n\nInformer\n\n Informer (h:int, input_size:int, stat_exog_list=None,\n           hist_exog_list=None, futr_exog_list=None,\n           decoder_input_size_multiplier:float=0.5, hidden_size:int=128,\n           dropout:float=0.05, factor:int=3, n_head:int=4,\n           conv_hidden_size:int=32, activation:str='gelu',\n           encoder_layers:int=2, decoder_layers:int=1, distil:bool=True,\n           loss=MAE(), valid_loss=None, max_steps:int=5000,\n           learning_rate:float=0.0001, num_lr_decays:int=-1,\n           early_stop_patience_steps:int=-1, val_check_steps:int=100,\n           batch_size:int=32, valid_batch_size:Optional[int]=None,\n           windows_batch_size=1024, step_size:int=1,\n           scaler_type:str='identity', random_seed:int=1,\n           num_workers_loader:int=0, drop_last_loader:bool=False,\n           **trainer_kwargs)\n\nInformer\nThe Informer model tackles the vanilla Transformer computational complexity challenges for long-horizon forecasting. \nThe architecture has three distinctive features:\n1) A ProbSparse self-attention mechanism with an O time and memory complexity Llog(L).\n2) A self-attention distilling process that prioritizes attention and efficiently handles long input sequences.\n3) An MLP multi-step decoder that predicts long time-series sequences in a single forward operation rather than step-by-step.\nThe Informer model utilizes a three-component approach to define its embedding: 1) It employs encoded autoregressive features obtained from a convolution network. 2) It uses window-relative positional embeddings derived from harmonic functions. 3) Absolute positional embeddings obtained from calendar features are utilized.\nParameters: h: int, forecast horizon. input_size: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history. futr_exog_list: str list, future exogenous columns. hist_exog_list: str list, historic exogenous columns. stat_exog_list: str list, static exogenous columns. decoder_input_size_multiplier: float = 0.5, . hidden_size: int=128, units of embeddings and encoders. n_head: int=4, controls number of multi-head’s attention. dropout: float (0, 1), dropout throughout Informer architecture. factor: int=3, Probsparse attention factor. conv_hidden_size: int=32, channels of the convolutional encoder. activation: str=GELU, activation from [‘ReLU’, ‘Softplus’, ‘Tanh’, ‘SELU’, ‘LeakyReLU’, ‘PReLU’, ‘Sigmoid’, ‘GELU’]. encoder_layers: int=2, number of layers for the TCN encoder. decoder_layers: int=1, number of layers for the MLP decoder. distil: bool = True, wether the Informer decoder uses bottlenecks. loss: PyTorch module, instantiated train loss class from losses collection. max_steps: int=1000, maximum number of training steps. learning_rate: float=1e-3, Learning rate between (0, 1). valid_batch_size: int=None, number of different series in each validation and test batch. num_lr_decays: int=-1, Number of learning rate decays, evenly distributed across max_steps. early_stop_patience_steps: int=-1, Number of validation iterations before early stopping. val_check_steps: int=100, Number of training steps between every validation loss check. batch_size: int=32, number of differentseries in each batch. scaler_type: str=‘robust’, type of scaler for temporal inputs normalization see temporal scalers. random_seed: int=1, random_seed for pytorch initializer and numpy generators. num_workers_loader: int=os.cpu_count(), workers to be used by TimeSeriesDataLoader. drop_last_loader: bool=False, if True TimeSeriesDataLoader drops last non-full batch. alias: str, optional, Custom name of the model. **trainer_kwargs: int, keyword trainer arguments inherited from PyTorch Lighning’s trainer.\n*References*&lt;br&gt;\n- [Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang. \"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\"](https://arxiv.org/abs/2012.07436)&lt;br&gt;\n\n\n\nInformer.fit\n\n Informer.fit (dataset, val_size=0, test_size=0, random_seed=None)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, windows_batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nInformer.predict\n\n Informer.predict (dataset, test_size=None, step_size=1, random_seed=None,\n                   **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. test_size: int=None, test size for temporal cross-validation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation."
  },
  {
    "objectID": "models.informer.html#usage-example",
    "href": "models.informer.html#usage-example",
    "title": "Informer",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import MLP\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = Informer(h=12,\n                 input_size=24,\n                 hidden_size = 16,\n                 conv_hidden_size = 32,\n                 n_head = 2,\n                 #loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n                 loss=MAE(),\n                 futr_exog_list=calendar_cols,\n                 scaler_type='robust',\n                 learning_rate=1e-3,\n                 max_steps=5,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['Informer-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['Informer-lo-90'][-12:].values, \n                    y2=plot_df['Informer-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['Informer'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()"
  },
  {
    "objectID": "models.patchtst.html",
    "href": "models.patchtst.html",
    "title": "PatchTST",
    "section": "",
    "text": "The PatchTST model is an efficient Transformer-based model for multivariate time series forecasting.\nIt is based on two key components: - segmentation of time series into windows (patches) which are served as input tokens to Transformer - channel-independence. where each channel contains a single univariate time series.\nReferences - Nie, Y., Nguyen, N. H., Sinthong, P., & Kalagnanam, J. (2022). “A Time Series is Worth 64 Words: Long-term Forecasting with Transformers”\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.patchtst.html#backbone",
    "href": "models.patchtst.html#backbone",
    "title": "PatchTST",
    "section": "1. Backbone",
    "text": "1. Backbone\n\nAuxiliary Functions\n\nsource\n\n\nget_activation_fn\n\n get_activation_fn (activation)\n\n\nsource\n\n\nTranspose\n\n Transpose (*dims, contiguous=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\nPositional Encoding\n\nsource\n\n\npositional_encoding\n\n positional_encoding (pe, learn_pe, q_len, hidden_size)\n\n\nsource\n\n\nCoord1dPosEncoding\n\n Coord1dPosEncoding (q_len, exponential=False, normalize=True)\n\n\nsource\n\n\nCoord2dPosEncoding\n\n Coord2dPosEncoding (q_len, hidden_size, exponential=False,\n                     normalize=True, eps=0.001)\n\n\nsource\n\n\nPositionalEncoding\n\n PositionalEncoding (q_len, hidden_size, normalize=True)\n\n\n\nRevIN\n\nsource\n\n\nRevIN\n\n RevIN (num_features:int, eps=1e-05, affine=True, subtract_last=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\nEncoder\n\nsource\n\n\nTSTEncoderLayer\n\n TSTEncoderLayer (q_len, hidden_size, n_heads, d_k=None, d_v=None,\n                  linear_hidden_size=256, store_attn=False,\n                  norm='BatchNorm', attn_dropout=0, dropout=0.0,\n                  bias=True, activation='gelu', res_attention=False,\n                  pre_norm=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTSTEncoder\n\n TSTEncoder (q_len, hidden_size, n_heads, d_k=None, d_v=None,\n             linear_hidden_size=None, norm='BatchNorm', attn_dropout=0.0,\n             dropout=0.0, activation='gelu', res_attention=False,\n             n_layers=1, pre_norm=False, store_attn=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTSTiEncoder\n\n TSTiEncoder (c_in, patch_num, patch_len, max_seq_len=1024, n_layers=3,\n              hidden_size=128, n_heads=16, d_k=None, d_v=None,\n              linear_hidden_size=256, norm='BatchNorm', attn_dropout=0.0,\n              dropout=0.0, act='gelu', store_attn=False,\n              key_padding_mask='auto', padding_var=None, attn_mask=None,\n              res_attention=True, pre_norm=False, pe='zeros',\n              learn_pe=True)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nFlatten_Head\n\n Flatten_Head (individual, n_vars, nf, h, c_out, head_dropout=0)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nPatchTST_backbone\n\n PatchTST_backbone (c_in:int, c_out:int, input_size:int, h:int,\n                    patch_len:int, stride:int,\n                    max_seq_len:Optional[int]=1024, n_layers:int=3,\n                    hidden_size=128, n_heads=16, d_k:Optional[int]=None,\n                    d_v:Optional[int]=None, linear_hidden_size:int=256,\n                    norm:str='BatchNorm', attn_dropout:float=0.0,\n                    dropout:float=0.0, act:str='gelu',\n                    key_padding_mask:str='auto',\n                    padding_var:Optional[int]=None,\n                    attn_mask:Optional[torch.Tensor]=None,\n                    res_attention:bool=True, pre_norm:bool=False,\n                    store_attn:bool=False, pe:str='zeros',\n                    learn_pe:bool=True, fc_dropout:float=0.0,\n                    head_dropout=0, padding_patch=None,\n                    pretrain_head:bool=False, head_type='flatten',\n                    individual=False, revin=True, affine=True,\n                    subtract_last=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "models.patchtst.html#model",
    "href": "models.patchtst.html#model",
    "title": "PatchTST",
    "section": "2. Model",
    "text": "2. Model\n\nsource\n\nPatchTST\n\n PatchTST (h, input_size, stat_exog_list=None, hist_exog_list=None,\n           futr_exog_list=None, encoder_layers:int=3, n_heads:int=16,\n           hidden_size:int=128, linear_hidden_size:int=256,\n           dropout:float=0.2, fc_dropout:float=0.2,\n           head_dropout:float=0.0, attn_dropout:float=0.0,\n           patch_len:int=16, stride:int=8, revin:bool=True,\n           revin_affine:bool=False, revin_subtract_last:bool=True,\n           activation:str='gelu', res_attention:bool=True,\n           batch_normalization:bool=False, learn_pos_embed:bool=True,\n           loss=MAE(), valid_loss=None, max_steps:int=5000,\n           learning_rate:float=0.0001, num_lr_decays:int=-1,\n           early_stop_patience_steps:int=-1, val_check_steps:int=100,\n           batch_size:int=32, valid_batch_size:Optional[int]=None,\n           windows_batch_size=1024, step_size:int=1,\n           scaler_type:str='identity', random_seed:int=1,\n           num_workers_loader:int=0, drop_last_loader:bool=False,\n           **trainer_kwargs)\n\nPatchTST\nThe PatchTST model is an efficient Transformer-based model for multivariate time series forecasting.\nIt is based on two key components: - segmentation of time series into windows (patches) which are served as input tokens to Transformer - channel-independence, where each channel contains a single univariate time series.\nParameters: h: int, Forecast horizon.  input_size: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -&gt; y_[t-2:t]=[1,2]. stat_exog_list: str list, static exogenous columns. hist_exog_list: str list, historic exogenous columns. futr_exog_list: str list, future exogenous columns. encoder_layers: int, number of layers for encoder. n_heads: int=16, number of multi-head’s attention. hidden_size: int=128, units of embeddings and encoders. linear_hidden_size: int=256, units of linear layer. dropout: float=0.1, dropout rate for residual connection. fc_dropout: float=0.1, dropout rate for linear layer. head_dropout: float=0.1, dropout rate for Flatten head layer. attn_dropout: float=0.1, dropout rate for attention layer. patch_len: int=32, length of patch. stride: int=16, stride of patch. revin: bool=True, bool to use RevIn. revin_affine: bool=False, bool to use affine in RevIn. revin_substract_last: bool=False, bool to use substract last in RevIn. activation: str=‘ReLU’, activation from [‘gelu’,‘relu’]. res_attention: bool=False, bool to use residual attention. batch_normalization: bool=False, bool to use batch normalization. learn_pos_embedding: bool=True, bool to learn positional embedding. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. max_steps: int=1000, maximum number of training steps. learning_rate: float=1e-3, Learning rate between (0, 1). num_lr_decays: int=-1, Number of learning rate decays, evenly distributed across max_steps. early_stop_patience_steps: int=-1, Number of validation iterations before early stopping. val_check_steps: int=100, Number of training steps between every validation loss check. batch_size: int, number of different series in each batch. windows_batch_size: int=None, windows sampled from rolled data, default uses all. valid_batch_size: int=None, number of different series in each validation and test batch. step_size: int=1, step size between each window of temporal data. scaler_type: str=‘identity’, type of scaler for temporal inputs normalization see temporal scalers. random_seed: int, random_seed for pytorch initializer and numpy generators. num_workers_loader: int=os.cpu_count(), workers to be used by TimeSeriesDataLoader. drop_last_loader: bool=False, if True TimeSeriesDataLoader drops last non-full batch. alias: str, optional, Custom name of the model. **trainer_kwargs: int, keyword trainer arguments inherited from PyTorch Lighning’s trainer.\nReferences: -Nie, Y., Nguyen, N. H., Sinthong, P., & Kalagnanam, J. (2022). “A Time Series is Worth 64 Words: Long-term Forecasting with Transformers”\n\n\n\nPatchTST.fit\n\n PatchTST.fit (dataset, val_size=0, test_size=0, random_seed=None)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, windows_batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nPatchTST.predict\n\n PatchTST.predict (dataset, test_size=None, step_size=1, random_seed=None,\n                   **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. test_size: int=None, test size for temporal cross-validation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation."
  },
  {
    "objectID": "models.patchtst.html#usage-example",
    "href": "models.patchtst.html#usage-example",
    "title": "PatchTST",
    "section": "Usage example",
    "text": "Usage example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import MLP\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = PatchTST(h=12,\n                 input_size=104,\n                 patch_len=24,\n                 stride=24,\n                 revin=False,\n                 hidden_size=16,\n                 n_heads=4,\n                 scaler_type='robust',\n                 loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n                 #loss=MAE(),\n                 learning_rate=1e-3,\n                 max_steps=500,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['PatchTST-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['PatchTST-lo-90'][-12:].values, \n                    y2=plot_df['PatchTST-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['PatchTST'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()\n\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline2'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['PatchTST-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['PatchTST-lo-90'][-12:].values, \n                    y2=plot_df['PatchTST-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline2'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['PatchTST'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()"
  },
  {
    "objectID": "models.nhits.html",
    "href": "models.nhits.html",
    "title": "NHITS",
    "section": "",
    "text": "Long-horizon forecasting is challenging because of the volatility of the predictions and the computational complexity. To solve this problem we created the Neural Hierarchical Interpolation for Time Series (NHITS). NHITS builds upon NBEATS and specializes its partial outputs in the different frequencies of the time series through hierarchical interpolation and multi-rate input processing. On the long-horizon forecasting task NHITS improved accuracy by 25% on AAAI’s best paper award the Informer, while being 50x faster.\nThe model is composed of several MLPs with ReLU non-linearities. Blocks are connected via doubly residual stacking principle with the backcast \\(\\mathbf{\\tilde{y}}_{t-L:t,l}\\) and forecast \\(\\mathbf{\\hat{y}}_{t+1:t+H,l}\\) outputs of the l-th block. Multi-rate input pooling, hierarchical interpolation and backcast residual connections together induce the specialization of the additive predictions in different signal bands, reducing memory footprint and computational time, thus improving the architecture parsimony and accuracy.\nReferences -Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2019). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”. -Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2022). “NHITS: Neural Hierarchical Interpolation for Time Series Forecasting”. Accepted at the Thirty-Seventh AAAI Conference on Artificial Intelligence. -Zhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.; and Zhang, W. (2020). “Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting”. Association for the Advancement of Artificial Intelligence Conference 2021 (AAAI 2021).\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.nhits.html#usage-example",
    "href": "models.nhits.html#usage-example",
    "title": "NHITS",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHITS\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss, PMM, GMM, NBMM\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NHITS(h=12,\n              input_size=24,\n              #loss=DistributionLoss(distribution='StudentT', level=[80, 90], return_params=True),\n              #loss=DistributionLoss(distribution='Normal', level=[80, 90], return_params=True),\n              #loss=DistributionLoss(distribution='Poisson', level=[80, 90], return_params=True),\n              #loss=DistributionLoss(distribution='Tweedie', level=[80, 90], rho=1.5),\n              #loss=DistributionLoss(distribution='NegativeBinomial', level=[80, 90], return_params=True),\n              #loss=NBMM(n_components=2, level=[80,90]),\n              #loss=GMM(n_components=2, level=[80,90]),\n              #loss=PMM(n_components=1, level=[80,90]),\n              stat_exog_list=['airline1'],\n              futr_exog_list=['trend'],\n              n_freq_downsample=[2, 1, 1],\n              scaler_type='robust',\n              max_steps=200,\n              early_stop_patience_steps=2,\n              val_check_steps=10,\n              learning_rate=1e-3)\n\nfcst = NeuralForecast(models=[model], freq='M')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['NHITS-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['NHITS-lo-90'][-12:].values, \n                 y2=plot_df['NHITS-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import MLP\nfrom neuralforecast.losses.pytorch import DistributionLoss, Accuracy\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\nAirPassengersPanel['y'] = 1 * (AirPassengersPanel['trend'] % 12) &lt; 2\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = MLP(h=12,\n            input_size=24,\n            loss=DistributionLoss(distribution='Bernoulli', level=[80, 90], return_params=True),\n            valid_loss=Accuracy(),\n            stat_exog_list=['airline1'],\n            scaler_type='robust',\n            max_steps=200,\n            early_stop_patience_steps=2,\n            val_check_steps=10,\n            learning_rate=1e-3)\n\nfcst = NeuralForecast(models=[model], freq='M')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['MLP-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['MLP-lo-90'][-12:].values, \n                 y2=plot_df['MLP-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Example Data",
    "section": "",
    "text": "1. Synthetic Panel Data \n\nsource\n\ngenerate_series\n\n generate_series (n_series:int, freq:str='D', min_length:int=50,\n                  max_length:int=500, n_temporal_features:int=0,\n                  n_static_features:int=0, equal_ends:bool=False,\n                  seed:int=0)\n\nGenerate Synthetic Panel Series.\nGenerates n_series of frequency freq of different lengths in the interval [min_length, max_length]. If n_temporal_features &gt; 0, then each serie gets temporal features with random values. If n_static_features &gt; 0, then a static dataframe is returned along the temporal dataframe. If equal_ends == True then all series end at the same date.\nParameters: n_series: int, number of series for synthetic panel. min_length: int, minimal length of synthetic panel’s series. max_length: int, minimal length of synthetic panel’s series. n_temporal_features: int, default=0, number of temporal exogenous variables for synthetic panel’s series. n_static_features: int, default=0, number of static exogenous variables for synthetic panel’s series. equal_ends: bool, if True, series finish in the same date stamp ds. freq: str, frequency of the data, panda’s available frequencies.\nReturns: freq: pandas.DataFrame, synthetic panel with columns [unique_id, ds, y] and exogenous.\n\nfrom neuralforecast.utils import generate_series\n\nsynthetic_panel = generate_series(n_series=2)\nsynthetic_panel.groupby('unique_id').head(4)\n\n\n\n\n\n\n\n\nds\ny\n\n\nunique_id\n\n\n\n\n\n\n0\n2000-01-01\n0.357595\n\n\n0\n2000-01-02\n1.301382\n\n\n0\n2000-01-03\n2.272442\n\n\n0\n2000-01-04\n3.211827\n\n\n1\n2000-01-01\n5.399023\n\n\n1\n2000-01-02\n6.092818\n\n\n1\n2000-01-03\n0.476396\n\n\n1\n2000-01-04\n1.343744\n\n\n\n\n\n\n\n\ntemporal_df, static_df = generate_series(n_series=1000, n_static_features=2,\n                                         n_temporal_features=4, equal_ends=False)\nstatic_df.head(2)\n\n\n\n\n2. AirPassengers Data \nThe classic Box & Jenkins airline data. Monthly totals of international airline passengers, 1949 to 1960.\nIt has been used as a reference on several forecasting libraries, since it is a series that shows clear trends and seasonalities it offers a nice opportunity to quickly showcase a model’s predictions performance.\n\nfrom neuralforecast.utils import AirPassengersDF\n\nAirPassengersDF.head(12)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nmonth\nyear\n\n\n\n\n0\n1.0\n1949-01-31\n112.0\n1\n1949\n\n\n1\n1.0\n1949-02-28\n118.0\n2\n1949\n\n\n2\n1.0\n1949-03-31\n132.0\n3\n1949\n\n\n3\n1.0\n1949-04-30\n129.0\n4\n1949\n\n\n4\n1.0\n1949-05-31\n121.0\n5\n1949\n\n\n5\n1.0\n1949-06-30\n135.0\n6\n1949\n\n\n6\n1.0\n1949-07-31\n148.0\n7\n1949\n\n\n7\n1.0\n1949-08-31\n148.0\n8\n1949\n\n\n8\n1.0\n1949-09-30\n136.0\n9\n1949\n\n\n9\n1.0\n1949-10-31\n119.0\n10\n1949\n\n\n10\n1.0\n1949-11-30\n104.0\n11\n1949\n\n\n11\n1.0\n1949-12-31\n118.0\n12\n1949\n\n\n\n\n\n\n\n\n#We are going to plot the ARIMA predictions, and the prediction intervals.\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = AirPassengersDF.set_index('ds')\n\nplot_df[['y']].plot(ax=ax, linewidth=2)\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\nimport numpy as np\nimport pandas as pd\n\nn_static_features = 3\nn_series = 5\n\nstatic_features = np.random.uniform(low=0.0, high=1.0, \n                        size=(n_series, n_static_features))\nstatic_df = pd.DataFrame.from_records(static_features, \n                   columns = [f'static_{i}'for i in  range(n_static_features)])\nstatic_df['unique_id'] = np.arange(n_series)\n\n\nstatic_df\n\n\n\n3. Panel AirPassengers Data \nExtension to classic Box & Jenkins airline data. Monthly totals of international airline passengers, 1949 to 1960.\nIt includes two series with static, temporal and future exogenous variables, that can help to explore the performance of models like NBEATSx and TFT.\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = AirPassengersPanel.set_index('ds')\n\nplot_df.groupby('unique_id')['y'].plot(legend=True)\nax.set_title('AirPassengers Panel Data', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(title='unique_id', prop={'size': 15})\nax.grid()\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = AirPassengersPanel[AirPassengersPanel.unique_id=='Airline1'].set_index('ds')\n\nplot_df[['y', 'trend', 'y_[lag12]']].plot(ax=ax, linewidth=2)\nax.set_title('Box-Cox AirPassengers Data', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n4. Time Features\nWe have developed a utility that generates normalized calendar features for use as absolute positional embeddings in Transformer-based models. These embeddings capture seasonal patterns in time series data and can be easily incorporated into the model architecture. Additionally, the features can be used as exogenous variables in other models to inform them of calendar patterns in the data.\nReferences - Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang. “Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting”\n\nsource\n\naugment_calendar_df\n\n augment_calendar_df (df, freq='h')\n\n\n\nQ - [month]\nM - [month]\nW - [Day of month, week of year]\nD - [Day of week, day of month, day of year]\nB - [Day of week, day of month, day of year]\nH - [Hour of day, day of week, day of month, day of year]\nT - [Minute of hour*, hour of day, day of week, day of month, day of year]\nS - [Second of minute, minute of hour, hour of day, day of week, day of month, day of year] *minute returns a number from 0-3 corresponding to the 15 minute period it falls into.\n\n\n\nsource\n\n\ntime_features_from_frequency_str\n\n time_features_from_frequency_str (freq_str:str)\n\nReturns a list of time features that will be appropriate for the given frequency string. Parameters ———- freq_str Frequency string of the form [multiple][granularity] such as “12H”, “5min”, “1D” etc.\n\nsource\n\n\nWeekOfYear\n\n WeekOfYear ()\n\nWeek of year encoded as value between [-0.5, 0.5]\n\nsource\n\n\nMonthOfYear\n\n MonthOfYear ()\n\nMonth of year encoded as value between [-0.5, 0.5]\n\nsource\n\n\nDayOfYear\n\n DayOfYear ()\n\nDay of year encoded as value between [-0.5, 0.5]\n\nsource\n\n\nDayOfMonth\n\n DayOfMonth ()\n\nDay of month encoded as value between [-0.5, 0.5]\n\nsource\n\n\nDayOfWeek\n\n DayOfWeek ()\n\nHour of day encoded as value between [-0.5, 0.5]\n\nsource\n\n\nHourOfDay\n\n HourOfDay ()\n\nHour of day encoded as value between [-0.5, 0.5]\n\nsource\n\n\nMinuteOfHour\n\n MinuteOfHour ()\n\nMinute of hour encoded as value between [-0.5, 0.5]\n\nsource\n\n\nSecondOfMinute\n\n SecondOfMinute ()\n\nMinute of hour encoded as value between [-0.5, 0.5]\n\nsource\n\n\nTimeFeature\n\n TimeFeature ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAirPassengerPanelCalendar, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\nAirPassengerPanelCalendar.head()\n\n\nplot_df = AirPassengerPanelCalendar[AirPassengerPanelCalendar.unique_id=='Airline1'].set_index('ds')\nplt.plot(plot_df['month'])\nplt.grid()\nplt.xlabel('Datestamp')\nplt.ylabel('Normalized Month')\nplt.show()\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "common.scalers.html",
    "href": "common.scalers.html",
    "title": "TemporalNorm",
    "section": "",
    "text": "Figure 1. Illustration of temporal normalization (left), layer normalization (center) and batch normalization (right). The entries in green show the components used to compute the normalizing statistics.\n\n\n\n 1. Auxiliary Functions \n\n\nmasked_median\n\n masked_median (x, mask, dim=-1, keepdim=True)\n\nMasked Median\nCompute the median of tensor x along dim, ignoring values where mask is False. x and mask need to be broadcastable.\nParameters: x: torch.Tensor to compute median of along dim dimension. mask: torch Tensor bool with same shape as x, where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. dim (int, optional): Dimension to take median of. Defaults to -1. keepdim (bool, optional): Keep dimension of x or not. Defaults to True.\nReturns: x_median: torch.Tensor with normalized values.\n\n\n\nmasked_mean\n\n masked_mean (x, mask, dim=-1, keepdim=True)\n\nMasked Mean\nCompute the mean of tensor x along dimension, ignoring values where mask is False. x and mask need to be broadcastable.\nParameters: x: torch.Tensor to compute mean of along dim dimension. mask: torch Tensor bool with same shape as x, where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. dim (int, optional): Dimension to take mean of. Defaults to -1. keepdim (bool, optional): Keep dimension of x or not. Defaults to True.\nReturns: x_mean: torch.Tensor with normalized values.\n\n\n\n 2. Scalers \n\n\nminmax_scaler\n\n minmax_scaler (x, mask, eps=1e-06, dim=-1)\n\nMinMax Scaler\nStandardizes temporal features by ensuring its range dweels between [0,1] range. This transformation is often used as an alternative to the standard scaler. The scaled features are obtained as:\n\\[\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\mathrm{min}({\\mathbf{x}})_{[B,1,C]})/\n    (\\mathrm{max}({\\mathbf{x}})_{[B,1,C]}- \\mathrm{min}({\\mathbf{x}})_{[B,1,C]})\\]\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute min and max. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nminmax1_scaler\n\n minmax1_scaler (x, mask, eps=1e-06, dim=-1)\n\nMinMax1 Scaler\nStandardizes temporal features by ensuring its range dweels between [-1,1] range. This transformation is often used as an alternative to the standard scaler or classic Min Max Scaler. The scaled features are obtained as:\n\\[\\mathbf{z} = 2 (\\mathbf{x}_{[B,T,C]}-\\mathrm{min}({\\mathbf{x}})_{[B,1,C]})/ (\\mathrm{max}({\\mathbf{x}})_{[B,1,C]}- \\mathrm{min}({\\mathbf{x}})_{[B,1,C]})-1\\]\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute min and max. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nstd_scaler\n\n std_scaler (x, mask, dim=-1, eps=1e-06)\n\nStandard Scaler\nStandardizes features by removing the mean and scaling to unit variance along the dim dimension.\nFor example, for base_windows models, the scaled features are obtained as (with dim=1):\n\\[\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\bar{\\mathbf{x}}_{[B,1,C]})/\\hat{\\sigma}_{[B,1,C]}\\]\nParameters: x: torch.Tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute mean and std. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nrobust_scaler\n\n robust_scaler (x, mask, dim=-1, eps=1e-06)\n\nRobust Median Scaler\nStandardizes features by removing the median and scaling with the mean absolute deviation (mad) a robust estimator of variance. This scaler is particularly useful with noisy data where outliers can heavily influence the sample mean / variance in a negative way. In these scenarios the median and amd give better results.\nFor example, for base_windows models, the scaled features are obtained as (with dim=1):\n\\[\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\textrm{median}(\\mathbf{x})_{[B,1,C]})/\\textrm{mad}(\\mathbf{x})_{[B,1,C]}\\]\n\\[\\textrm{mad}(\\mathbf{x}) = \\frac{1}{N} \\sum_{}|\\mathbf{x} - \\mathrm{median}(x)|\\]\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute median and mad. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\ninvariant_scaler\n\n invariant_scaler (x, mask, dim=-1, eps=1e-06)\n\nInvariant Median Scaler\nStandardizes features by removing the median and scaling with the mean absolute deviation (mad) a robust estimator of variance. Aditionally it complements the transformation with the arcsinh transformation.\nFor example, for base_windows models, the scaled features are obtained as (with dim=1):\n\\[\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\textrm{median}(\\mathbf{x})_{[B,1,C]})/\\textrm{mad}(\\mathbf{x})_{[B,1,C]}\\]\n\\[\\mathbf{z} = \\textrm{arcsinh}(\\mathbf{z})\\]\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute median and mad. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nidentity_scaler\n\n identity_scaler (x, mask, dim=-1, eps=1e-06)\n\nIdentity Scaler\nA placeholder identity scaler, that is argument insensitive.\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute median and mad. Defaults to -1.\nReturns: x: original torch.Tensor x.\n\n\n\n 3. TemporalNorm Module \n\n\nTemporalNorm\n\n TemporalNorm (scaler_type='robust', dim=-1, eps=1e-06)\n\nTemporal Normalization\nStandardization of the features is a common requirement for many machine learning estimators, and it is commonly achieved by removing the level and scaling its variance. The TemporalNorm module applies temporal normalization over the batch of inputs as defined by the type of scaler.\n\\[\\mathbf{z}_{[B,T,C]} = \\textrm{Scaler}(\\mathbf{x}_{[B,T,C]})\\]\nParameters: scaler_type: str, defines the type of scaler used by TemporalNorm. available [identity, standard, robust, minmax, minmax1, invariant]. dim (int, optional): Dimension over to compute scale and shift. Defaults to -1. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6.\n\n\n\nTemporalNorm.transform\n\n TemporalNorm.transform (x, mask)\n\nCenter and scale the data.\nParameters: x: torch.Tensor shape [batch, time, channels]. mask: torch Tensor bool, shape [batch, time] where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nTemporalNorm.inverse_transform\n\n TemporalNorm.inverse_transform (z, x_shift=None, x_scale=None)\n\nScale back the data to the original representation.\nParameters: z: torch.Tensor shape [batch, time, channels], scaled.\nReturns: x: torch.Tensor original data.\n\n\n\n Example \n\nimport numpy as np\n\n\n# Declare synthetic batch to normalize\nx1 = 10**0 * np.arange(36)[:, None]\nx2 = 10**1 * np.arange(36)[:, None]\n\nnp_x = np.concatenate([x1, x2], axis=1)\nnp_x = np.repeat(np_x[None, :,:], repeats=2, axis=0)\nnp_x[0,:,:] = np_x[0,:,:] + 100\n\nnp_mask = np.ones(np_x.shape)\nnp_mask[:, -12:, :] = 0\n\nprint(f'x.shape [batch, time, features]={np_x.shape}')\nprint(f'mask.shape [batch, time, features]={np_mask.shape}')\n\n\n# Validate scalers\nx = 1.0*torch.tensor(np_x)\nmask = torch.tensor(np_mask)\nscaler = TemporalNorm(scaler_type='standard', dim=1)\nx_scaled = scaler.transform(x=x, mask=mask)\nx_recovered = scaler.inverse_transform(x_scaled)\n\nplt.plot(x[0,:,0], label='x1', color='#78ACA8')\nplt.plot(x[0,:,1], label='x2',  color='#E3A39A')\nplt.title('Before TemporalNorm')\nplt.xlabel('Time')\nplt.legend()\nplt.show()\n\nplt.plot(x_scaled[0,:,0], label='x1', color='#78ACA8')\nplt.plot(x_scaled[0,:,1]+0.1, label='x2+0.1', color='#E3A39A')\nplt.title(f'TemporalNorm \\'{scaler.scaler_type}\\' ')\nplt.xlabel('Time')\nplt.legend()\nplt.show()\n\nplt.plot(x_recovered[0,:,0], label='x1', color='#78ACA8')\nplt.plot(x_recovered[0,:,1], label='x2', color='#E3A39A')\nplt.title('Recovered')\nplt.xlabel('Time')\nplt.legend()\nplt.show()\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.tft.html",
    "href": "models.tft.html",
    "title": "TFT",
    "section": "",
    "text": "In summary Temporal Fusion Transformer (TFT) combines gating layers, an LSTM recurrent encoder, with multi-head attention layers for a multi-step forecasting strategy decoder.TFT’s inputs are static exogenous \\(\\mathbf{x}^{(s)}\\), historic exogenous \\(\\mathbf{x}^{(h)}_{[:t]}\\), exogenous available at the time of the prediction \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) and autorregresive features \\(\\mathbf{y}_{[:t]}\\), each of these inputs is further decomposed into categorical and continuous. The network uses a multi-quantile regression to model the following conditional probability:\\[\\mathbb{P}(\\mathbf{y}_{[t+1:t+H]}|\\;\\mathbf{y}_{[:t]},\\; \\mathbf{x}^{(h)}_{[:t]},\\; \\mathbf{x}^{(f)}_{[:t+H]},\\; \\mathbf{x}^{(s)})\\]\nReferences - Jan Golda, Krzysztof Kudrynski. “NVIDIA, Deep Learning Forecasting Examples” - Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister, “Temporal Fusion Transformers for interpretable multi-horizon time series forecasting”\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.tft.html#auxiliary-functions",
    "href": "models.tft.html#auxiliary-functions",
    "title": "TFT",
    "section": "1. Auxiliary Functions",
    "text": "1. Auxiliary Functions\n\n1.1 Gating Mechanisms\nThe Gated Residual Network (GRN) provides adaptive depth and network complexity capable of accommodating different size datasets. As residual connections allow for the network to skip the non-linear transformation of input \\(\\mathbf{a}\\) and context \\(\\mathbf{c}\\).\n\\[\\begin{align}\n\\eta_{1} &= \\mathrm{ELU}(\\mathbf{W}_{1}\\mathbf{a}+\\mathbf{W}_{2}\\mathbf{c}+\\mathbf{b}_{1}) \\\\\n\\eta_{2} &= \\mathbf{W}_{2}\\eta_{1}+b_{2} \\\\\n\\mathrm{GRN}(\\mathbf{a}, \\mathbf{c}) &= \\mathrm{LayerNorm}(a + \\textrm{GLU}(\\eta_{2}))\n\\end{align}\\]\nThe Gated Linear Unit (GLU) provides the flexibility of supressing unnecesary parts of the GRN. Consider GRN’s output \\(\\gamma\\) then GLU transformation is defined by:\n\\[\\mathrm{GLU}(\\gamma) = \\sigma(\\mathbf{W}_{4}\\gamma +b_{4}) \\odot (\\mathbf{W}_{5}\\gamma +b_{5})\\]\n\n\n\nFigure 2. Gated Residual Network.\n\n\n\n\n1.2 Variable Selection Networks\nTFT includes automated variable selection capabilities, through its variable selection network (VSN) components. The VSN takes the original input \\(\\{\\mathbf{x}^{(s)}, \\mathbf{x}^{(h)}_{[:t]}, \\mathbf{x}^{(f)}_{[:t]}\\}\\) and transforms it through embeddings or linear transformations into a high dimensional space \\(\\{\\mathbf{E}^{(s)}, \\mathbf{E}^{(h)}_{[:t]}, \\mathbf{E}^{(f)}_{[:t+H]}\\}\\).\nFor the observed historic data, the embedding matrix \\(\\mathbf{E}^{(h)}_{t}\\) at time \\(t\\) is a concatenation of \\(j\\) variable \\(e^{(h)}_{t,j}\\) embeddings: \\[\\begin{align}\n\\mathbf{E}^{(h)}_{t} &= [e^{(h)}_{t,1},\\dots,e^{(h)}_{t,j},\\dots,e^{(h)}_{t,n_{h}}] \\\\\n\\mathbf{\\tilde{e}}^{(h)}_{t,j} &= \\mathrm{GRN}(e^{(h)}_{t,j})\n\\end{align}\\]\nThe variable selection weights are given by: \\[s^{(h)}_{t}=\\mathrm{SoftMax}(\\mathrm{GRN}(\\mathbf{E}^{(h)}_{t},\\mathbf{E}^{(s)}))\\]\nThe VSN processed features are then: \\[\\tilde{\\mathbf{E}}^{(h)}_{t}= \\sum_{j} s^{(h)}_{j} \\tilde{e}^{(h)}_{t,j}\\]\n\n\n\nFigure 3. Variable Selection Network.\n\n\n\n\n1.3. Multi-Head Attention\nTo avoid information bottlenecks from the classic Seq2Seq architecture, TFT incorporates a decoder-encoder attention mechanism inherited transformer architectures (Li et. al 2019, Vaswani et. al 2017). It transform the the outputs of the LSTM encoded temporal features, and helps the decoder better capture long-term relationships.\nThe original multihead attention for each component \\(H_{m}\\) and its query, key, and value representations are denoted by \\(Q_{m}, K_{m}, V_{m}\\), its transformation is given by:\n\\[\\begin{align}\nQ_{m} = Q W_{Q,m} \\quad K_{m} = K W_{K,h} \\quad V_{m} = V W_{V,m} \\\\\nH_{m}=\\mathrm{Attention}(Q_{m}, K_{m}, V_{m}) = \\mathrm{SoftMax}(Q_{m} K^{\\intercal}_{m}/\\mathrm{scale}) \\; V_{m} \\\\\n\\mathrm{MultiHead}(Q, K, V) = [H_{1},\\dots,H_{M}] W_{M}\n\\end{align}\\]\nTFT modifies the original multihead attention to improve its interpretability. To do it it uses shared values \\(\\tilde{V}\\) across heads and employs additive aggregation, \\(\\mathrm{InterpretableMultiHead}(Q,K,V) = \\tilde{H} W_{M}\\). The mechanism has a great resemblence to a single attention layer, but it allows for \\(M\\) multiple attention weights, and can be therefore be interpreted as the average ensemble of \\(M\\) single attention layers.\n\\[\\begin{align}\n\\tilde{H} &= \\left(\\frac{1}{M} \\sum_{m} \\mathrm{SoftMax}(Q_{m} K^{\\intercal}_{m}/\\mathrm{scale}) \\right) \\tilde{V}\n          = \\frac{1}{M} \\sum_{m} \\mathrm{Attention}(Q_{m}, K_{m}, \\tilde{V}) \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "models.tft.html#tft-architecture",
    "href": "models.tft.html#tft-architecture",
    "title": "TFT",
    "section": "2. TFT Architecture",
    "text": "2. TFT Architecture\nThe first TFT’s step is embed the original input \\(\\{\\mathbf{x}^{(s)}, \\mathbf{x}^{(h)}, \\mathbf{x}^{(f)}\\}\\) into a high dimensional space \\(\\{\\mathbf{E}^{(s)}, \\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}\\}\\), after which each embedding is gated by a variable selection network (VSN). The static embedding \\(\\mathbf{E}^{(s)}\\) is used as context for variable selection and as initial condition to the LSTM. Finally the encoded variables are fed into the multi-head attention decoder.\n\\[\\begin{align}\nc_{s}, c_{e}, (c_{h}, c_{c}) &=\\textrm{StaticCovariateEncoder}(\\mathbf{E}^{(s)}) \\\\\n      h_{[:t]}, h_{[t+1:t+H]}  &=\\textrm{TemporalCovariateEncoder}(\\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}, c_{h}, c_{c}) \\\\\n\\hat{\\mathbf{y}}^{(q)}_{[t+1:t+H]} &=\\textrm{TemporalFusionDecoder}(h_{[t+1:t+H]}, c_{e})\n\\end{align}\\]\n\n2.1 Static Covariate Encoder\nThe static embedding \\(\\mathbf{E}^{(s)}\\) is transformed by the StaticCovariateEncoder into contexts \\(c_{s}, c_{e}, c_{h}, c_{c}\\). Where \\(c_{s}\\) are temporal variable selection contexts, \\(c_{e}\\) are TemporalFusionDecoder enriching contexts, and \\(c_{h}, c_{c}\\) are LSTM’s hidden/contexts for the TemporalCovariateEncoder.\n\\[\\begin{align}\nc_{s}, c_{e}, (c_{h}, c_{c}) & = \\textrm{GRN}(\\textrm{VSN}(\\mathbf{E}^{(s)}))\n\\end{align}\\]\n\n\n2.2 Temporal Covariate Encoder\nTemporalCovariateEncoder encodes the embeddings \\(\\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}\\) and contexts \\((c_{h}, c_{c})\\) with an LSTM.\n\\[\\begin{align}\n\\tilde{\\mathbf{E}}^{(h)}_{[:t]} & = \\textrm{VSN}(\\mathbf{E}^{(h)}_{[:t]}, c_{s}) \\\\\n\\tilde{\\mathbf{E}}^{(h)}_{[:t]} &= \\mathrm{LSTM}(\\tilde{\\mathbf{E}}^{(h)}_{[:t]}, (c_{h}, c_{c})) \\\\\nh_{[:t]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(\\tilde{\\mathbf{E}}^{(h)}_{[:t]}))\n\\end{align}\\]\nAn analogous process is repeated for the future data, with the main difference that \\(\\mathbf{E}^{(f)}\\) contains the future available information.\n\\[\\begin{align}\n\\tilde{\\mathbf{E}}^{(f)}_{[t+1:t+h]} & = \\textrm{VSN}(\\mathbf{E}^{(h)}_{t+1:t+H}, \\mathbf{E}^{(f)}_{t+1:t+H}, c_{s}) \\\\\n\\tilde{\\mathbf{E}}^{(f)}_{[t+1:t+h]} &= \\mathrm{LSTM}(\\tilde{\\mathbf{E}}^{(h)}_{[t+1:t+h]}, (c_{h}, c_{c})) \\\\\nh_{[t+1:t+H]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(\\tilde{\\mathbf{E}}^{(f)}_{[t+1:t+h]}))\n\\end{align}\\]\n\n\n2.3 Temporal Fusion Decoder\nThe TemporalFusionDecoder enriches the LSTM’s outputs with \\(c_{e}\\) and then uses an attention layer, and multi-step adapter. \\[\\begin{align}\nh_{[t+1:t+H]} &= \\mathrm{MultiHeadAttention}(h_{[:t]}, h_{[t+1:t+H]}, c_{e}) \\\\\nh_{[t+1:t+H]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(h_{[t+1:t+H]}) \\\\\nh_{[t+1:t+H]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(\\mathrm{GRN}(h_{[t+1:t+H]})) \\\\\n\\hat{\\mathbf{y}}^{(q)}_{[t+1:t+H]} &= \\mathrm{MLP}(h_{[t+1:t+H]})\n\\end{align}\\]"
  },
  {
    "objectID": "models.tft.html#tft-methods",
    "href": "models.tft.html#tft-methods",
    "title": "TFT",
    "section": "3. TFT methods",
    "text": "3. TFT methods\n\nsource\n\nTFT\n\n TFT (h, input_size, tgt_size:int=1, stat_exog_list=None,\n      hist_exog_list=None, futr_exog_list=None, hidden_size:int=128,\n      n_head:int=4, attn_dropout:float=0.0, dropout:float=0.1, loss=MAE(),\n      valid_loss=None, max_steps:int=1000, learning_rate:float=0.001,\n      num_lr_decays:int=-1, early_stop_patience_steps:int=-1,\n      val_check_steps:int=100, batch_size:int=32,\n      windows_batch_size:int=1024, valid_batch_size:Optional[int]=None,\n      step_size:int=1, scaler_type:str='robust', num_workers_loader=0,\n      drop_last_loader=False, random_seed:int=1, **trainer_kwargs)\n\nTFT\nThe Temporal Fusion Transformer architecture (TFT) is an Sequence-to-Sequence model that combines static, historic and future available data to predict an univariate target. The method combines gating layers, an LSTM recurrent encoder, with and interpretable multi-head attention layer and a multi-step forecasting strategy decoder.\nParameters: h: int, Forecast horizon.  input_size: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -&gt; y_[t-2:t]=[1,2]. stat_exog_list: str list, static continuous columns. hist_exog_list: str list, historic continuous columns. futr_exog_list: str list, future continuous columns. hidden_size: int, units of embeddings and encoders. dropout: float (0, 1), dropout of inputs VSNs. attn_dropout: float (0, 1), dropout of fusion decoder’s attention layer. shared_weights: bool, If True, all blocks within each stack will share parameters.  activation: str, activation from [‘ReLU’, ‘Softplus’, ‘Tanh’, ‘SELU’, ‘LeakyReLU’, ‘PReLU’, ‘Sigmoid’]. loss: PyTorch module, instantiated train loss class from losses collection. valid_loss: PyTorch module=loss, instantiated valid loss class from losses collection. max_steps: int=1000, maximum number of training steps. learning_rate: float=1e-3, Learning rate between (0, 1). num_lr_decays: int=-1, Number of learning rate decays, evenly distributed across max_steps. early_stop_patience_steps: int=-1, Number of validation iterations before early stopping. val_check_steps: int=100, Number of training steps between every validation loss check. batch_size: int, number of different series in each batch. windows_batch_size: int=None, windows sampled from rolled data, default uses all. valid_batch_size: int=None, number of different series in each validation and test batch. step_size: int=1, step size between each window of temporal data. scaler_type: str=‘robust’, type of scaler for temporal inputs normalization see temporal scalers. random_seed: int, random seed initialization for replicability. num_workers_loader: int=os.cpu_count(), workers to be used by TimeSeriesDataLoader. drop_last_loader: bool=False, if True TimeSeriesDataLoader drops last non-full batch. alias: str, optional, Custom name of the model. **trainer_kwargs: int, keyword trainer arguments inherited from PyTorch Lighning’s trainer.\nReferences: - Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister, “Temporal Fusion Transformers for interpretable multi-horizon time series forecasting”\n\n\n\nTFT.fit\n\n TFT.fit (dataset, val_size=0, test_size=0, random_seed=None)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, windows_batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nTFT.predict\n\n TFT.predict (dataset, test_size=None, step_size=1, random_seed=None,\n              **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. test_size: int=None, test size for temporal cross-validation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation."
  },
  {
    "objectID": "models.tft.html#usage-example",
    "href": "models.tft.html#usage-example",
    "title": "TFT",
    "section": "Usage Example",
    "text": "Usage Example\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss, GMM, PMM\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\n#from neuralforecast.models import TFT\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss, GMM, PMM\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\n#AirPassengersPanel['y'] = AirPassengersPanel['y'] + 10\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nnf = NeuralForecast(\n    models=[TFT(h=12, input_size=48,\n                hidden_size=20,\n                #loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n                #loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n                learning_rate=0.005,\n                stat_exog_list=['airline1'],\n                futr_exog_list=['y_[lag12]'],\n                max_steps=500,\n                val_check_steps=10,\n                early_stop_patience_steps=10,\n                scaler_type='robust',\n                windows_batch_size=None,\n                enable_progress_bar=True),\n    ],\n    freq='M'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nY_hat_df = nf.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['TFT'], c='purple', label='mean')\nplt.plot(plot_df['ds'], plot_df['TFT-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['TFT-lo-90'][-12:].values, \n                 y2=plot_df['TFT-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "losses.pytorch.html",
    "href": "losses.pytorch.html",
    "title": "PyTorch Losses",
    "section": "",
    "text": "These metrics are on the same scale as the data.\n\n\n\nsource\n\n\n\n MAE.__init__ ()\n\nMean Absolute Error\nCalculates Mean Absolute Error between y and y_hat. MAE measures the relative prediction accuracy of a forecasting method by calculating the deviation of the prediction and the true value at a given time and averages these devations over the length of the series.\n\\[ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| \\]\n\nsource\n\n\n\n\n MAE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n               mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mae: tensor (single value).\n\n\n\n\n\n\nsource\n\n\n\n MSE.__init__ ()\n\nMean Squared Error\nCalculates Mean Squared Error between y and y_hat. MSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the true value at a given time, and averages these devations over the length of the series.\n\\[ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\]\n\nsource\n\n\n\n\n MSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n               mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mse: tensor (single value).\n\n\n\n\n\n\nsource\n\n\n\n RMSE.__init__ ()\n\nRoot Mean Squared Error\nCalculates Root Mean Squared Error between y and y_hat. RMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. Finally the RMSE will be in the same scale as the original time series so its comparison with other series is possible only if they share a common scale. RMSE has a direct connection to the L2 norm.\n\\[ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} \\]\n\nsource\n\n\n\n\n RMSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: rmse: tensor (single value).\nGive us a ⭐ on Github"
  },
  {
    "objectID": "losses.pytorch.html#mean-absolute-error-mae",
    "href": "losses.pytorch.html#mean-absolute-error-mae",
    "title": "PyTorch Losses",
    "section": "",
    "text": "source\n\n\n\n MAE.__init__ ()\n\nMean Absolute Error\nCalculates Mean Absolute Error between y and y_hat. MAE measures the relative prediction accuracy of a forecasting method by calculating the deviation of the prediction and the true value at a given time and averages these devations over the length of the series.\n\\[ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| \\]\n\nsource\n\n\n\n\n MAE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n               mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mae: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#mean-squared-error-mse",
    "href": "losses.pytorch.html#mean-squared-error-mse",
    "title": "PyTorch Losses",
    "section": "",
    "text": "source\n\n\n\n MSE.__init__ ()\n\nMean Squared Error\nCalculates Mean Squared Error between y and y_hat. MSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the true value at a given time, and averages these devations over the length of the series.\n\\[ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\]\n\nsource\n\n\n\n\n MSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n               mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mse: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#root-mean-squared-error-rmse",
    "href": "losses.pytorch.html#root-mean-squared-error-rmse",
    "title": "PyTorch Losses",
    "section": "",
    "text": "source\n\n\n\n RMSE.__init__ ()\n\nRoot Mean Squared Error\nCalculates Root Mean Squared Error between y and y_hat. RMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. Finally the RMSE will be in the same scale as the original time series so its comparison with other series is possible only if they share a common scale. RMSE has a direct connection to the L2 norm.\n\\[ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} \\]\n\nsource\n\n\n\n\n RMSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: rmse: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#mean-absolute-percentage-error-mape",
    "href": "losses.pytorch.html#mean-absolute-percentage-error-mape",
    "title": "PyTorch Losses",
    "section": "Mean Absolute Percentage Error (MAPE)",
    "text": "Mean Absolute Percentage Error (MAPE)\n\nsource\n\nMAPE.__init__\n\n MAPE.__init__ ()\n\nMean Absolute Percentage Error\nCalculates Mean Absolute Percentage Error between y and y_hat. MAPE measures the relative prediction accuracy of a forecasting method by calculating the percentual deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. The closer to zero an observed value is, the higher penalty MAPE loss assigns to the corresponding error.\n\\[ \\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|} \\]\n\nsource\n\n\nMAPE.__call__\n\n MAPE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mape: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#symmetric-mape-smape",
    "href": "losses.pytorch.html#symmetric-mape-smape",
    "title": "PyTorch Losses",
    "section": "Symmetric MAPE (sMAPE)",
    "text": "Symmetric MAPE (sMAPE)\n\nsource\n\nSMAPE.__init__\n\n SMAPE.__init__ ()\n\nSymmetric Mean Absolute Percentage Error\nCalculates Symmetric Mean Absolute Percentage Error between y and y_hat. SMAPE measures the relative prediction accuracy of a forecasting method by calculating the relative deviation of the prediction and the observed value scaled by the sum of the absolute values for the prediction and observed value at a given time, then averages these devations over the length of the series. This allows the SMAPE to have bounds between 0% and 200% which is desireble compared to normal MAPE that may be undetermined when the target is zero.\n\\[ \\mathrm{sMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|} \\]\nReferences: Makridakis S., “Accuracy measures: theoretical and practical concerns”.\n\nsource\n\n\nSMAPE.__call__\n\n SMAPE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                 mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: smape: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#mean-absolute-scaled-error-mase",
    "href": "losses.pytorch.html#mean-absolute-scaled-error-mase",
    "title": "PyTorch Losses",
    "section": "Mean Absolute Scaled Error (MASE)",
    "text": "Mean Absolute Scaled Error (MASE)\n\nsource\n\nMASE.__init__\n\n MASE.__init__ (seasonality:int)\n\nMean Absolute Scaled Error Calculates the Mean Absolute Scaled Error between y and y_hat. MASE measures the relative prediction accuracy of a forecasting method by comparinng the mean absolute errors of the prediction and the observed value against the mean absolute errors of the seasonal naive model. The MASE partially composed the Overall Weighted Average (OWA), used in the M4 Competition.\n\\[ \\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})} \\]\nParameters: seasonality: int. Main frequency of the time series; Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\nReferences: Rob J. Hyndman, & Koehler, A. B. “Another look at measures of forecast accuracy”. Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, “The M4 Competition: 100,000 time series and 61 forecasting methods”.\n\nsource\n\n\nMASE.__call__\n\n MASE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                y_insample:torch.Tensor, mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor (batch_size, output_size), Actual values. y_hat: tensor (batch_size, output_size)), Predicted values. y_insample: tensor (batch_size, input_size), Actual insample Seasonal Naive predictions. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mase: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#quantile-loss",
    "href": "losses.pytorch.html#quantile-loss",
    "title": "PyTorch Losses",
    "section": "Quantile Loss",
    "text": "Quantile Loss\n\nsource\n\nQuantileLoss.__init__\n\n QuantileLoss.__init__ (q)\n\nQuantile Loss\nComputes the quantile loss between y and y_hat. QL measures the deviation of a quantile forecast. By weighting the absolute deviation in a non symmetric way, the loss pays more attention to under or over estimation. A common value for q is 0.5 for the deviation from the median (Pinball loss).\n\\[ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) \\]\nParameters: q: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”.\n\nsource\n\n\nQuantileLoss.__call__\n\n QuantileLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                        mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: quantile_loss: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#accuracy",
    "href": "losses.pytorch.html#accuracy",
    "title": "PyTorch Losses",
    "section": "Accuracy",
    "text": "Accuracy\n\nsource\n\nAccuracy.__init__\n\n Accuracy.__init__ ()\n\nAccuracy\nComputes the accuracy between categorical y and y_hat. This evaluation metric is only meant for evalution, as it is not differentiable.\n\\[ \\mathrm{Accuracy}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\mathrm{1}\\{\\mathbf{y}_{\\tau}==\\mathbf{\\hat{y}}_{\\tau}\\} \\]\n\nsource\n\n\nAccuracy.__call__\n\n Accuracy.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                    mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: accuracy: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#multi-quantile-loss-mqloss",
    "href": "losses.pytorch.html#multi-quantile-loss-mqloss",
    "title": "PyTorch Losses",
    "section": "Multi Quantile Loss (MQLoss)",
    "text": "Multi Quantile Loss (MQLoss)\n\nsource\n\nMQLoss.__init__\n\n MQLoss.__init__ (level=[80, 90], quantiles=None)\n\nMulti-Quantile loss\nCalculates the Multi-Quantile loss (MQL) between y and y_hat. MQL calculates the average multi-quantile Loss for a given set of quantiles, based on the absolute difference between predicted quantiles and observed values.\n\\[ \\mathrm{MQL}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) \\]\nThe limit behavior of MQL allows to measure the accuracy of a full predictive distribution \\(\\mathbf{\\hat{F}}_{\\tau}\\) with the continuous ranked probability score (CRPS). This can be achieved through a numerical integration technique, that discretizes the quantiles and treats the CRPS integral with a left Riemann approximation, averaging over uniformly distanced quantiles.\n\\[ \\mathrm{CRPS}(y_{\\tau}, \\mathbf{\\hat{F}}_{\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) dq \\]\nParameters: level: int list [0,100]. Probability levels for prediction intervals (Defaults median). quantiles: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”. James E. Matheson and Robert L. Winkler, “Scoring Rules for Continuous Probability Distributions”.\n\nsource\n\n\nMQLoss.__call__\n\n MQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                  mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mqloss: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#weighted-mqloss-wmqloss",
    "href": "losses.pytorch.html#weighted-mqloss-wmqloss",
    "title": "PyTorch Losses",
    "section": "Weighted MQLoss (wMQLoss)",
    "text": "Weighted MQLoss (wMQLoss)\n\nsource\n\nwMQLoss.__init__\n\n wMQLoss.__init__ (level=[80, 90], quantiles=None)\n\nWeighted Multi-Quantile loss\nCalculates the Weighted Multi-Quantile loss (WMQL) between y and y_hat. WMQL calculates the weighted average multi-quantile Loss for a given set of quantiles, based on the absolute difference between predicted quantiles and observed values.\n\\[ \\mathrm{wMQL}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \\frac{1}{n} \\sum_{q_{i}} \\frac{\\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau})}{\\sum^{t+H}_{\\tau=t+1} |y_{\\tau}|} \\]\nParameters: level: int list [0,100]. Probability levels for prediction intervals (Defaults median). quantiles: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”. James E. Matheson and Robert L. Winkler, “Scoring Rules for Continuous Probability Distributions”.\n\nsource\n\n\nwMQLoss.__call__\n\n wMQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                   mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mqloss: tensor (single value).\n\nsource\n\n\nsCRPS.__init__\n\n sCRPS.__init__ (level=[80, 90], quantiles=None)\n\nScaled Continues Ranked Probability Score\nCalculates a scaled variation of the CRPS, as proposed by Rangapuram (2021), to measure the accuracy of predicted quantiles y_hat compared to the observation y.\nThis metric averages percentual weighted absolute deviations as defined by the quantile losses.\n\\[ \\mathrm{sCRPS}(\\mathbf{\\hat{y}}^{(q)}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n\\int^{1}_{0}\n\\frac{\\mathrm{QL}(\\mathbf{\\hat{y}}^{(q}_{\\tau} y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq \\]\nwhere \\(\\mathbf{\\hat{y}}^{(q}_{\\tau}\\) is the estimated quantile, and \\(y_{i,\\tau}\\) are the target variable realizations.\nParameters: level: int list [0,100]. Probability levels for prediction intervals (Defaults median). quantiles: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\nReferences: - Gneiting, Tilmann. (2011). “Quantiles as optimal point forecasts”. International Journal of Forecasting. - Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). “The M5 uncertainty competition: Results, findings and conclusions”. International Journal of Forecasting. - Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). “End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series”. Proceedings of the 38th International Conference on Machine Learning (ICML).\n\nsource\n\n\nsCRPS.__call__\n\n sCRPS.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                 mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per series to consider in loss.\nReturns: scrps: tensor (single value).\n\nsource\n\n\nrelMSE.__init__\n\n relMSE.__init__ (y_train)\n\nRelative Mean Squared Error Computes Relative Mean Squared Error (relMSE), as proposed by Hyndman & Koehler (2006) as an alternative to percentage errors, to avoid measure unstability. \\[ \\mathrm{relMSE}(\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{\\hat{y}}^{naive1}) =\n\\frac{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}}^{naive1})} \\] Parameters: y_train: numpy array, Training values. References: - Hyndman, R. J and Koehler, A. B. (2006). “Another look at measures of forecast accuracy”, International Journal of Forecasting, Volume 22, Issue 4. - Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. “Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International Journal Forecasting, Working paper available at arxiv.\n\nsource\n\n\nrelMSE.__call__\n\n relMSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                  mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per series to consider in loss.\nReturns: relmse: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#distributionloss",
    "href": "losses.pytorch.html#distributionloss",
    "title": "PyTorch Losses",
    "section": "DistributionLoss",
    "text": "DistributionLoss\n\nsource\n\nDistributionLoss.__init__\n\n DistributionLoss.__init__ (distribution, level=[80, 90], quantiles=None,\n                            num_samples=1000, return_params=False,\n                            **distribution_kwargs)\n\nDistributionLoss\nThis PyTorch module wraps the torch.distribution classes allowing it to interact with NeuralForecast models modularly. It shares the negative log-likelihood as the optimization objective and a sample method to generate empirically the quantiles defined by the level list.\nAdditionally, it implements a distribution transformation that factorizes the scale-dependent likelihood parameters into a base scale and a multiplier efficiently learnable within the network’s non-linearities operating ranges.\nAvailable distributions: - Poisson - Normal - StudentT - NegativeBinomial - Tweedie\nParameters: distribution: str, identifier of a torch.distributions.Distribution class. level: float list [0,100], confidence levels for prediction intervals. quantiles: float list [0,1], alternative to level list, target quantiles. num_samples: int=500, number of samples for the empirical quantiles. return_params: bool=False, wether or not return the Distribution parameters.\nReferences: - PyTorch Probability Distributions Package: StudentT. - David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). “DeepAR: Probabilistic forecasting with autoregressive recurrent networks”. International Journal of Forecasting.\n\nsource\n\n\nDistributionLoss.sample\n\n DistributionLoss.sample (distr_args:torch.Tensor,\n                          num_samples:Optional[int]=None)\n\nConstruct the empirical quantiles from the estimated Distribution, sampling from it num_samples independently.\nParameters distr_args: Constructor arguments for the underlying Distribution type. loc: Optional tensor, of the same shape as the batch_shape + event_shape of the resulting distribution. scale: Optional tensor, of the same shape as the batch_shape+event_shape of the resulting distribution. num_samples: int=500, overwrite number of samples for the empirical quantiles.\nReturns samples: tensor, shape [B,H,num_samples]. quantiles: tensor, empirical quantiles defined by levels.\n\nsource\n\n\nDistributionLoss.__call__\n\n DistributionLoss.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n                            mask:Optional[torch.Tensor]=None)\n\nComputes the negative log-likelihood objective function. To estimate the following predictive distribution:\n\\[\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))\\]\nwhere \\(\\theta\\) represents the distributions parameters. It aditionally summarizes the objective signal using a weighted average using the mask tensor.\nParameters y: tensor, Actual values. distr_args: Constructor arguments for the underlying Distribution type. loc: Optional tensor, of the same shape as the batch_shape + event_shape of the resulting distribution. scale: Optional tensor, of the same shape as the batch_shape+event_shape of the resulting distribution. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns loss: scalar, weighted loss function against which backpropagation will be performed."
  },
  {
    "objectID": "losses.pytorch.html#poisson-mixture-mesh-pmm",
    "href": "losses.pytorch.html#poisson-mixture-mesh-pmm",
    "title": "PyTorch Losses",
    "section": "Poisson Mixture Mesh (PMM)",
    "text": "Poisson Mixture Mesh (PMM)\n\nsource\n\nPMM.__init__\n\n PMM.__init__ (n_components=10, level=[80, 90], quantiles=None,\n               num_samples=1000, return_params=False,\n               batch_correlation=False, horizon_correlation=False)\n\nPoisson Mixture Mesh\nThis Poisson Mixture statistical model assumes independence across groups of data \\(\\mathcal{G}=\\{[g_{i}]\\}\\), and estimates relationships within the group.\n\\[ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) =\n\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P} \\left(\\mathbf{y}_{[g_{i}][\\tau]} \\right) =\n\\prod_{\\beta\\in[g_{i}]}\n\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]} \\mathrm{Poisson}(y_{\\beta,\\tau}, \\hat{\\lambda}_{\\beta,\\tau,k}) \\right)\\]\nParameters: n_components: int=10, the number of mixture components. level: float list [0,100], confidence levels for prediction intervals. quantiles: float list [0,1], alternative to level list, target quantiles. return_params: bool=False, wether or not return the Distribution parameters. batch_correlation: bool=False, wether or not model batch correlations. horizon_correlation: bool=False, wether or not model horizon correlations.\nReferences: Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International Journal Forecasting, Working paper available at arxiv.\n\nsource\n\n\nPMM.sample\n\n PMM.sample (distr_args, num_samples=None)\n\nConstruct the empirical quantiles from the estimated Distribution, sampling from it num_samples independently.\nParameters distr_args: Constructor arguments for the underlying Distribution type. loc: Optional tensor, of the same shape as the batch_shape + event_shape of the resulting distribution. scale: Optional tensor, of the same shape as the batch_shape+event_shape of the resulting distribution. num_samples: int=500, overwrites number of samples for the empirical quantiles.\nReturns samples: tensor, shape [B,H,num_samples]. quantiles: tensor, empirical quantiles defined by levels.\n\nsource\n\n\nPMM.__call__\n\n PMM.__call__ (y:torch.Tensor, distr_args:Tuple[torch.Tensor],\n               mask:Optional[torch.Tensor]=None)\n\nCall self as a function."
  },
  {
    "objectID": "losses.pytorch.html#gaussian-mixture-mesh-gmm",
    "href": "losses.pytorch.html#gaussian-mixture-mesh-gmm",
    "title": "PyTorch Losses",
    "section": "Gaussian Mixture Mesh (GMM)",
    "text": "Gaussian Mixture Mesh (GMM)\n\nsource\n\nGMM.__init__\n\n GMM.__init__ (n_components=1, level=[80, 90], quantiles=None,\n               num_samples=1000, return_params=False,\n               batch_correlation=False, horizon_correlation=False)\n\nGaussian Mixture Mesh\nThis Gaussian Mixture statistical model assumes independence across groups of data \\(\\mathcal{G}=\\{[g_{i}]\\}\\), and estimates relationships within the group.\n\\[ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) =\n\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\tau]}\\right)=\n\\prod_{\\beta\\in[g_{i}]}\n\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]}\n\\mathrm{Gaussian}(y_{\\beta,\\tau}, \\hat{\\mu}_{\\beta,\\tau,k}, \\sigma_{\\beta,\\tau,k})\\right)\\]\nParameters: n_components: int=10, the number of mixture components. level: float list [0,100], confidence levels for prediction intervals. quantiles: float list [0,1], alternative to level list, target quantiles. return_params: bool=False, wether or not return the Distribution parameters. batch_correlation: bool=False, wether or not model batch correlations. horizon_correlation: bool=False, wether or not model horizon correlations.\nReferences: Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International Journal Forecasting, Working paper available at arxiv.\n\nsource\n\n\nGMM.sample\n\n GMM.sample (distr_args, num_samples=None)\n\nConstruct the empirical quantiles from the estimated Distribution, sampling from it num_samples independently.\nParameters distr_args: Constructor arguments for the underlying Distribution type. loc: Optional tensor, of the same shape as the batch_shape + event_shape of the resulting distribution. scale: Optional tensor, of the same shape as the batch_shape+event_shape of the resulting distribution. num_samples: int=500, number of samples for the empirical quantiles.\nReturns samples: tensor, shape [B,H,num_samples]. quantiles: tensor, empirical quantiles defined by levels.\n\nsource\n\n\nGMM.__call__\n\n GMM.__call__ (y:torch.Tensor,\n               distr_args:Tuple[torch.Tensor,torch.Tensor],\n               mask:Optional[torch.Tensor]=None)\n\nCall self as a function."
  },
  {
    "objectID": "losses.pytorch.html#negative-binomial-mixture-mesh-nbmm",
    "href": "losses.pytorch.html#negative-binomial-mixture-mesh-nbmm",
    "title": "PyTorch Losses",
    "section": "Negative Binomial Mixture Mesh (NBMM)",
    "text": "Negative Binomial Mixture Mesh (NBMM)\n\nsource\n\nNBMM.__init__\n\n NBMM.__init__ (n_components=1, level=[80, 90], quantiles=None,\n                num_samples=1000, return_params=False)\n\nNegative Binomial Mixture Mesh\nThis N. Binomial Mixture statistical model assumes independence across groups of data \\(\\mathcal{G}=\\{[g_{i}]\\}\\), and estimates relationships within the group.\n\\[ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) =\n\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\tau]}\\right)=\n\\prod_{\\beta\\in[g_{i}]}\n\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]}\n\\mathrm{NBinomial}(y_{\\beta,\\tau}, \\hat{r}_{\\beta,\\tau,k}, \\hat{p}_{\\beta,\\tau,k})\\right)\\]\nParameters: n_components: int=10, the number of mixture components. level: float list [0,100], confidence levels for prediction intervals. quantiles: float list [0,1], alternative to level list, target quantiles. return_params: bool=False, wether or not return the Distribution parameters.\nReferences: Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International Journal Forecasting, Working paper available at arxiv.\n\nsource\n\n\nNBMM.sample\n\n NBMM.sample (distr_args, num_samples=None)\n\nConstruct the empirical quantiles from the estimated Distribution, sampling from it num_samples independently.\nParameters distr_args: Constructor arguments for the underlying Distribution type. loc: Optional tensor, of the same shape as the batch_shape + event_shape of the resulting distribution. scale: Optional tensor, of the same shape as the batch_shape+event_shape of the resulting distribution. num_samples: int=500, number of samples for the empirical quantiles.\nReturns samples: tensor, shape [B,H,num_samples]. quantiles: tensor, empirical quantiles defined by levels.\n\nsource\n\n\nNBMM.__call__\n\n NBMM.__call__ (y:torch.Tensor,\n                distr_args:Tuple[torch.Tensor,torch.Tensor],\n                mask:Optional[torch.Tensor]=None)\n\nCall self as a function."
  },
  {
    "objectID": "models.lstm.html",
    "href": "models.lstm.html",
    "title": "LSTM",
    "section": "",
    "text": "The Long Short-Term Memory Recurrent Neural Network (LSTM), uses a multilayer LSTM encoder and an MLP decoder. It builds upon the LSTM-cell that improves the exploding and vanishing gradients of classic RNN’s. This network has been extensively used in sequential prediction tasks like language modeling, phonetic labeling, and forecasting. The predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{LSTM}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nwhere \\(\\mathbf{h}_{t}\\), is the hidden state for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction.\nReferences-Jeffrey L. Elman (1990). “Finding Structure in Time”.-Haşim Sak, Andrew Senior, Françoise Beaufays (2014). “Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition.”\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.lstm.html#usage-example",
    "href": "models.lstm.html#usage-example",
    "title": "LSTM",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import LSTM\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nnf = NeuralForecast(\n    models=[LSTM(h=12, input_size=-1,\n                 loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                 scaler_type='robust',\n                 encoder_n_layers=2,\n                 encoder_hidden_size=128,\n                 context_size=10,\n                 decoder_hidden_size=128,\n                 decoder_layers=2,\n                 max_steps=200,\n                 futr_exog_list=['y_[lag12]'],\n                 #hist_exog_list=['y_[lag12]'],\n                 stat_exog_list=['airline1'],\n                 )\n    ],\n    freq='M'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic)\nY_hat_df = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['LSTM'], c='purple', label='mean')\nplt.plot(plot_df['ds'], plot_df['LSTM-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['LSTM-lo-90'][-12:].values, \n                 y2=plot_df['LSTM-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": " Core ",
    "section": "",
    "text": "source\n\nNeuralForecast\n\n NeuralForecast (models:List[Any], freq:str)\n\nThe core.StatsForecast class allows you to efficiently fit multiple NeuralForecast models for large sets of time series. It operates with pandas DataFrame df that identifies series and datestamps with the unique_id and ds columns. The y column denotes the target time series variable.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ntyping.List[typing.Any]\nInstantiated neuralforecast.models see collection here.\n\n\nfreq\nstr\nFrequency of the data, see panda’s available frequencies.\n\n\nReturns\nNeuralForecast\nReturns instantiated NeuralForecast class.\n\n\n\n\nsource\n\n\nNeuralForecast.fit\n\n NeuralForecast.fit (df:Optional[pandas.core.frame.DataFrame]=None,\n                     static_df:Optional[pandas.core.frame.DataFrame]=None,\n                     val_size:Optional[int]=0, sort_df:bool=True,\n                     verbose:bool=False)\n\nFit the core.NeuralForecast.\nFit models to a large set of time series from DataFrame df. and store fitted models for later inspection.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous variables.If None, a previously stored dataset is required.\n\n\nstatic_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id] and static exogenous.\n\n\nval_size\ntyping.Optional[int]\n0\nSize of validation set.\n\n\nsort_df\nbool\nTrue\nSort df before fitting.\n\n\nverbose\nbool\nFalse\nPrint processing steps.\n\n\nReturns\nNeuralForecast\n\nReturns NeuralForecast class with fitted models.\n\n\n\n\nsource\n\n\nNeuralForecast.predict\n\n NeuralForecast.predict (df:Optional[pandas.core.frame.DataFrame]=None,\n                         static_df:Optional[pandas.core.frame.DataFrame]=N\n                         one, futr_df:Optional[pandas.core.frame.DataFrame\n                         ]=None, sort_df:bool=True, verbose:bool=False,\n                         **data_kwargs)\n\nPredict with core.NeuralForecast.\nUse stored fitted models to predict large set of time series from DataFrame df.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous variables.If a DataFrame is passed, it is used to generate forecasts.\n\n\nstatic_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id] and static exogenous.\n\n\nfutr_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with [unique_id, ds] columns and df’s future exogenous.\n\n\nsort_df\nbool\nTrue\nSort df before fitting.\n\n\nverbose\nbool\nFalse\nPrint processing steps.\n\n\ndata_kwargs\nkwargs\n\nExtra arguments to be passed to the dataset within each model.\n\n\nReturns\npandas.DataFrame\n\nDataFrame with insample models columns for point predictions and probabilisticpredictions for all fitted models. \n\n\n\n\nsource\n\n\nNeuralForecast.cross_validation\n\n NeuralForecast.cross_validation\n                                  (df:Optional[pandas.core.frame.DataFrame\n                                  ]=None, static_df:Optional[pandas.core.f\n                                  rame.DataFrame]=None, n_windows:int=1,\n                                  step_size:int=1,\n                                  val_size:Optional[int]=0,\n                                  test_size:Optional[int]=None,\n                                  sort_df:bool=True, fit_models:bool=True,\n                                  verbose:bool=False, **data_kwargs)\n\nTemporal Cross-Validation with core.NeuralForecast.\ncore.NeuralForecast’s cross-validation efficiently fits a list of NeuralForecast models through multiple windows, in either chained or rolled manner.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous variables.If None, a previously stored dataset is required.\n\n\nstatic_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id] and static exogenous.\n\n\nn_windows\nint\n1\nNumber of windows used for cross validation.\n\n\nstep_size\nint\n1\nStep size between each window.\n\n\nval_size\ntyping.Optional[int]\n0\nLength of validation size. If passed, set n_windows=None.\n\n\ntest_size\ntyping.Optional[int]\nNone\nLength of test size. If passed, set n_windows=None.\n\n\nsort_df\nbool\nTrue\nSort df before fitting.\n\n\nfit_models\nbool\nTrue\n\n\n\nverbose\nbool\nFalse\nPrint processing steps.\n\n\ndata_kwargs\nkwargs\n\nExtra arguments to be passed to the dataset within each model.\n\n\nReturns\npandas.DataFrame\n\nDataFrame with insample models columns for point predictions and probabilisticpredictions for all fitted models. \n\n\n\n\nsource\n\n\nNeuralForecast.predict_rolled\n\n NeuralForecast.predict_rolled\n                                (df:Optional[pandas.core.frame.DataFrame]=\n                                None, static_df:Optional[pandas.core.frame\n                                .DataFrame]=None, n_windows:int=1,\n                                step_size:int=1,\n                                insample_size:Optional[int]=None,\n                                sort_df:bool=True, verbose:bool=False,\n                                **data_kwargs)\n\nPredict insample with core.NeuralForecast.\nUse stored fitted models to predict historic values of a time series from DataFrame df.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous variables.If None, a previously stored dataset is required.\n\n\nstatic_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id] and static exogenous.\n\n\nn_windows\nint\n1\nNumber of windows used for cross validation.\n\n\nstep_size\nint\n1\nStep size between each window.\n\n\ninsample_size\ntyping.Optional[int]\nNone\nLength of insample size to produce forecasts. If passed, set n_windows=None.\n\n\nsort_df\nbool\nTrue\nSort df before fitting.\n\n\nverbose\nbool\nFalse\nPrint processing steps.\n\n\ndata_kwargs\nkwargs\n\nExtra arguments to be passed to the dataset within each model.\n\n\nReturns\npandas.DataFrame\n\nDataFrame with insample models columns for point predictions and probabilisticpredictions for all fitted models. \n\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.nbeats.html",
    "href": "models.nbeats.html",
    "title": "NBEATS",
    "section": "",
    "text": "The Neural Basis Expansion Analysis (NBEATS) is an MLP-based deep neural architecture with backward and forward residual links. The network has two variants: (1) in its interpretable configuration, NBEATS sequentially projects the signal into polynomials and harmonic basis to learn trend and seasonality components; (2) in its generic configuration, it substitutes the polynomial and harmonic basis for identity basis and larger network’s depth. The Neural Basis Expansion Analysis with Exogenous (NBEATSx), incorporates projections to exogenous temporal variables available at the time of the prediction.\nThis method proved state-of-the-art performance on the M3, M4, and Tourism Competition datasets, improving accuracy by 3% over the ESRNN M4 competition winner.\nReferences -Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2019). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”.\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.nbeats.html#usage-example",
    "href": "models.nbeats.html#usage-example",
    "title": "NBEATS",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATS\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NBEATS(h=12, input_size=24,\n               loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n               stack_types = ['identity', 'trend', 'seasonality'],\n               max_steps=100,\n               val_check_steps=10,\n               early_stop_patience_steps=2)\n\nfcst = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['NBEATS-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['NBEATS-lo-90'][-12:].values, \n                 y2=plot_df['NBEATS-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.grid()\nplt.legend()\nplt.plot()"
  },
  {
    "objectID": "common.base_multivariate.html",
    "href": "common.base_multivariate.html",
    "title": "BaseMultivariate",
    "section": "",
    "text": "BaseMultivariate\n\n BaseMultivariate (h, input_size, loss, valid_loss, learning_rate,\n                   max_steps, val_check_steps, n_series, batch_size,\n                   step_size=1, num_lr_decays=0,\n                   early_stop_patience_steps=-1, scaler_type='robust',\n                   futr_exog_list=None, hist_exog_list=None,\n                   stat_exog_list=None, num_workers_loader=0,\n                   drop_last_loader=False, random_seed=1, alias=None,\n                   **trainer_kwargs)\n\nBase Multivariate\nBase class for all multivariate models. The forecasts for all time-series are produced simultaneously within each window, which are randomly sampled during training.\nThis class implements the basic functionality for all windows-based models, including: - PyTorch Lightning’s methods training_step, validation_step, predict_step. - fit and predict methods used by NeuralForecast.core class. - sampling and wrangling methods to generate multivariate windows.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "common.modules.html",
    "href": "common.modules.html",
    "title": "NN Modules",
    "section": "",
    "text": "Multi-Layer Perceptron\n\nsource\n\n\n\n MLP (in_features, out_features, activation, hidden_size, num_layers,\n      dropout)\n\nMulti-Layer Perceptron Class\nParameters: in_features: int, dimension of input. out_features: int, dimension of output. activation: str, activation function to use. hidden_size: int, dimension of hidden layers. num_layers: int, number of hidden layers. dropout: float, dropout rate.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "common.modules.html#mlp",
    "href": "common.modules.html#mlp",
    "title": "NN Modules",
    "section": "",
    "text": "Multi-Layer Perceptron\n\nsource\n\n\n\n MLP (in_features, out_features, activation, hidden_size, num_layers,\n      dropout)\n\nMulti-Layer Perceptron Class\nParameters: in_features: int, dimension of input. out_features: int, dimension of output. activation: str, activation function to use. hidden_size: int, dimension of hidden layers. num_layers: int, number of hidden layers. dropout: float, dropout rate."
  },
  {
    "objectID": "common.modules.html#temporal-convolutions",
    "href": "common.modules.html#temporal-convolutions",
    "title": "NN Modules",
    "section": "2. Temporal Convolutions",
    "text": "2. Temporal Convolutions\nFor long time in deep learning, sequence modelling was synonymous with recurrent networks, yet several papers have shown that simple convolutional architectures can outperform canonical recurrent networks like LSTMs by demonstrating longer effective memory.\nReferences -van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. W., & Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. Computing Research Repository, abs/1609.03499. URL: http://arxiv.org/abs/1609.03499. arXiv:1609.03499. -Shaojie Bai, Zico Kolter, Vladlen Koltun. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. Computing Research Repository, abs/1803.01271. URL: https://arxiv.org/abs/1803.01271.\n\n\nChomp1d\n\n Chomp1d (horizon)\n\nChomp1d\nReceives x input of dim [N,C,T], and trims it so that only ‘time available’ information is used. Used by one dimensional causal convolutions CausalConv1d.\nParameters: horizon: int, length of outsample values to skip.\n\n\n\nCausalConv1d\n\n CausalConv1d (in_channels, out_channels, kernel_size, padding, dilation,\n               activation, stride:int=1)\n\nCausal Convolution 1d\nReceives x input of dim [N,C_in,T], and computes a causal convolution in the time dimension. Skipping the H steps of the forecast horizon, through its dilation. Consider a batch of one element, the dilated convolution operation on the \\(t\\) time step is defined:\n\\(\\mathrm{Conv1D}(\\mathbf{x},\\mathbf{w})(t) = (\\mathbf{x}_{[*d]} \\mathbf{w})(t) = \\sum^{K}_{k=1} w_{k} \\mathbf{x}_{t-dk}\\)\nwhere \\(d\\) is the dilation factor, \\(K\\) is the kernel size, \\(t-dk\\) is the index of the considered past observation. The dilation effectively applies a filter with skip connections. If \\(d=1\\) one recovers a normal convolution.\nParameters: in_channels: int, dimension of x input’s initial channels. out_channels: int, dimension of x outputs’s channels. activation: str, identifying activations from PyTorch activations. select from ‘ReLU’,‘Softplus’,‘Tanh’,‘SELU’, ‘LeakyReLU’,‘PReLU’,‘Sigmoid’. padding: int, number of zero padding used to the left. kernel_size: int, convolution’s kernel size. dilation: int, dilation skip connections.\nReturns: x: tensor, torch tensor of dim [N,C_out,T] activation(conv1d(inputs, kernel) + bias). \n\n\n\nTemporalConvolutionEncoder\n\n TemporalConvolutionEncoder (in_channels, out_channels, kernel_size,\n                             dilations, activation:str='ReLU')\n\nTemporal Convolution Encoder\nReceives x input of dim [N,T,C_in], permutes it to [N,C_in,T] applies a deep stack of exponentially dilated causal convolutions. The exponentially increasing dilations of the convolutions allow for the creation of weighted averages of exponentially large long-term memory.\nParameters: in_channels: int, dimension of x input’s initial channels. out_channels: int, dimension of x outputs’s channels. kernel_size: int, size of the convolving kernel. dilations: int list, controls the temporal spacing between the kernel points. activation: str, identifying activations from PyTorch activations. select from ‘ReLU’,‘Softplus’,‘Tanh’,‘SELU’, ‘LeakyReLU’,‘PReLU’,‘Sigmoid’.\nReturns: x: tensor, torch tensor of dim [N,T,C_out]."
  },
  {
    "objectID": "common.modules.html#transformers",
    "href": "common.modules.html#transformers",
    "title": "NN Modules",
    "section": "3. Transformers",
    "text": "3. Transformers\nReferences - Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang. “Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting” - Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.\n\n\nTransEncoder\n\n TransEncoder (attn_layers, conv_layers=None, norm_layer=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nTransEncoderLayer\n\n TransEncoderLayer (attention, hidden_size, conv_hidden_size=None,\n                    dropout=0.1, activation='relu')\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nTransDecoder\n\n TransDecoder (layers, norm_layer=None, projection=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nTransDecoderLayer\n\n TransDecoderLayer (self_attention, cross_attention, hidden_size,\n                    conv_hidden_size=None, dropout=0.1, activation='relu')\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nAttentionLayer\n\n AttentionLayer (attention, hidden_size, n_head, d_keys=None,\n                 d_values=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nDataEmbedding\n\n DataEmbedding (c_in, exog_input_size, hidden_size, pos_embedding=True,\n                dropout=0.1)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nTimeFeatureEmbedding\n\n TimeFeatureEmbedding (input_size, hidden_size)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nTokenEmbedding\n\n TokenEmbedding (c_in, hidden_size)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nPositionalEmbedding\n\n PositionalEmbedding (hidden_size, max_len=5000)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "models.rnn.html",
    "href": "models.rnn.html",
    "title": "RNN",
    "section": "",
    "text": "Elman proposed this classic recurrent neural network (RNN) in 1990, where each layer uses the following recurrent transformation: \\[\\mathbf{h}^{l}_{t} = \\mathrm{Activation}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}] W^{\\intercal}_{ih} + b_{ih}  +  \\mathbf{h}^{l}_{t-1} W^{\\intercal}_{hh} + b_{hh})\\]\nwhere \\(\\mathbf{h}^{l}_{t}\\), is the hidden state of RNN layer \\(l\\) for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction. The available activations are tanh, and relu. The predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{RNN}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nReferences -Jeffrey L. Elman (1990). “Finding Structure in Time”. -Cho, K., van Merrienboer, B., Gülcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation.\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.rnn.html#usage-example",
    "href": "models.rnn.html#usage-example",
    "title": "RNN",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import RNN\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds&lt;AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds&gt;=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[RNN(h=12,\n                input_size=-1,\n                inference_input_size=24,\n                loss=MQLoss(level=[80, 90]),\n                scaler_type='robust',\n                encoder_n_layers=2,\n                encoder_hidden_size=128,\n                context_size=10,\n                decoder_hidden_size=128,\n                decoder_layers=2,\n                max_steps=300,\n                futr_exog_list=['y_[lag12]'],\n                #hist_exog_list=['y_[lag12]'],\n                stat_exog_list=['airline1'],\n                )\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['RNN-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['RNN-lo-90'][-12:].values, \n                 y2=plot_df['RNN-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "losses.numpy.html",
    "href": "losses.numpy.html",
    "title": "NumPy Evaluation",
    "section": "",
    "text": "These metrics are on the same scale as the data.\n\n\n\nsource\n\n\n\n mae (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMean Absolute Error\nCalculates Mean Absolute Error between y and y_hat. MAE measures the relative prediction accuracy of a forecasting method by calculating the deviation of the prediction and the true value at a given time and averages these devations over the length of the series.\n\\[ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mae: numpy array, (single value).\n\n\n\n\n\n\nsource\n\n\n\n mse (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMean Squared Error\nCalculates Mean Squared Error between y and y_hat. MSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the true value at a given time, and averages these devations over the length of the series.\n\\[ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mse: numpy array, (single value).\n\n\n\n\n\n\nsource\n\n\n\n rmse (y:numpy.ndarray, y_hat:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nRoot Mean Squared Error\nCalculates Root Mean Squared Error between y and y_hat. RMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. Finally the RMSE will be in the same scale as the original time series so its comparison with other series is possible only if they share a common scale. RMSE has a direct connection to the L2 norm.\n\\[ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: rmse: numpy array, (single value).\nGive us a ⭐ on Github"
  },
  {
    "objectID": "losses.numpy.html#mean-absolute-error",
    "href": "losses.numpy.html#mean-absolute-error",
    "title": "NumPy Evaluation",
    "section": "",
    "text": "source\n\n\n\n mae (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMean Absolute Error\nCalculates Mean Absolute Error between y and y_hat. MAE measures the relative prediction accuracy of a forecasting method by calculating the deviation of the prediction and the true value at a given time and averages these devations over the length of the series.\n\\[ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mae: numpy array, (single value)."
  },
  {
    "objectID": "losses.numpy.html#mean-squared-error",
    "href": "losses.numpy.html#mean-squared-error",
    "title": "NumPy Evaluation",
    "section": "",
    "text": "source\n\n\n\n mse (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMean Squared Error\nCalculates Mean Squared Error between y and y_hat. MSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the true value at a given time, and averages these devations over the length of the series.\n\\[ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mse: numpy array, (single value)."
  },
  {
    "objectID": "losses.numpy.html#root-mean-squared-error",
    "href": "losses.numpy.html#root-mean-squared-error",
    "title": "NumPy Evaluation",
    "section": "",
    "text": "source\n\n\n\n rmse (y:numpy.ndarray, y_hat:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nRoot Mean Squared Error\nCalculates Root Mean Squared Error between y and y_hat. RMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. Finally the RMSE will be in the same scale as the original time series so its comparison with other series is possible only if they share a common scale. RMSE has a direct connection to the L2 norm.\n\\[ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: rmse: numpy array, (single value)."
  },
  {
    "objectID": "losses.numpy.html#mean-absolute-percentage-error",
    "href": "losses.numpy.html#mean-absolute-percentage-error",
    "title": "NumPy Evaluation",
    "section": "Mean Absolute Percentage Error",
    "text": "Mean Absolute Percentage Error\n\nsource\n\nmape\n\n mape (y:numpy.ndarray, y_hat:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMean Absolute Percentage Error\nCalculates Mean Absolute Percentage Error between y and y_hat. MAPE measures the relative prediction accuracy of a forecasting method by calculating the percentual deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. The closer to zero an observed value is, the higher penalty MAPE loss assigns to the corresponding error.\n\\[ \\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mape: numpy array, (single value)."
  },
  {
    "objectID": "losses.numpy.html#smape",
    "href": "losses.numpy.html#smape",
    "title": "NumPy Evaluation",
    "section": "SMAPE",
    "text": "SMAPE\n\nsource\n\nsmape\n\n smape (y:numpy.ndarray, y_hat:numpy.ndarray,\n        weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nSymmetric Mean Absolute Percentage Error\nCalculates Symmetric Mean Absolute Percentage Error between y and y_hat. SMAPE measures the relative prediction accuracy of a forecasting method by calculating the relative deviation of the prediction and the observed value scaled by the sum of the absolute values for the prediction and observed value at a given time, then averages these devations over the length of the series. This allows the SMAPE to have bounds between 0% and 200% which is desirable compared to normal MAPE that may be undetermined when the target is zero.\n\\[ \\mathrm{sMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: smape: numpy array, (single value).\nReferences: Makridakis S., “Accuracy measures: theoretical and practical concerns”."
  },
  {
    "objectID": "losses.numpy.html#mean-absolute-scaled-error",
    "href": "losses.numpy.html#mean-absolute-scaled-error",
    "title": "NumPy Evaluation",
    "section": "Mean Absolute Scaled Error",
    "text": "Mean Absolute Scaled Error\n\nsource\n\nmase\n\n mase (y:numpy.ndarray, y_hat:numpy.ndarray, y_train:numpy.ndarray,\n       seasonality:int, weights:Optional[numpy.ndarray]=None,\n       axis:Optional[int]=None)\n\nMean Absolute Scaled Error Calculates the Mean Absolute Scaled Error between y and y_hat. MASE measures the relative prediction accuracy of a forecasting method by comparinng the mean absolute errors of the prediction and the observed value against the mean absolute errors of the seasonal naive model. The MASE partially composed the Overall Weighted Average (OWA), used in the M4 Competition.\n\\[ \\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})} \\]\nParameters: y: numpy array, (batch_size, output_size), Actual values. y_hat: numpy array, (batch_size, output_size)), Predicted values. y_insample: numpy array, (batch_size, input_size), Actual insample Seasonal Naive predictions. seasonality: int. Main frequency of the time series; Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\nmask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mase: numpy array, (single value).\nReferences: Rob J. Hyndman, & Koehler, A. B. “Another look at measures of forecast accuracy”. Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, “The M4 Competition: 100,000 time series and 61 forecasting methods”."
  },
  {
    "objectID": "losses.numpy.html#relative-mean-absolute-error",
    "href": "losses.numpy.html#relative-mean-absolute-error",
    "title": "NumPy Evaluation",
    "section": "Relative Mean Absolute Error",
    "text": "Relative Mean Absolute Error\n\nsource\n\nrmae\n\n rmae (y:numpy.ndarray, y_hat1:numpy.ndarray, y_hat2:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nRMAE\nCalculates Relative Mean Absolute Error (RMAE) between two sets of forecasts (from two different forecasting methods). A number smaller than one implies that the forecast in the numerator is better than the forecast in the denominator.\n\\[ \\mathrm{rMAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau})} \\]\nParameters: y: numpy array, observed values. y_hat1: numpy array. Predicted values of first model. y_hat2: numpy array. Predicted values of baseline model. weights: numpy array, optional. Weights for weighted average. axis: None or int, optional.Axis or axes along which to average a. The default, axis=None, will average over all of the elements of the input array.\nReturns: rmae: numpy array or double.\nReferences: Rob J. Hyndman, & Koehler, A. B. “Another look at measures of forecast accuracy”."
  },
  {
    "objectID": "losses.numpy.html#quantile-loss",
    "href": "losses.numpy.html#quantile-loss",
    "title": "NumPy Evaluation",
    "section": "Quantile Loss",
    "text": "Quantile Loss\n\nsource\n\nquantile_loss\n\n quantile_loss (y:numpy.ndarray, y_hat:numpy.ndarray, q:float=0.5,\n                weights:Optional[numpy.ndarray]=None,\n                axis:Optional[int]=None)\n\nQuantile Loss\nComputes the quantile loss between y and y_hat. QL measures the deviation of a quantile forecast. By weighting the absolute deviation in a non symmetric way, the loss pays more attention to under or over estimation. A common value for q is 0.5 for the deviation from the median (Pinball loss).\n\\[ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. q: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: quantile_loss: numpy array, (single value).\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”."
  },
  {
    "objectID": "losses.numpy.html#multi-quantile-loss",
    "href": "losses.numpy.html#multi-quantile-loss",
    "title": "NumPy Evaluation",
    "section": "Multi-Quantile Loss",
    "text": "Multi-Quantile Loss\n\nsource\n\nmqloss\n\n mqloss (y:numpy.ndarray, y_hat:numpy.ndarray, quantiles:numpy.ndarray,\n         weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMulti-Quantile loss\nCalculates the Multi-Quantile loss (MQL) between y and y_hat. MQL calculates the average multi-quantile Loss for a given set of quantiles, based on the absolute difference between predicted quantiles and observed values.\n\\[ \\mathrm{MQL}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) \\]\nThe limit behavior of MQL allows to measure the accuracy of a full predictive distribution \\(\\mathbf{\\hat{F}}_{\\tau}\\) with the continuous ranked probability score (CRPS). This can be achieved through a numerical integration technique, that discretizes the quantiles and treats the CRPS integral with a left Riemann approximation, averaging over uniformly distanced quantiles.\n\\[ \\mathrm{CRPS}(y_{\\tau}, \\mathbf{\\hat{F}}_{\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) dq \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. quantiles: numpy array,(n_quantiles). Quantiles to estimate from the distribution of y. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mqloss: numpy array, (single value).\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”. James E. Matheson and Robert L. Winkler, “Scoring Rules for Continuous Probability Distributions”."
  }
]